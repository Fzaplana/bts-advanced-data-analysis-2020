{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02_Advanced_Data_Analysis_Keras_API_cted_NOTsolved.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aB_ck5OSE3TU",
        "colab_type": "text"
      },
      "source": [
        "![BTS](https://github.com/vfp1/bts-mbds-data-science-foundations-2019/raw/master/sessions/img/Logo-BTS.jpg)\n",
        "\n",
        "# Session 02: Keras API cted\n",
        "### Victor F. Pajuelo Madrigal <victor.pajuelo@bts.tech> - Advanced Data Analysis (28-03-2020)\n",
        "\n",
        "Open this notebook in Google Colaboratory: [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vfp1/bts-advanced-data-analysis-2020/blob/master/S02_TF_intro/02_Advanced_Data_Analysis_Keras_API_cted_NOTsolved.ipynb)\n",
        "\n",
        "**Resources (code patched and updated from):**\n",
        "* Sklearn\n",
        "* TensorFlow Authors\n",
        "* Aurelien Geron's O'Reilly's \"Hands-On Machine Learning with Scikit-Learn, Keras & Tensorflow\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hzDxyCXqXlt_"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "reTbVTMhXluB"
      },
      "source": [
        "First, let's import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures. We also check that Python 3.5 or later is installed (although Python 2.x may work, it is deprecated so we strongly recommend you use Python 3 instead), as well as Scikit-Learn ≥0.20 and TensorFlow ≥2.0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3QX_xNLrXluB",
        "colab": {}
      },
      "source": [
        "# Python ≥3.5 is required\n",
        "import sys\n",
        "assert sys.version_info >= (3, 5)\n",
        "\n",
        "# Scikit-Learn ≥0.20 is required\n",
        "import sklearn\n",
        "assert sklearn.__version__ >= \"0.20\"\n",
        "\n",
        "try:\n",
        "    # %tensorflow_version only exists in Colab.\n",
        "    %tensorflow_version 2.x\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# TensorFlow ≥2.0 is required\n",
        "import tensorflow as tf\n",
        "assert tf.__version__ >= \"2.0\"\n",
        "\n",
        "# Import Keras from TensorFlow\n",
        "from tensorflow import keras\n",
        "\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# to make this notebook's output stable across runs\n",
        "np.random.seed(42)\n",
        "\n",
        "# To plot pretty figures\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)\n",
        "\n",
        "# Where to save the figures\n",
        "PROJECT_ROOT_DIR = \".\"\n",
        "CHAPTER_ID = \"ann\"\n",
        "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
        "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
        "\n",
        "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
        "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
        "    print(\"Saving figure\", fig_id)\n",
        "    if tight_layout:\n",
        "        plt.tight_layout()\n",
        "    plt.savefig(path, format=fig_extension, dpi=resolution)\n",
        "\n",
        "# Ignore useless warnings (see SciPy issue #5998)\n",
        "import warnings\n",
        "warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ml9GUPN-Iw1m",
        "colab_type": "text"
      },
      "source": [
        "# A paradigm change\n",
        "\n",
        "UUID - #S2C1\n",
        "\n",
        "Source: The TensorFlow Authors (Laurence Moroney)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fA93WUy1zzWf"
      },
      "source": [
        "Like every first app you should start with something super simple that shows the overall scaffolding for how your code works. \n",
        "\n",
        "In the case of creating neural networks, the sample I like to use is one where it learns the relationship between two numbers. So, for example, if you were writing code for a function like this, you already know the 'rules' — \n",
        "\n",
        "\n",
        "```\n",
        "float hw_function(float x){\n",
        "    float y = (2 * x) - 1;\n",
        "    return y;\n",
        "}\n",
        "```\n",
        "\n",
        "So how would you train a neural network to do the equivalent task? Using data! By feeding it with a set of Xs, and a set of Ys, it should be able to figure out the relationship between them. \n",
        "\n",
        "This is obviously a very different paradigm than what you might be used to, so let's step through it piece by piece.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wwJGmDrQ0EoB"
      },
      "source": [
        "## Define and Compile the Neural Network\n",
        "\n",
        "Next we will create the simplest possible neural network. It has 1 layer, and that layer has 1 neuron, and the input shape to it is just 1 value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kQFAr_xo0M4T",
        "colab": {}
      },
      "source": [
        "model = tf.keras.Sequential([keras.layers.Dense(units=1, input_shape=[1])])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KhjZjZ-c0Ok9"
      },
      "source": [
        "Now we compile our Neural Network. When we do so, we have to specify 2 functions, a loss and an optimizer.\n",
        "\n",
        "If you've seen lots of math for machine learning, here's where it's usually used, but in this case it's nicely encapsulated in functions for you. But what happens here — let's explain...\n",
        "\n",
        "We know that in our function, the relationship between the numbers is y=2x-1. \n",
        "\n",
        "When the computer is trying to 'learn' that, it makes a guess...maybe y=10x+10. The LOSS function measures the guessed answers against the known correct answers and measures how well or how badly it did.\n",
        "\n",
        "It then uses the OPTIMIZER function to make another guess. Based on how the loss function went, it will try to minimize the loss. At that point maybe it will come up with somehting like y=5x+5, which, while still pretty bad, is closer to the correct result (i.e. the loss is lower)\n",
        "\n",
        "It will repeat this for the number of EPOCHS which you will see shortly. But first, here's how we tell it to use 'MEAN SQUARED ERROR' for the loss and 'STOCHASTIC GRADIENT DESCENT' for the optimizer. You don't need to understand the math for these yet, but you can see that they work! :)\n",
        "\n",
        "Over time you will learn the different and appropriate loss and optimizer functions for different scenarios. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "m8YQN1H41L-Y",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='sgd', loss='mean_squared_error')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5QyOUhFw1OUX"
      },
      "source": [
        "## Providing the Data\n",
        "\n",
        "Next up we'll feed in some data. In this case we are taking 6 xs and 6ys. You can see that the relationship between these is that y=2x-1, so where x = -1, y=-3 etc. etc. \n",
        "\n",
        "A python library called 'Numpy' provides lots of array type data structures that are a defacto standard way of doing it. We declare that we want to use these by specifying the values as an np.array[]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4Dxk4q-jzEy4",
        "colab": {}
      },
      "source": [
        "xs = np.array([-1.0,  0.0, 1.0, 2.0, 3.0, 4.0], dtype=float)\n",
        "ys = np.array([-3.0, -1.0, 1.0, 3.0, 5.0, 7.0], dtype=float)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "n_YcWRElnM_b"
      },
      "source": [
        "## Training the Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "c-Jk4dG91dvD"
      },
      "source": [
        "The process of training the neural network, where it 'learns' the relationship between the Xs and Ys is in the **model.fit**  call. This is where it will go through the loop we spoke about above, making a guess, measuring how good or bad it is (aka the loss), using the opimizer to make another guess etc. It will do it for the number of epochs you specify. When you run this code, you'll see the loss on the right hand side."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lpRrl7WK10Pq",
        "colab": {}
      },
      "source": [
        "model.fit(xs, ys, epochs=500)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kaFIr71H2OZ-"
      },
      "source": [
        "Ok, now you have a model that has been trained to learn the relationshop between X and Y. You can use the **model.predict** method to have it figure out the Y for a previously unknown X. So, for example, if X = 10, what do you think Y will be? Take a guess before you run this code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oxNzL4lS2Gui",
        "colab": {}
      },
      "source": [
        "print(model.predict([10.0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "btF2CSFH2iEX"
      },
      "source": [
        "You might have thought 19, right? But it ended up being a little under. Why do you think that is? \n",
        "\n",
        "Remember that neural networks deal with probabilities, so given the data that we fed the NN with, it calculated that there is a very high probability that the relationship between X and Y is Y=2X-1, but with only 6 data points we can't know for sure. As a result, the result for 10 is very close to 19, but not necessarily 19. \n",
        "\n",
        "As you work with neural networks, you'll see this pattern recurring. You will almost always deal with probabilities, not certainties, and will do a little bit of coding to figure out what the result is based on the probabilities, particularly when it comes to classification.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjUMSjcftOV5",
        "colab_type": "text"
      },
      "source": [
        "## In class exercise: understanding neural nets\n",
        "\n",
        "UUID - #S2E1\n",
        "\n",
        "In this exercise you'll try to build a neural network that predicts the price of a house according to a simple formula.\n",
        "\n",
        "So, imagine if house pricing was as easy as a house costs 50k + 50k per bedroom, so that a 1 bedroom house costs 100k, a 2 bedroom house costs 150k etc.\n",
        "\n",
        "How would you create a neural network that learns this relationship so that it would predict a 7 bedroom house as costing close to 400k etc.\n",
        "\n",
        "Hint: Your network might work better if you scale the house price down. You don't have to give the answer 400...it might be better to create something that predicts the number 4, and then your answer is in the 'hundreds of thousands' etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSCull1NtZ9e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# GRADED FUNCTION: house_model\n",
        "def house_model(y_new):\n",
        "    xs = #your code here\n",
        "    ys = #your code here\n",
        "    model = #your code here\n",
        "    model.compile(optimizer='sgd', loss='mean_squared_error')\n",
        "    model.fit(#your code here)\n",
        "    return model.predict(y_new)[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LgdgZUXtticF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prediction = house_model([7.0])\n",
        "print(prediction)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjXvbRexKKyW",
        "colab_type": "text"
      },
      "source": [
        "# Building an MLP Regressor\n",
        "\n",
        "UUID - #S2C2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KU029kLKCED",
        "colab_type": "text"
      },
      "source": [
        "Let's load, split and scale the California housing dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwBhgQ02KCEE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "housing = fetch_california_housing()\n",
        "\n",
        "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target, random_state=42)\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_valid = scaler.transform(X_valid)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PavMhzNKCEK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAIJi3vwKCET",
        "colab_type": "code",
        "outputId": "7009b4e8-4b79-43cb-dde0-d076d2ec5651",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        }
      },
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
        "    keras.layers.Dense(1)\n",
        "])\n",
        "model.compile(loss=\"mean_squared_error\", optimizer=keras.optimizers.SGD(lr=1e-3))\n",
        "history = model.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid))\n",
        "mse_test = model.evaluate(X_test, y_test)\n",
        "X_new = X_test[:3]\n",
        "y_pred = model.predict(X_new)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 1.6415 - val_loss: 0.8557\n",
            "Epoch 2/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.7047 - val_loss: 0.6528\n",
            "Epoch 3/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.6345 - val_loss: 0.6097\n",
            "Epoch 4/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.5980 - val_loss: 0.5656\n",
            "Epoch 5/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.5707 - val_loss: 0.5353\n",
            "Epoch 6/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.5473 - val_loss: 0.5171\n",
            "Epoch 7/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.5288 - val_loss: 0.5079\n",
            "Epoch 8/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.5130 - val_loss: 0.4798\n",
            "Epoch 9/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4990 - val_loss: 0.4688\n",
            "Epoch 10/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4876 - val_loss: 0.4655\n",
            "Epoch 11/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4777 - val_loss: 0.4481\n",
            "Epoch 12/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4686 - val_loss: 0.4478\n",
            "Epoch 13/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4614 - val_loss: 0.4294\n",
            "Epoch 14/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4547 - val_loss: 0.4232\n",
            "Epoch 15/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4486 - val_loss: 0.4174\n",
            "Epoch 16/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4434 - val_loss: 0.4122\n",
            "Epoch 17/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4390 - val_loss: 0.4070\n",
            "Epoch 18/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4346 - val_loss: 0.4035\n",
            "Epoch 19/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4305 - val_loss: 0.3999\n",
            "Epoch 20/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4274 - val_loss: 0.3968\n",
            "162/162 [==============================] - 0s 770us/step - loss: 0.4205\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_9Ce1b7KCEX",
        "colab_type": "code",
        "outputId": "15bcdf42-1114-4b72-fc19-b054b80e8b40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        }
      },
      "source": [
        "plt.plot(pd.DataFrame(history.history))\n",
        "plt.grid(True)\n",
        "plt.gca().set_ylim(0, 1)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEACAYAAABI5zaHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rppDoHtKKCEe",
        "colab_type": "code",
        "outputId": "69c3d0b5-f74b-4870-98b3-190bb521adff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "y_pred"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.38856643],\n",
              "       [1.6792021 ],\n",
              "       [3.1022794 ]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1Xo3zyROXA7",
        "colab_type": "text"
      },
      "source": [
        "# The Functional API\n",
        "\n",
        "UUID - #S2C3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNqT32NUKCEj",
        "colab_type": "text"
      },
      "source": [
        "Not all neural network models are simply sequential. Some may have complex topologies. Some may have multiple inputs and/or multiple outputs. For example, a Wide & Deep neural network (see [paper](https://ai.google/research/pubs/pub45413)) connects all or part of the inputs directly to the output layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jzP05_W6KCEj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b27EyWVZKCEm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_ = keras.layers.Input(shape=X_train.shape[1:])\n",
        "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_)\n",
        "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
        "concat = keras.layers.concatenate([input_, hidden2])\n",
        "output = keras.layers.Dense(1)(concat)\n",
        "model = keras.models.Model(inputs=[input_], outputs=[output])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6LG8jeeKCEp",
        "colab_type": "code",
        "outputId": "3d3a9fb2-5ddb-470f-a1b7-272877fbcaf1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 8)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 30)           270         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 30)           930         dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 38)           0           input_1[0][0]                    \n",
            "                                                                 dense_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 1)            39          concatenate[0][0]                \n",
            "==================================================================================================\n",
            "Total params: 1,239\n",
            "Trainable params: 1,239\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3o3GA9bQYm_n",
        "colab_type": "code",
        "outputId": "5bab629d-d453-4647-8345-602dbc3bb96a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 533
        }
      },
      "source": [
        "keras.utils.plot_model(model, \"my_nonseq_model.png\", show_shapes=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAc0AAAIECAIAAAD0KOKrAAAABmJLR0QA/wD/AP+gvaeTAAAgAElE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\n5II6CwAA5II6CwAA5Pp/nmOasIUwaX8AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_FHtDMV9KCEw",
        "colab_type": "code",
        "outputId": "5310f97b-b814-437f-cb82-c65dfb1483f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        }
      },
      "source": [
        "model.compile(loss=\"mean_squared_error\", optimizer=keras.optimizers.SGD(lr=1e-3))\n",
        "history = model.fit(X_train, y_train, epochs=20,\n",
        "                    validation_data=(X_valid, y_valid))\n",
        "mse_test = model.evaluate(X_test, y_test)\n",
        "y_pred = model.predict(X_new)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 1.2608 - val_loss: 3.3923\n",
            "Epoch 2/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.6580 - val_loss: 0.9356\n",
            "Epoch 3/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.5878 - val_loss: 0.5647\n",
            "Epoch 4/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.5585 - val_loss: 0.5710\n",
            "Epoch 5/20\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.5349 - val_loss: 0.5043\n",
            "Epoch 6/20\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.5158 - val_loss: 0.4830\n",
            "Epoch 7/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.5003 - val_loss: 0.4638\n",
            "Epoch 8/20\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.4876 - val_loss: 0.4636\n",
            "Epoch 9/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4759 - val_loss: 0.4420\n",
            "Epoch 10/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4659 - val_loss: 0.4311\n",
            "Epoch 11/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4576 - val_loss: 0.4344\n",
            "Epoch 12/20\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.4497 - val_loss: 0.4166\n",
            "Epoch 13/20\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.4427 - val_loss: 0.4228\n",
            "Epoch 14/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4365 - val_loss: 0.4046\n",
            "Epoch 15/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4306 - val_loss: 0.4077\n",
            "Epoch 16/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4256 - val_loss: 0.3937\n",
            "Epoch 17/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4211 - val_loss: 0.3951\n",
            "Epoch 18/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4167 - val_loss: 0.3859\n",
            "Epoch 19/20\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.4120 - val_loss: 0.3825\n",
            "Epoch 20/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4089 - val_loss: 0.4052\n",
            "162/162 [==============================] - 0s 917us/step - loss: 0.4025\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CYLD3oDdAOi",
        "colab_type": "text"
      },
      "source": [
        "## Multiple Inputs\n",
        "\n",
        "UUID - #S2C4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySm5Ic3vKCEz",
        "colab_type": "text"
      },
      "source": [
        "What if you want to send different subsets of input features through the wide or deep paths? We will send 5 features (features 0 to 4), and 6 through the deep path (features 2 to 7). Note that 3 features will go through both (features 2, 3 and 4)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcwkFaD3KCE0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jio0Zx2UKCE3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_A = keras.layers.Input(shape=[5], name=\"wide_input\")\n",
        "input_B = keras.layers.Input(shape=[6], name=\"deep_input\")\n",
        "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_B)\n",
        "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
        "concat = keras.layers.concatenate([input_A, hidden2])\n",
        "output = keras.layers.Dense(1, name=\"output\")(concat)\n",
        "model = keras.models.Model(inputs=[input_A, input_B], outputs=[output])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrOGHlx6fCUl",
        "colab_type": "code",
        "outputId": "e6b5cb19-2a8d-4a50-ad87-44fe6222b0f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 533
        }
      },
      "source": [
        "keras.utils.plot_model(model, \"my_multipleinput_model.png\", show_shapes=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsoAAAIECAIAAACPFp/TAAAABmJLR0QA/wD/AP+gvaeTAAAgAElE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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VR-PjBuNKCE5",
        "colab_type": "code",
        "outputId": "a674d0cc-9aef-444e-e18e-ec40194baded",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        }
      },
      "source": [
        "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(lr=1e-3))\n",
        "\n",
        "X_train_A, X_train_B = X_train[:, :5], X_train[:, 2:]\n",
        "X_valid_A, X_valid_B = X_valid[:, :5], X_valid[:, 2:]\n",
        "X_test_A, X_test_B = X_test[:, :5], X_test[:, 2:]\n",
        "X_new_A, X_new_B = X_test_A[:3], X_test_B[:3]\n",
        "\n",
        "history = model.fit((X_train_A, X_train_B), y_train, epochs=20,\n",
        "                    validation_data=((X_valid_A, X_valid_B), y_valid))\n",
        "mse_test = model.evaluate((X_test_A, X_test_B), y_test)\n",
        "y_pred = model.predict((X_new_A, X_new_B))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 1.8140 - val_loss: 0.8070\n",
            "Epoch 2/20\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.6771 - val_loss: 0.6656\n",
            "Epoch 3/20\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.5978 - val_loss: 0.5686\n",
            "Epoch 4/20\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.5587 - val_loss: 0.5295\n",
            "Epoch 5/20\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.5336 - val_loss: 0.4991\n",
            "Epoch 6/20\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.5121 - val_loss: 0.4809\n",
            "Epoch 7/20\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.4971 - val_loss: 0.4694\n",
            "Epoch 8/20\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.4843 - val_loss: 0.4494\n",
            "Epoch 9/20\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.4729 - val_loss: 0.4403\n",
            "Epoch 10/20\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.4644 - val_loss: 0.4314\n",
            "Epoch 11/20\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.4569 - val_loss: 0.4266\n",
            "Epoch 12/20\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.4509 - val_loss: 0.4165\n",
            "Epoch 13/20\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.4461 - val_loss: 0.4124\n",
            "Epoch 14/20\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.4420 - val_loss: 0.4073\n",
            "Epoch 15/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4384 - val_loss: 0.4043\n",
            "Epoch 16/20\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.4355 - val_loss: 0.4006\n",
            "Epoch 17/20\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.4324 - val_loss: 0.4012\n",
            "Epoch 18/20\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.4305 - val_loss: 0.3986\n",
            "Epoch 19/20\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.4273 - val_loss: 0.3933\n",
            "Epoch 20/20\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.4262 - val_loss: 0.4203\n",
            "162/162 [==============================] - 0s 995us/step - loss: 0.4216\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezrjpS7mh7gE",
        "colab_type": "text"
      },
      "source": [
        "## Multiple outputs for regularization\n",
        "\n",
        "UUID -#S2C5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDzQ-c0GKCE9",
        "colab_type": "text"
      },
      "source": [
        "Adding an auxiliary output for regularization:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lbZCGZZKCE9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWPGIbQjKCE_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_A = keras.layers.Input(shape=[5], name=\"wide_input\")\n",
        "input_B = keras.layers.Input(shape=[6], name=\"deep_input\")\n",
        "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_B)\n",
        "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
        "concat = keras.layers.concatenate([input_A, hidden2])\n",
        "output = keras.layers.Dense(1, name=\"main_output\")(concat)\n",
        "aux_output = keras.layers.Dense(1, name=\"aux_output\")(hidden2)\n",
        "model = keras.models.Model(inputs=[input_A, input_B],\n",
        "                           outputs=[output, aux_output])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGFOFsUJKCFD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss=[\"mse\", \"mse\"], loss_weights=[0.9, 0.1], optimizer=keras.optimizers.SGD(lr=1e-3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXzWqc6jKCFI",
        "colab_type": "code",
        "outputId": "a9b72dab-e896-470e-cca3-06aefd4eff4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        }
      },
      "source": [
        "history = model.fit([X_train_A, X_train_B], [y_train, y_train], epochs=20,\n",
        "                    validation_data=([X_valid_A, X_valid_B], [y_valid, y_valid]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 2.0979 - main_output_loss: 1.8516 - aux_output_loss: 4.3146 - val_loss: 1.5426 - val_main_output_loss: 0.9142 - val_aux_output_loss: 7.1981\n",
            "Epoch 2/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.9781 - main_output_loss: 0.7778 - aux_output_loss: 2.7808 - val_loss: 1.3113 - val_main_output_loss: 0.6823 - val_aux_output_loss: 6.9728\n",
            "Epoch 3/20\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.8064 - main_output_loss: 0.6684 - aux_output_loss: 2.0487 - val_loss: 1.2618 - val_main_output_loss: 0.6457 - val_aux_output_loss: 6.8068\n",
            "Epoch 4/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.7201 - main_output_loss: 0.6118 - aux_output_loss: 1.6954 - val_loss: 1.2018 - val_main_output_loss: 0.6134 - val_aux_output_loss: 6.4974\n",
            "Epoch 5/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.6702 - main_output_loss: 0.5758 - aux_output_loss: 1.5196 - val_loss: 1.1391 - val_main_output_loss: 0.5935 - val_aux_output_loss: 6.0494\n",
            "Epoch 6/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.6353 - main_output_loss: 0.5481 - aux_output_loss: 1.4202 - val_loss: 1.0776 - val_main_output_loss: 0.5754 - val_aux_output_loss: 5.5971\n",
            "Epoch 7/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.6104 - main_output_loss: 0.5276 - aux_output_loss: 1.3560 - val_loss: 1.0276 - val_main_output_loss: 0.5734 - val_aux_output_loss: 5.1150\n",
            "Epoch 8/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.5898 - main_output_loss: 0.5102 - aux_output_loss: 1.3063 - val_loss: 0.9415 - val_main_output_loss: 0.5301 - val_aux_output_loss: 4.6439\n",
            "Epoch 9/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.5721 - main_output_loss: 0.4950 - aux_output_loss: 1.2662 - val_loss: 0.8705 - val_main_output_loss: 0.4976 - val_aux_output_loss: 4.2259\n",
            "Epoch 10/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.5572 - main_output_loss: 0.4824 - aux_output_loss: 1.2304 - val_loss: 0.8192 - val_main_output_loss: 0.4840 - val_aux_output_loss: 3.8362\n",
            "Epoch 11/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.5438 - main_output_loss: 0.4713 - aux_output_loss: 1.1967 - val_loss: 0.7534 - val_main_output_loss: 0.4493 - val_aux_output_loss: 3.4908\n",
            "Epoch 12/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.5340 - main_output_loss: 0.4639 - aux_output_loss: 1.1653 - val_loss: 0.7158 - val_main_output_loss: 0.4404 - val_aux_output_loss: 3.1938\n",
            "Epoch 13/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.5259 - main_output_loss: 0.4578 - aux_output_loss: 1.1391 - val_loss: 0.6764 - val_main_output_loss: 0.4276 - val_aux_output_loss: 2.9155\n",
            "Epoch 14/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.5196 - main_output_loss: 0.4534 - aux_output_loss: 1.1153 - val_loss: 0.6481 - val_main_output_loss: 0.4203 - val_aux_output_loss: 2.6977\n",
            "Epoch 15/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.5137 - main_output_loss: 0.4493 - aux_output_loss: 1.0938 - val_loss: 0.6233 - val_main_output_loss: 0.4133 - val_aux_output_loss: 2.5137\n",
            "Epoch 16/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.5086 - main_output_loss: 0.4458 - aux_output_loss: 1.0735 - val_loss: 0.6046 - val_main_output_loss: 0.4095 - val_aux_output_loss: 2.3598\n",
            "Epoch 17/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.5037 - main_output_loss: 0.4425 - aux_output_loss: 1.0548 - val_loss: 0.5918 - val_main_output_loss: 0.4065 - val_aux_output_loss: 2.2598\n",
            "Epoch 18/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.5000 - main_output_loss: 0.4403 - aux_output_loss: 1.0365 - val_loss: 0.5789 - val_main_output_loss: 0.4036 - val_aux_output_loss: 2.1563\n",
            "Epoch 19/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4956 - main_output_loss: 0.4374 - aux_output_loss: 1.0191 - val_loss: 0.5686 - val_main_output_loss: 0.4016 - val_aux_output_loss: 2.0716\n",
            "Epoch 20/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4920 - main_output_loss: 0.4353 - aux_output_loss: 1.0029 - val_loss: 0.5609 - val_main_output_loss: 0.4012 - val_aux_output_loss: 1.9982\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdjJ5qQZKCFK",
        "colab_type": "code",
        "outputId": "4da4a3d4-2baa-43f9-faf3-0f4833d0b927",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "total_loss, main_loss, aux_loss = model.evaluate(\n",
        "    [X_test_A, X_test_B], [y_test, y_test])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "162/162 [==============================] - 0s 974us/step - loss: 0.4825 - main_output_loss: 0.4276 - aux_output_loss: 0.9765\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GV91ZIgkKG6",
        "colab_type": "code",
        "outputId": "3e56b641-872b-453b-acc5-ce5c962903b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "y_pred_main, y_pred_aux = model.predict([X_new_A, X_new_B])\n",
        "print(y_pred_main, y_pred_aux)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.32896826]\n",
            " [1.8843658 ]\n",
            " [3.3908076 ]] [[1.314269 ]\n",
            " [1.9748342]\n",
            " [2.62874  ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQtIQYNuKCFa",
        "colab_type": "text"
      },
      "source": [
        "# Saving and Restoring\n",
        "\n",
        "UUID - #S2C6"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcI005eXKCFb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WC9oMrOwKCFf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.Dense(30, activation=\"relu\", input_shape=[8]),\n",
        "    keras.layers.Dense(30, activation=\"relu\"),\n",
        "    keras.layers.Dense(1)\n",
        "])    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "heA3bDCsKCFk",
        "colab_type": "code",
        "outputId": "07882593-5be0-4f4b-e75b-5c6bedf87a7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(lr=1e-3))\n",
        "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid))\n",
        "mse_test = model.evaluate(X_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 1.8861 - val_loss: 0.7125\n",
            "Epoch 2/10\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.6577 - val_loss: 0.6878\n",
            "Epoch 3/10\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.5934 - val_loss: 0.5802\n",
            "Epoch 4/10\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.5560 - val_loss: 0.5165\n",
            "Epoch 5/10\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.5272 - val_loss: 0.4894\n",
            "Epoch 6/10\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.5033 - val_loss: 0.4949\n",
            "Epoch 7/10\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4854 - val_loss: 0.4860\n",
            "Epoch 8/10\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4709 - val_loss: 0.4552\n",
            "Epoch 9/10\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4577 - val_loss: 0.4412\n",
            "Epoch 10/10\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4475 - val_loss: 0.4377\n",
            "162/162 [==============================] - 0s 825us/step - loss: 0.4376\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLVN5jlLKCFn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save(\"my_keras_model.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nrBYB5jsKCFp",
        "colab_type": "code",
        "outputId": "d1acbefc-57aa-4cb2-e5e4-4ebb16b722ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "model = keras.models.load_model(\"my_keras_model.h5\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rpyqs8cYKCFr",
        "colab_type": "code",
        "outputId": "2af6d97d-1888-4154-ef33-106b2d05d6ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "model.predict(X_new)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.5400236],\n",
              "       [1.650597 ],\n",
              "       [3.009824 ]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkLkIVelKCFv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save_weights(\"my_keras_weights.ckpt\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBiy8wFfKCFw",
        "colab_type": "code",
        "outputId": "a98f916c-e923-490c-ede2-98efb280f493",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model.load_weights(\"my_keras_weights.ckpt\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f3cd9188dd8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "theeogfVvY1D",
        "colab_type": "text"
      },
      "source": [
        "## In class exercise: sharing your model\n",
        "\n",
        "UUID - #S2E2\n",
        "\n",
        "Train the housing dataset with multi input features:\n",
        "\n",
        "\n",
        "\n",
        "*   8 passing throught the deep part\n",
        "*   3 passing through the wide part\n",
        "\n",
        "Predict over some new dataset. \n",
        "\n",
        "Once the model is finished, you will be asked to share the model with a colleague in the form of a h5 file. Your colleague should get about the same predictions over the same new dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDVptXXnKCFz",
        "colab_type": "text"
      },
      "source": [
        "# Using Callbacks during Training\n",
        "\n",
        "UUID - #S2C8"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tc9-RTPXKCF0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "keras.backend.clear_session()\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5Sp_kGjKCF3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.Dense(30, activation=\"relu\", input_shape=[8]),\n",
        "    keras.layers.Dense(30, activation=\"relu\"),\n",
        "    keras.layers.Dense(1)\n",
        "])    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zQCks5LKCF8",
        "colab_type": "code",
        "outputId": "f01795d4-2c72-4f89-bfb5-65102549008d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        }
      },
      "source": [
        "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(lr=1e-3))\n",
        "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_keras_model.h5\", save_best_only=True)\n",
        "history = model.fit(X_train, y_train, epochs=10,\n",
        "                    validation_data=(X_valid, y_valid),\n",
        "                    callbacks=[checkpoint_cb])\n",
        "model = keras.models.load_model(\"my_keras_model.h5\") # rollback to best model\n",
        "mse_test = model.evaluate(X_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 1.8861 - val_loss: 0.7125\n",
            "Epoch 2/10\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.6577 - val_loss: 0.6878\n",
            "Epoch 3/10\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.5934 - val_loss: 0.5802\n",
            "Epoch 4/10\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.5560 - val_loss: 0.5165\n",
            "Epoch 5/10\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.5272 - val_loss: 0.4894\n",
            "Epoch 6/10\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.5033 - val_loss: 0.4949\n",
            "Epoch 7/10\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4854 - val_loss: 0.4860\n",
            "Epoch 8/10\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.4709 - val_loss: 0.4552\n",
            "Epoch 9/10\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.4577 - val_loss: 0.4412\n",
            "Epoch 10/10\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.4475 - val_loss: 0.4377\n",
            "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
            "162/162 [==============================] - 0s 919us/step - loss: 0.4376\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okXzX5M-KCGC",
        "colab_type": "code",
        "outputId": "7f300dc7-087e-431a-c984-b8ea9d3cf455",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(lr=1e-3))\n",
        "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10,\n",
        "                                                  restore_best_weights=True)\n",
        "history = model.fit(X_train, y_train, epochs=100,\n",
        "                    validation_data=(X_valid, y_valid),\n",
        "                    callbacks=[checkpoint_cb, early_stopping_cb])\n",
        "mse_test = model.evaluate(X_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4394 - val_loss: 0.4108\n",
            "Epoch 2/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.4315 - val_loss: 0.4265\n",
            "Epoch 3/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.4259 - val_loss: 0.3995\n",
            "Epoch 4/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.4202 - val_loss: 0.3938\n",
            "Epoch 5/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.4155 - val_loss: 0.3888\n",
            "Epoch 6/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.4112 - val_loss: 0.3865\n",
            "Epoch 7/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.4074 - val_loss: 0.3859\n",
            "Epoch 8/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4040 - val_loss: 0.3792\n",
            "Epoch 9/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.4006 - val_loss: 0.3745\n",
            "Epoch 10/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3976 - val_loss: 0.3722\n",
            "Epoch 11/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3950 - val_loss: 0.3696\n",
            "Epoch 12/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3923 - val_loss: 0.3668\n",
            "Epoch 13/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3897 - val_loss: 0.3660\n",
            "Epoch 14/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3873 - val_loss: 0.3630\n",
            "Epoch 15/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3850 - val_loss: 0.3659\n",
            "Epoch 16/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3829 - val_loss: 0.3624\n",
            "Epoch 17/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3811 - val_loss: 0.3591\n",
            "Epoch 18/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3787 - val_loss: 0.3562\n",
            "Epoch 19/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3765 - val_loss: 0.3534\n",
            "Epoch 20/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3751 - val_loss: 0.3708\n",
            "Epoch 21/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3732 - val_loss: 0.3511\n",
            "Epoch 22/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3718 - val_loss: 0.3698\n",
            "Epoch 23/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3700 - val_loss: 0.3475\n",
            "Epoch 24/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3685 - val_loss: 0.3560\n",
            "Epoch 25/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3671 - val_loss: 0.3526\n",
            "Epoch 26/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3657 - val_loss: 0.3700\n",
            "Epoch 27/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3648 - val_loss: 0.3431\n",
            "Epoch 28/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3635 - val_loss: 0.3591\n",
            "Epoch 29/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3625 - val_loss: 0.3520\n",
            "Epoch 30/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3612 - val_loss: 0.3625\n",
            "Epoch 31/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3600 - val_loss: 0.3430\n",
            "Epoch 32/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3590 - val_loss: 0.3765\n",
            "Epoch 33/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3583 - val_loss: 0.3373\n",
            "Epoch 34/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3575 - val_loss: 0.3406\n",
            "Epoch 35/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3563 - val_loss: 0.3613\n",
            "Epoch 36/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3554 - val_loss: 0.3348\n",
            "Epoch 37/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3547 - val_loss: 0.3572\n",
            "Epoch 38/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3538 - val_loss: 0.3366\n",
            "Epoch 39/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3530 - val_loss: 0.3424\n",
            "Epoch 40/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3523 - val_loss: 0.3368\n",
            "Epoch 41/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3515 - val_loss: 0.3513\n",
            "Epoch 42/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3510 - val_loss: 0.3426\n",
            "Epoch 43/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3504 - val_loss: 0.3677\n",
            "Epoch 44/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3496 - val_loss: 0.3562\n",
            "Epoch 45/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3491 - val_loss: 0.3335\n",
            "Epoch 46/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3480 - val_loss: 0.3456\n",
            "Epoch 47/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3479 - val_loss: 0.3432\n",
            "Epoch 48/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3470 - val_loss: 0.3657\n",
            "Epoch 49/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3466 - val_loss: 0.3285\n",
            "Epoch 50/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3458 - val_loss: 0.3267\n",
            "Epoch 51/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3453 - val_loss: 0.3437\n",
            "Epoch 52/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3449 - val_loss: 0.3262\n",
            "Epoch 53/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3445 - val_loss: 0.3908\n",
            "Epoch 54/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3439 - val_loss: 0.3274\n",
            "Epoch 55/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3435 - val_loss: 0.3559\n",
            "Epoch 56/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3429 - val_loss: 0.3236\n",
            "Epoch 57/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3423 - val_loss: 0.3241\n",
            "Epoch 58/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3423 - val_loss: 0.3763\n",
            "Epoch 59/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3417 - val_loss: 0.3289\n",
            "Epoch 60/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3409 - val_loss: 0.3501\n",
            "Epoch 61/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3403 - val_loss: 0.3456\n",
            "Epoch 62/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3402 - val_loss: 0.3444\n",
            "Epoch 63/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3392 - val_loss: 0.3289\n",
            "Epoch 64/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3393 - val_loss: 0.3216\n",
            "Epoch 65/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3386 - val_loss: 0.3350\n",
            "Epoch 66/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3383 - val_loss: 0.3231\n",
            "Epoch 67/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3376 - val_loss: 0.3567\n",
            "Epoch 68/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3374 - val_loss: 0.3256\n",
            "Epoch 69/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3370 - val_loss: 0.3348\n",
            "Epoch 70/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3365 - val_loss: 0.3559\n",
            "Epoch 71/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3361 - val_loss: 0.3580\n",
            "Epoch 72/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3358 - val_loss: 0.3287\n",
            "Epoch 73/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3350 - val_loss: 0.3202\n",
            "Epoch 74/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3350 - val_loss: 0.3839\n",
            "Epoch 75/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3347 - val_loss: 0.3233\n",
            "Epoch 76/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3342 - val_loss: 0.3474\n",
            "Epoch 77/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3338 - val_loss: 0.3408\n",
            "Epoch 78/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3334 - val_loss: 0.3460\n",
            "Epoch 79/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3331 - val_loss: 0.3347\n",
            "Epoch 80/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3328 - val_loss: 0.3354\n",
            "Epoch 81/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3323 - val_loss: 0.3275\n",
            "Epoch 82/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3319 - val_loss: 0.3166\n",
            "Epoch 83/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3316 - val_loss: 0.3278\n",
            "Epoch 84/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3313 - val_loss: 0.3636\n",
            "Epoch 85/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3310 - val_loss: 0.3174\n",
            "Epoch 86/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3308 - val_loss: 0.3155\n",
            "Epoch 87/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3304 - val_loss: 0.3528\n",
            "Epoch 88/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3299 - val_loss: 0.3255\n",
            "Epoch 89/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3293 - val_loss: 0.3629\n",
            "Epoch 90/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3296 - val_loss: 0.3382\n",
            "Epoch 91/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3291 - val_loss: 0.3211\n",
            "Epoch 92/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3289 - val_loss: 0.3457\n",
            "Epoch 93/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3284 - val_loss: 0.3158\n",
            "Epoch 94/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3284 - val_loss: 0.3409\n",
            "Epoch 95/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3275 - val_loss: 0.3380\n",
            "Epoch 96/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3273 - val_loss: 0.3214\n",
            "162/162 [==============================] - 0s 846us/step - loss: 0.3300\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKvgNlelKCGF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PrintValTrainRatioCallback(keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs):\n",
        "        print(\"\\nval/train: {:.2f}\".format(logs[\"val_loss\"] / logs[\"loss\"]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irmdExMKKCGJ",
        "colab_type": "code",
        "outputId": "61f3d05e-d855-4126-a615-4d30b8299bf4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "val_train_ratio_cb = PrintValTrainRatioCallback()\n",
        "history = model.fit(X_train, y_train, epochs=1,\n",
        "                    validation_data=(X_valid, y_valid),\n",
        "                    callbacks=[val_train_ratio_cb])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "337/363 [==========================>...] - ETA: 0s - loss: 0.3290\n",
            "val/train: 1.08\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3303 - val_loss: 0.3560\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IdSlVLm-KCGs",
        "colab_type": "text"
      },
      "source": [
        "# Hyperparameter Tuning\n",
        "\n",
        "UUID - #S2C9"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBMTAaNNKCGs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "keras.backend.clear_session()\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3LJFFQuKCGu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(n_hidden=1, n_neurons=30, learning_rate=3e-3, input_shape=[8]):\n",
        "    model = keras.models.Sequential()\n",
        "    model.add(keras.layers.InputLayer(input_shape=input_shape))\n",
        "    for layer in range(n_hidden):\n",
        "        model.add(keras.layers.Dense(n_neurons, activation=\"relu\"))\n",
        "    model.add(keras.layers.Dense(1))\n",
        "    optimizer = keras.optimizers.SGD(lr=learning_rate)\n",
        "    model.compile(loss=\"mse\", optimizer=optimizer)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7YdUYKCHKCGw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qYA12MwKCGy",
        "colab_type": "code",
        "outputId": "0acc1853-f4aa-44f0-bb56-337f0b8f30cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "keras_reg.fit(X_train, y_train, epochs=100,\n",
        "              validation_data=(X_valid, y_valid),\n",
        "              callbacks=[keras.callbacks.EarlyStopping(patience=10)])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 1.0894 - val_loss: 20.7615\n",
            "Epoch 2/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.7605 - val_loss: 5.0241\n",
            "Epoch 3/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.5456 - val_loss: 0.5488\n",
            "Epoch 4/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4734 - val_loss: 0.4527\n",
            "Epoch 5/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4504 - val_loss: 0.4187\n",
            "Epoch 6/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4338 - val_loss: 0.4128\n",
            "Epoch 7/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4242 - val_loss: 0.4003\n",
            "Epoch 8/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4168 - val_loss: 0.3942\n",
            "Epoch 9/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4107 - val_loss: 0.3960\n",
            "Epoch 10/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4061 - val_loss: 0.4069\n",
            "Epoch 11/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4020 - val_loss: 0.3854\n",
            "Epoch 12/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3983 - val_loss: 0.4134\n",
            "Epoch 13/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3950 - val_loss: 0.3996\n",
            "Epoch 14/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3921 - val_loss: 0.3817\n",
            "Epoch 15/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3893 - val_loss: 0.3828\n",
            "Epoch 16/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3868 - val_loss: 0.3738\n",
            "Epoch 17/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3849 - val_loss: 0.4020\n",
            "Epoch 18/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3829 - val_loss: 0.3872\n",
            "Epoch 19/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3806 - val_loss: 0.3767\n",
            "Epoch 20/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3792 - val_loss: 0.4189\n",
            "Epoch 21/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3775 - val_loss: 0.3925\n",
            "Epoch 22/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3758 - val_loss: 0.4236\n",
            "Epoch 23/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3741 - val_loss: 0.3522\n",
            "Epoch 24/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3725 - val_loss: 0.3841\n",
            "Epoch 25/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3710 - val_loss: 0.4161\n",
            "Epoch 26/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3699 - val_loss: 0.3979\n",
            "Epoch 27/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3692 - val_loss: 0.3473\n",
            "Epoch 28/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3676 - val_loss: 0.3919\n",
            "Epoch 29/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3670 - val_loss: 0.3565\n",
            "Epoch 30/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3653 - val_loss: 0.4189\n",
            "Epoch 31/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3646 - val_loss: 0.3720\n",
            "Epoch 32/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3634 - val_loss: 0.3947\n",
            "Epoch 33/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3631 - val_loss: 0.3422\n",
            "Epoch 34/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3621 - val_loss: 0.3453\n",
            "Epoch 35/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3610 - val_loss: 0.4067\n",
            "Epoch 36/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3607 - val_loss: 0.3416\n",
            "Epoch 37/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3597 - val_loss: 0.3786\n",
            "Epoch 38/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3588 - val_loss: 0.3378\n",
            "Epoch 39/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3582 - val_loss: 0.3418\n",
            "Epoch 40/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3572 - val_loss: 0.3704\n",
            "Epoch 41/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3570 - val_loss: 0.3658\n",
            "Epoch 42/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3562 - val_loss: 0.3802\n",
            "Epoch 43/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3556 - val_loss: 0.3764\n",
            "Epoch 44/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3548 - val_loss: 0.3812\n",
            "Epoch 45/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3543 - val_loss: 0.3325\n",
            "Epoch 46/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3531 - val_loss: 0.3384\n",
            "Epoch 47/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3529 - val_loss: 0.3654\n",
            "Epoch 48/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3520 - val_loss: 0.3578\n",
            "Epoch 49/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3525 - val_loss: 0.3359\n",
            "Epoch 50/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3509 - val_loss: 0.3317\n",
            "Epoch 51/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3503 - val_loss: 0.3561\n",
            "Epoch 52/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3501 - val_loss: 0.3520\n",
            "Epoch 53/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3498 - val_loss: 0.4578\n",
            "Epoch 54/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3496 - val_loss: 0.3808\n",
            "Epoch 55/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3489 - val_loss: 0.3539\n",
            "Epoch 56/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3484 - val_loss: 0.3724\n",
            "Epoch 57/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3478 - val_loss: 0.3336\n",
            "Epoch 58/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3474 - val_loss: 0.4010\n",
            "Epoch 59/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3477 - val_loss: 0.3262\n",
            "Epoch 60/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3465 - val_loss: 0.3270\n",
            "Epoch 61/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3452 - val_loss: 0.3349\n",
            "Epoch 62/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3453 - val_loss: 0.3540\n",
            "Epoch 63/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3445 - val_loss: 0.3428\n",
            "Epoch 64/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3451 - val_loss: 0.3280\n",
            "Epoch 65/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3437 - val_loss: 0.3292\n",
            "Epoch 66/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3432 - val_loss: 0.3300\n",
            "Epoch 67/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3428 - val_loss: 0.3253\n",
            "Epoch 68/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3422 - val_loss: 0.3245\n",
            "Epoch 69/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3419 - val_loss: 0.3254\n",
            "Epoch 70/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3412 - val_loss: 0.3665\n",
            "Epoch 71/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3413 - val_loss: 0.3369\n",
            "Epoch 72/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3406 - val_loss: 0.3267\n",
            "Epoch 73/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3399 - val_loss: 0.3244\n",
            "Epoch 74/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3402 - val_loss: 0.3662\n",
            "Epoch 75/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3396 - val_loss: 0.3290\n",
            "Epoch 76/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3394 - val_loss: 0.3235\n",
            "Epoch 77/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3383 - val_loss: 0.3385\n",
            "Epoch 78/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3383 - val_loss: 0.3362\n",
            "Epoch 79/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3383 - val_loss: 0.3221\n",
            "Epoch 80/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3376 - val_loss: 0.3643\n",
            "Epoch 81/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3383 - val_loss: 0.3419\n",
            "Epoch 82/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3370 - val_loss: 0.3252\n",
            "Epoch 83/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3367 - val_loss: 0.3245\n",
            "Epoch 84/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3362 - val_loss: 0.3952\n",
            "Epoch 85/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3372 - val_loss: 0.3414\n",
            "Epoch 86/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3359 - val_loss: 0.3190\n",
            "Epoch 87/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3355 - val_loss: 0.3276\n",
            "Epoch 88/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3350 - val_loss: 0.3294\n",
            "Epoch 89/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3347 - val_loss: 0.3246\n",
            "Epoch 90/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3344 - val_loss: 0.3280\n",
            "Epoch 91/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3341 - val_loss: 0.3200\n",
            "Epoch 92/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3340 - val_loss: 0.3392\n",
            "Epoch 93/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3335 - val_loss: 0.3170\n",
            "Epoch 94/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3336 - val_loss: 0.3525\n",
            "Epoch 95/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3328 - val_loss: 0.4812\n",
            "Epoch 96/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3339 - val_loss: 0.3464\n",
            "Epoch 97/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3323 - val_loss: 0.4630\n",
            "Epoch 98/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3333 - val_loss: 0.6722\n",
            "Epoch 99/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3329 - val_loss: 0.5922\n",
            "Epoch 100/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3342 - val_loss: 0.5270\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f3cda244a58>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLgvpIl0KCG1",
        "colab_type": "code",
        "outputId": "cb5f0439-d5e4-49c4-c9aa-336d43f8519b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "mse_test = keras_reg.score(X_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "162/162 [==============================] - 0s 833us/step - loss: 0.3335\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pup6S2mlKCG2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = keras_reg.predict(X_new)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nd8onwYYKCG4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCm5sWszKCG6",
        "colab_type": "text"
      },
      "source": [
        "**Warning**: the following cell crashes at the end of training. This seems to be caused by [Keras issue #13586](https://github.com/keras-team/keras/issues/13586), which was triggered by a recent change in Scikit-Learn. [Pull Request #13598](https://github.com/keras-team/keras/pull/13598) seems to fix the issue, so this problem should be resolved soon."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ljXhK-KKCG6",
        "colab_type": "code",
        "outputId": "80334a9d-2c4a-4daa-8c55-585fa30aa95d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from scipy.stats import reciprocal\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "param_distribs = {\n",
        "    \"n_hidden\": [0, 1, 2],\n",
        "    \"n_neurons\": np.arange(1, 50),\n",
        "    \"learning_rate\": reciprocal(3e-4, 3e-2),\n",
        "}\n",
        "\n",
        "rnd_search_cv = RandomizedSearchCV(keras_reg, param_distribs, n_iter=10, cv=3, verbose=2)\n",
        "rnd_search_cv.fit(X_train, y_train, epochs=100,\n",
        "                  validation_data=(X_valid, y_valid),\n",
        "                  callbacks=[keras.callbacks.EarlyStopping(patience=10)])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
            "[CV] learning_rate=0.00037192261022352417, n_hidden=2, n_neurons=47 ..\n",
            "Epoch 1/100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "242/242 [==============================] - 0s 2ms/step - loss: 3.6671 - val_loss: 2.9353\n",
            "Epoch 2/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 1.7978 - val_loss: 2.1152\n",
            "Epoch 3/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 1.1464 - val_loss: 1.5584\n",
            "Epoch 4/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.8871 - val_loss: 1.0674\n",
            "Epoch 5/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.7715 - val_loss: 0.7901\n",
            "Epoch 6/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.7138 - val_loss: 0.6936\n",
            "Epoch 7/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.6816 - val_loss: 0.6321\n",
            "Epoch 8/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.6604 - val_loss: 0.6070\n",
            "Epoch 9/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.6453 - val_loss: 0.5917\n",
            "Epoch 10/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.6322 - val_loss: 0.5789\n",
            "Epoch 11/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.6209 - val_loss: 0.5682\n",
            "Epoch 12/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.6110 - val_loss: 0.5587\n",
            "Epoch 13/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.6008 - val_loss: 0.5502\n",
            "Epoch 14/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5919 - val_loss: 0.5415\n",
            "Epoch 15/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5828 - val_loss: 0.5330\n",
            "Epoch 16/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5741 - val_loss: 0.5252\n",
            "Epoch 17/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5656 - val_loss: 0.5175\n",
            "Epoch 18/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5575 - val_loss: 0.5103\n",
            "Epoch 19/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5495 - val_loss: 0.5033\n",
            "Epoch 20/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5420 - val_loss: 0.4966\n",
            "Epoch 21/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5347 - val_loss: 0.4897\n",
            "Epoch 22/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5274 - val_loss: 0.4832\n",
            "Epoch 23/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5205 - val_loss: 0.4769\n",
            "Epoch 24/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5137 - val_loss: 0.4717\n",
            "Epoch 25/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5077 - val_loss: 0.4651\n",
            "Epoch 26/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5015 - val_loss: 0.4597\n",
            "Epoch 27/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4955 - val_loss: 0.4543\n",
            "Epoch 28/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4898 - val_loss: 0.4494\n",
            "Epoch 29/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4843 - val_loss: 0.4447\n",
            "Epoch 30/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4794 - val_loss: 0.4397\n",
            "Epoch 31/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4741 - val_loss: 0.4350\n",
            "Epoch 32/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4693 - val_loss: 0.4312\n",
            "Epoch 33/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4648 - val_loss: 0.4273\n",
            "Epoch 34/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4602 - val_loss: 0.4236\n",
            "Epoch 35/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4561 - val_loss: 0.4209\n",
            "Epoch 36/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4521 - val_loss: 0.4169\n",
            "Epoch 37/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4482 - val_loss: 0.4157\n",
            "Epoch 38/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4446 - val_loss: 0.4111\n",
            "Epoch 39/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4411 - val_loss: 0.4094\n",
            "Epoch 40/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4377 - val_loss: 0.4084\n",
            "Epoch 41/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4345 - val_loss: 0.4053\n",
            "Epoch 42/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4317 - val_loss: 0.4034\n",
            "Epoch 43/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4287 - val_loss: 0.4007\n",
            "Epoch 44/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4259 - val_loss: 0.4023\n",
            "Epoch 45/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4235 - val_loss: 0.3975\n",
            "Epoch 46/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4210 - val_loss: 0.3950\n",
            "Epoch 47/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4187 - val_loss: 0.3971\n",
            "Epoch 48/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4163 - val_loss: 0.3931\n",
            "Epoch 49/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4142 - val_loss: 0.3930\n",
            "Epoch 50/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4123 - val_loss: 0.3916\n",
            "Epoch 51/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4107 - val_loss: 0.3882\n",
            "Epoch 52/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4085 - val_loss: 0.3908\n",
            "Epoch 53/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4067 - val_loss: 0.3909\n",
            "Epoch 54/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4051 - val_loss: 0.3863\n",
            "Epoch 55/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4034 - val_loss: 0.3892\n",
            "Epoch 56/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4020 - val_loss: 0.3906\n",
            "Epoch 57/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4007 - val_loss: 0.3880\n",
            "Epoch 58/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3992 - val_loss: 0.3917\n",
            "Epoch 59/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3980 - val_loss: 0.3858\n",
            "Epoch 60/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3968 - val_loss: 0.3855\n",
            "Epoch 61/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3958 - val_loss: 0.3871\n",
            "Epoch 62/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3945 - val_loss: 0.3908\n",
            "Epoch 63/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3932 - val_loss: 0.3843\n",
            "Epoch 64/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3924 - val_loss: 0.3906\n",
            "Epoch 65/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3913 - val_loss: 0.3941\n",
            "Epoch 66/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3903 - val_loss: 0.3864\n",
            "Epoch 67/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3892 - val_loss: 0.3863\n",
            "Epoch 68/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3883 - val_loss: 0.3898\n",
            "Epoch 69/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3874 - val_loss: 0.3884\n",
            "Epoch 70/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3866 - val_loss: 0.3859\n",
            "Epoch 71/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3859 - val_loss: 0.3891\n",
            "Epoch 72/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3850 - val_loss: 0.3896\n",
            "Epoch 73/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3844 - val_loss: 0.3824\n",
            "Epoch 74/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3834 - val_loss: 0.3893\n",
            "Epoch 75/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3830 - val_loss: 0.3886\n",
            "Epoch 76/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3818 - val_loss: 0.3875\n",
            "Epoch 77/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3813 - val_loss: 0.3805\n",
            "Epoch 78/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3806 - val_loss: 0.3880\n",
            "Epoch 79/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3800 - val_loss: 0.3878\n",
            "Epoch 80/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3792 - val_loss: 0.3815\n",
            "Epoch 81/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3785 - val_loss: 0.3907\n",
            "Epoch 82/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3779 - val_loss: 0.3951\n",
            "Epoch 83/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3774 - val_loss: 0.3869\n",
            "Epoch 84/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3766 - val_loss: 0.3889\n",
            "Epoch 85/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3762 - val_loss: 0.3818\n",
            "Epoch 86/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3754 - val_loss: 0.3936\n",
            "Epoch 87/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3751 - val_loss: 0.3798\n",
            "Epoch 88/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3748 - val_loss: 0.3762\n",
            "Epoch 89/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3741 - val_loss: 0.3801\n",
            "Epoch 90/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3735 - val_loss: 0.3875\n",
            "Epoch 91/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3733 - val_loss: 0.3792\n",
            "Epoch 92/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3726 - val_loss: 0.3776\n",
            "Epoch 93/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3720 - val_loss: 0.3811\n",
            "Epoch 94/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3716 - val_loss: 0.3806\n",
            "Epoch 95/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3712 - val_loss: 0.3777\n",
            "Epoch 96/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3707 - val_loss: 0.3773\n",
            "Epoch 97/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3702 - val_loss: 0.3714\n",
            "Epoch 98/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3698 - val_loss: 0.3704\n",
            "Epoch 99/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3693 - val_loss: 0.3922\n",
            "Epoch 100/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3691 - val_loss: 0.3756\n",
            "121/121 [==============================] - 0s 906us/step - loss: 0.3833\n",
            "[CV]  learning_rate=0.00037192261022352417, n_hidden=2, n_neurons=47, total=  39.3s\n",
            "[CV] learning_rate=0.00037192261022352417, n_hidden=2, n_neurons=47 ..\n",
            "Epoch 1/100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   39.3s remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "242/242 [==============================] - 0s 2ms/step - loss: 3.8241 - val_loss: 4.9566\n",
            "Epoch 2/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 1.5946 - val_loss: 9.0648\n",
            "Epoch 3/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 1.0572 - val_loss: 9.6631\n",
            "Epoch 4/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.8935 - val_loss: 8.4994\n",
            "Epoch 5/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.8157 - val_loss: 7.0606\n",
            "Epoch 6/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.7702 - val_loss: 5.8696\n",
            "Epoch 7/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.7402 - val_loss: 4.8773\n",
            "Epoch 8/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.7178 - val_loss: 4.0528\n",
            "Epoch 9/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.6999 - val_loss: 3.4236\n",
            "Epoch 10/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.6843 - val_loss: 2.9124\n",
            "Epoch 11/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.6705 - val_loss: 2.5106\n",
            "Epoch 12/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.6582 - val_loss: 2.1210\n",
            "Epoch 13/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.6466 - val_loss: 1.8233\n",
            "Epoch 14/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.6359 - val_loss: 1.5961\n",
            "Epoch 15/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.6260 - val_loss: 1.3778\n",
            "Epoch 16/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.6162 - val_loss: 1.2141\n",
            "Epoch 17/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.6073 - val_loss: 1.0585\n",
            "Epoch 18/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5987 - val_loss: 0.9376\n",
            "Epoch 19/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5906 - val_loss: 0.8309\n",
            "Epoch 20/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5827 - val_loss: 0.7483\n",
            "Epoch 21/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5754 - val_loss: 0.6781\n",
            "Epoch 22/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5681 - val_loss: 0.6303\n",
            "Epoch 23/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5616 - val_loss: 0.5902\n",
            "Epoch 24/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5550 - val_loss: 0.5602\n",
            "Epoch 25/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5488 - val_loss: 0.5389\n",
            "Epoch 26/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5429 - val_loss: 0.5267\n",
            "Epoch 27/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5372 - val_loss: 0.5211\n",
            "Epoch 28/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5318 - val_loss: 0.5207\n",
            "Epoch 29/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5263 - val_loss: 0.5216\n",
            "Epoch 30/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5218 - val_loss: 0.5309\n",
            "Epoch 31/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5166 - val_loss: 0.5440\n",
            "Epoch 32/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5118 - val_loss: 0.5565\n",
            "Epoch 33/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5073 - val_loss: 0.5715\n",
            "Epoch 34/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5028 - val_loss: 0.5818\n",
            "Epoch 35/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4986 - val_loss: 0.5959\n",
            "Epoch 36/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4945 - val_loss: 0.6219\n",
            "Epoch 37/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4906 - val_loss: 0.6327\n",
            "Epoch 38/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4869 - val_loss: 0.6540\n",
            "121/121 [==============================] - 0s 924us/step - loss: 0.5121\n",
            "[CV]  learning_rate=0.00037192261022352417, n_hidden=2, n_neurons=47, total=  15.4s\n",
            "[CV] learning_rate=0.00037192261022352417, n_hidden=2, n_neurons=47 ..\n",
            "Epoch 1/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 3.3190 - val_loss: 8.0601\n",
            "Epoch 2/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 1.5453 - val_loss: 5.6398\n",
            "Epoch 3/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.9781 - val_loss: 2.7603\n",
            "Epoch 4/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.7859 - val_loss: 1.5512\n",
            "Epoch 5/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.7128 - val_loss: 1.0609\n",
            "Epoch 6/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.6771 - val_loss: 0.8529\n",
            "Epoch 7/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.6547 - val_loss: 0.7445\n",
            "Epoch 8/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.6377 - val_loss: 0.6633\n",
            "Epoch 9/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.6237 - val_loss: 0.6271\n",
            "Epoch 10/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.6119 - val_loss: 0.6014\n",
            "Epoch 11/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.6011 - val_loss: 0.5843\n",
            "Epoch 12/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5915 - val_loss: 0.5711\n",
            "Epoch 13/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5826 - val_loss: 0.5612\n",
            "Epoch 14/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5743 - val_loss: 0.5530\n",
            "Epoch 15/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5663 - val_loss: 0.5453\n",
            "Epoch 16/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5588 - val_loss: 0.5379\n",
            "Epoch 17/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5519 - val_loss: 0.5311\n",
            "Epoch 18/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5451 - val_loss: 0.5249\n",
            "Epoch 19/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5389 - val_loss: 0.5192\n",
            "Epoch 20/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5329 - val_loss: 0.5128\n",
            "Epoch 21/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5271 - val_loss: 0.5069\n",
            "Epoch 22/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5214 - val_loss: 0.5015\n",
            "Epoch 23/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5163 - val_loss: 0.4956\n",
            "Epoch 24/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5109 - val_loss: 0.4898\n",
            "Epoch 25/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5062 - val_loss: 0.4850\n",
            "Epoch 26/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5014 - val_loss: 0.4802\n",
            "Epoch 27/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4969 - val_loss: 0.4757\n",
            "Epoch 28/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4929 - val_loss: 0.4710\n",
            "Epoch 29/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4884 - val_loss: 0.4667\n",
            "Epoch 30/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4848 - val_loss: 0.4623\n",
            "Epoch 31/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4807 - val_loss: 0.4587\n",
            "Epoch 32/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4769 - val_loss: 0.4547\n",
            "Epoch 33/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4734 - val_loss: 0.4510\n",
            "Epoch 34/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4699 - val_loss: 0.4479\n",
            "Epoch 35/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4666 - val_loss: 0.4443\n",
            "Epoch 36/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4634 - val_loss: 0.4409\n",
            "Epoch 37/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4607 - val_loss: 0.4379\n",
            "Epoch 38/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4575 - val_loss: 0.4348\n",
            "Epoch 39/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4545 - val_loss: 0.4321\n",
            "Epoch 40/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4519 - val_loss: 0.4293\n",
            "Epoch 41/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4491 - val_loss: 0.4281\n",
            "Epoch 42/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4467 - val_loss: 0.4251\n",
            "Epoch 43/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4445 - val_loss: 0.4221\n",
            "Epoch 44/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4420 - val_loss: 0.4203\n",
            "Epoch 45/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4396 - val_loss: 0.4181\n",
            "Epoch 46/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4374 - val_loss: 0.4154\n",
            "Epoch 47/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4353 - val_loss: 0.4141\n",
            "Epoch 48/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4334 - val_loss: 0.4121\n",
            "Epoch 49/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4315 - val_loss: 0.4104\n",
            "Epoch 50/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4296 - val_loss: 0.4093\n",
            "Epoch 51/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4276 - val_loss: 0.4075\n",
            "Epoch 52/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4260 - val_loss: 0.4066\n",
            "Epoch 53/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4243 - val_loss: 0.4047\n",
            "Epoch 54/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4228 - val_loss: 0.4029\n",
            "Epoch 55/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4209 - val_loss: 0.4038\n",
            "Epoch 56/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4195 - val_loss: 0.4034\n",
            "Epoch 57/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4181 - val_loss: 0.4021\n",
            "Epoch 58/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4166 - val_loss: 0.4020\n",
            "Epoch 59/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4153 - val_loss: 0.4012\n",
            "Epoch 60/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4137 - val_loss: 0.3991\n",
            "Epoch 61/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4125 - val_loss: 0.4026\n",
            "Epoch 62/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4113 - val_loss: 0.4023\n",
            "Epoch 63/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4101 - val_loss: 0.3996\n",
            "Epoch 64/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4089 - val_loss: 0.3981\n",
            "Epoch 65/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4078 - val_loss: 0.3986\n",
            "Epoch 66/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4065 - val_loss: 0.4016\n",
            "Epoch 67/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4056 - val_loss: 0.3967\n",
            "Epoch 68/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4045 - val_loss: 0.3984\n",
            "Epoch 69/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4035 - val_loss: 0.3963\n",
            "Epoch 70/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4025 - val_loss: 0.3978\n",
            "Epoch 71/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4013 - val_loss: 0.3941\n",
            "Epoch 72/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4005 - val_loss: 0.3969\n",
            "Epoch 73/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3993 - val_loss: 0.3930\n",
            "Epoch 74/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3985 - val_loss: 0.3956\n",
            "Epoch 75/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3978 - val_loss: 0.3976\n",
            "Epoch 76/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3966 - val_loss: 0.3986\n",
            "Epoch 77/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3957 - val_loss: 0.3956\n",
            "Epoch 78/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3952 - val_loss: 0.3940\n",
            "Epoch 79/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3942 - val_loss: 0.3933\n",
            "Epoch 80/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3933 - val_loss: 0.3986\n",
            "Epoch 81/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3926 - val_loss: 0.3965\n",
            "Epoch 82/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3920 - val_loss: 0.3948\n",
            "Epoch 83/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3912 - val_loss: 0.3941\n",
            "121/121 [==============================] - 0s 954us/step - loss: 0.3964\n",
            "[CV]  learning_rate=0.00037192261022352417, n_hidden=2, n_neurons=47, total=  32.9s\n",
            "[CV] learning_rate=0.0051747964719537, n_hidden=2, n_neurons=3 .......\n",
            "Epoch 1/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 1.4612 - val_loss: 0.7610\n",
            "Epoch 2/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.6316 - val_loss: 0.5237\n",
            "Epoch 3/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5401 - val_loss: 0.4747\n",
            "Epoch 4/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5043 - val_loss: 0.4491\n",
            "Epoch 5/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4826 - val_loss: 0.4319\n",
            "Epoch 6/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4676 - val_loss: 0.4251\n",
            "Epoch 7/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4574 - val_loss: 0.4137\n",
            "Epoch 8/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4497 - val_loss: 0.4101\n",
            "Epoch 9/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4457 - val_loss: 0.4133\n",
            "Epoch 10/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4431 - val_loss: 0.4141\n",
            "Epoch 11/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4410 - val_loss: 0.4192\n",
            "Epoch 12/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4384 - val_loss: 0.4299\n",
            "Epoch 13/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4366 - val_loss: 0.4312\n",
            "Epoch 14/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4358 - val_loss: 0.4450\n",
            "Epoch 15/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4345 - val_loss: 0.4460\n",
            "Epoch 16/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4333 - val_loss: 0.4555\n",
            "Epoch 17/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4327 - val_loss: 0.4580\n",
            "Epoch 18/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4321 - val_loss: 0.4631\n",
            "121/121 [==============================] - 0s 887us/step - loss: 0.4376\n",
            "[CV]  learning_rate=0.0051747964719537, n_hidden=2, n_neurons=3, total=   7.2s\n",
            "[CV] learning_rate=0.0051747964719537, n_hidden=2, n_neurons=3 .......\n",
            "Epoch 1/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 1.2933 - val_loss: 0.7511\n",
            "Epoch 2/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.7404 - val_loss: 0.6353\n",
            "Epoch 3/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.6467 - val_loss: 0.5665\n",
            "Epoch 4/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5824 - val_loss: 0.5155\n",
            "Epoch 5/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5353 - val_loss: 0.4767\n",
            "Epoch 6/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5008 - val_loss: 0.4516\n",
            "Epoch 7/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4777 - val_loss: 0.4306\n",
            "Epoch 8/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4617 - val_loss: 0.4175\n",
            "Epoch 9/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4517 - val_loss: 0.4115\n",
            "Epoch 10/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4449 - val_loss: 0.4064\n",
            "Epoch 11/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4408 - val_loss: 0.4049\n",
            "Epoch 12/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4388 - val_loss: 0.4078\n",
            "Epoch 13/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4380 - val_loss: 0.4034\n",
            "Epoch 14/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4375 - val_loss: 0.4041\n",
            "Epoch 15/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4358 - val_loss: 0.4001\n",
            "Epoch 16/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4346 - val_loss: 0.4006\n",
            "Epoch 17/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4344 - val_loss: 0.3983\n",
            "Epoch 18/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4353 - val_loss: 0.3997\n",
            "Epoch 19/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4336 - val_loss: 0.3990\n",
            "Epoch 20/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4339 - val_loss: 0.3985\n",
            "Epoch 21/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4341 - val_loss: 0.3999\n",
            "Epoch 22/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4335 - val_loss: 0.3978\n",
            "Epoch 23/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4334 - val_loss: 0.3991\n",
            "Epoch 24/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4330 - val_loss: 0.3982\n",
            "Epoch 25/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4327 - val_loss: 0.4017\n",
            "Epoch 26/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4329 - val_loss: 0.3987\n",
            "Epoch 27/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4321 - val_loss: 0.3975\n",
            "Epoch 28/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4331 - val_loss: 0.3965\n",
            "Epoch 29/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4310 - val_loss: 0.4020\n",
            "Epoch 30/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4329 - val_loss: 0.3975\n",
            "Epoch 31/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4320 - val_loss: 0.3961\n",
            "Epoch 32/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4311 - val_loss: 0.3970\n",
            "Epoch 33/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4312 - val_loss: 0.3988\n",
            "Epoch 34/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4312 - val_loss: 0.3962\n",
            "Epoch 35/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4312 - val_loss: 0.3960\n",
            "Epoch 36/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4305 - val_loss: 0.3962\n",
            "Epoch 37/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4306 - val_loss: 0.3980\n",
            "Epoch 38/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4300 - val_loss: 0.4007\n",
            "Epoch 39/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4302 - val_loss: 0.3974\n",
            "Epoch 40/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4304 - val_loss: 0.3960\n",
            "Epoch 41/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4298 - val_loss: 0.3989\n",
            "Epoch 42/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4296 - val_loss: 0.3971\n",
            "Epoch 43/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4295 - val_loss: 0.3954\n",
            "Epoch 44/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4297 - val_loss: 0.3976\n",
            "Epoch 45/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4293 - val_loss: 0.3985\n",
            "Epoch 46/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4292 - val_loss: 0.3958\n",
            "Epoch 47/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4292 - val_loss: 0.3952\n",
            "Epoch 48/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4289 - val_loss: 0.3952\n",
            "Epoch 49/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4289 - val_loss: 0.3978\n",
            "Epoch 50/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4298 - val_loss: 0.3949\n",
            "Epoch 51/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4285 - val_loss: 0.4086\n",
            "Epoch 52/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4292 - val_loss: 0.3958\n",
            "Epoch 53/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4287 - val_loss: 0.3946\n",
            "Epoch 54/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4288 - val_loss: 0.3949\n",
            "Epoch 55/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4284 - val_loss: 0.3953\n",
            "Epoch 56/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4282 - val_loss: 0.3942\n",
            "Epoch 57/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4284 - val_loss: 0.3935\n",
            "Epoch 58/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4288 - val_loss: 0.3941\n",
            "Epoch 59/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4282 - val_loss: 0.3939\n",
            "Epoch 60/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4272 - val_loss: 0.3945\n",
            "Epoch 61/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4274 - val_loss: 0.3935\n",
            "Epoch 62/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4278 - val_loss: 0.3948\n",
            "Epoch 63/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4270 - val_loss: 0.3930\n",
            "Epoch 64/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4278 - val_loss: 0.3929\n",
            "Epoch 65/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4273 - val_loss: 0.3977\n",
            "Epoch 66/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4275 - val_loss: 0.3967\n",
            "Epoch 67/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4275 - val_loss: 0.3963\n",
            "Epoch 68/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4267 - val_loss: 0.3939\n",
            "Epoch 69/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4270 - val_loss: 0.3931\n",
            "Epoch 70/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4264 - val_loss: 0.3929\n",
            "Epoch 71/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4264 - val_loss: 0.3924\n",
            "Epoch 72/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4262 - val_loss: 0.3937\n",
            "Epoch 73/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4260 - val_loss: 0.3928\n",
            "Epoch 74/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4269 - val_loss: 0.3932\n",
            "Epoch 75/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4266 - val_loss: 0.3932\n",
            "Epoch 76/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4260 - val_loss: 0.3922\n",
            "Epoch 77/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4258 - val_loss: 0.3913\n",
            "Epoch 78/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4260 - val_loss: 0.3932\n",
            "Epoch 79/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4258 - val_loss: 0.3938\n",
            "Epoch 80/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4259 - val_loss: 0.3923\n",
            "Epoch 81/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4253 - val_loss: 0.3912\n",
            "Epoch 82/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4256 - val_loss: 0.3911\n",
            "Epoch 83/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4248 - val_loss: 0.3930\n",
            "Epoch 84/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4258 - val_loss: 0.3934\n",
            "Epoch 85/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4246 - val_loss: 0.3941\n",
            "Epoch 86/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4250 - val_loss: 0.3920\n",
            "Epoch 87/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4250 - val_loss: 0.3905\n",
            "Epoch 88/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4238 - val_loss: 0.3934\n",
            "Epoch 89/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4248 - val_loss: 0.3917\n",
            "Epoch 90/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4245 - val_loss: 0.3913\n",
            "Epoch 91/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4244 - val_loss: 0.3894\n",
            "Epoch 92/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4238 - val_loss: 0.3914\n",
            "Epoch 93/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4231 - val_loss: 0.3922\n",
            "Epoch 94/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4223 - val_loss: 0.3888\n",
            "Epoch 95/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4217 - val_loss: 0.3907\n",
            "Epoch 96/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4212 - val_loss: 0.3897\n",
            "Epoch 97/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4197 - val_loss: 0.3880\n",
            "Epoch 98/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4197 - val_loss: 0.3880\n",
            "Epoch 99/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4194 - val_loss: 0.3882\n",
            "Epoch 100/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4191 - val_loss: 0.3861\n",
            "121/121 [==============================] - 0s 842us/step - loss: 0.4316\n",
            "[CV]  learning_rate=0.0051747964719537, n_hidden=2, n_neurons=3, total=  38.0s\n",
            "[CV] learning_rate=0.0051747964719537, n_hidden=2, n_neurons=3 .......\n",
            "Epoch 1/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 2.2804 - val_loss: 1.3310\n",
            "Epoch 2/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 1.3294 - val_loss: 1.3072\n",
            "Epoch 3/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 1.2712 - val_loss: 1.2961\n",
            "Epoch 4/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 1.1024 - val_loss: 1.1947\n",
            "Epoch 5/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.8683 - val_loss: 0.8109\n",
            "Epoch 6/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.7192 - val_loss: 0.6500\n",
            "Epoch 7/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.6270 - val_loss: 0.5744\n",
            "Epoch 8/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5698 - val_loss: 0.5243\n",
            "Epoch 9/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5291 - val_loss: 0.4874\n",
            "Epoch 10/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5010 - val_loss: 0.4626\n",
            "Epoch 11/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4797 - val_loss: 0.4605\n",
            "Epoch 12/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4648 - val_loss: 0.4276\n",
            "Epoch 13/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4531 - val_loss: 0.4193\n",
            "Epoch 14/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4431 - val_loss: 0.4132\n",
            "Epoch 15/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4355 - val_loss: 0.4013\n",
            "Epoch 16/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4283 - val_loss: 0.4080\n",
            "Epoch 17/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4238 - val_loss: 0.3934\n",
            "Epoch 18/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4194 - val_loss: 0.3869\n",
            "Epoch 19/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4152 - val_loss: 0.4079\n",
            "Epoch 20/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4148 - val_loss: 0.3832\n",
            "Epoch 21/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4104 - val_loss: 0.3785\n",
            "Epoch 22/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4083 - val_loss: 0.3748\n",
            "Epoch 23/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4063 - val_loss: 0.3767\n",
            "Epoch 24/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4047 - val_loss: 0.4028\n",
            "Epoch 25/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4041 - val_loss: 0.3759\n",
            "Epoch 26/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4030 - val_loss: 0.3703\n",
            "Epoch 27/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4009 - val_loss: 0.3832\n",
            "Epoch 28/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4001 - val_loss: 0.3836\n",
            "Epoch 29/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4008 - val_loss: 0.3734\n",
            "Epoch 30/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3995 - val_loss: 0.3707\n",
            "Epoch 31/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3978 - val_loss: 0.3849\n",
            "Epoch 32/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3982 - val_loss: 0.3925\n",
            "Epoch 33/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3978 - val_loss: 0.3811\n",
            "Epoch 34/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3967 - val_loss: 0.3948\n",
            "Epoch 35/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3991 - val_loss: 0.3837\n",
            "Epoch 36/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3954 - val_loss: 0.3720\n",
            "121/121 [==============================] - 0s 831us/step - loss: 0.3801\n",
            "[CV]  learning_rate=0.0051747964719537, n_hidden=2, n_neurons=3, total=  14.0s\n",
            "[CV] learning_rate=0.01573990360087585, n_hidden=2, n_neurons=21 .....\n",
            "Epoch 1/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.9549 - val_loss: 14.3943\n",
            "Epoch 2/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.6167 - val_loss: 1.3951\n",
            "Epoch 3/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4718 - val_loss: 5.1012\n",
            "Epoch 4/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4308 - val_loss: 0.3599\n",
            "Epoch 5/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3902 - val_loss: 0.3634\n",
            "Epoch 6/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3805 - val_loss: 0.3572\n",
            "Epoch 7/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3748 - val_loss: 0.3609\n",
            "Epoch 8/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3684 - val_loss: 0.3643\n",
            "Epoch 9/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3660 - val_loss: 0.3811\n",
            "Epoch 10/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3649 - val_loss: 0.3612\n",
            "Epoch 11/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3632 - val_loss: 0.3686\n",
            "Epoch 12/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3582 - val_loss: 0.3915\n",
            "Epoch 13/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3526 - val_loss: 0.3714\n",
            "Epoch 14/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3519 - val_loss: 0.3614\n",
            "Epoch 15/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3483 - val_loss: 0.3588\n",
            "Epoch 16/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3463 - val_loss: 0.3634\n",
            "121/121 [==============================] - 0s 924us/step - loss: 0.3628\n",
            "[CV]  learning_rate=0.01573990360087585, n_hidden=2, n_neurons=21, total=   6.6s\n",
            "[CV] learning_rate=0.01573990360087585, n_hidden=2, n_neurons=21 .....\n",
            "Epoch 1/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.8603 - val_loss: 1.1634\n",
            "Epoch 2/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4745 - val_loss: 0.4448\n",
            "Epoch 3/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4169 - val_loss: 1.5024\n",
            "Epoch 4/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4012 - val_loss: 0.6651\n",
            "Epoch 5/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3860 - val_loss: 0.6541\n",
            "Epoch 6/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3718 - val_loss: 1.1368\n",
            "Epoch 7/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3716 - val_loss: 1.0258\n",
            "Epoch 8/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3647 - val_loss: 0.8781\n",
            "Epoch 9/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3568 - val_loss: 0.9266\n",
            "Epoch 10/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3548 - val_loss: 0.3832\n",
            "Epoch 11/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3497 - val_loss: 0.4872\n",
            "Epoch 12/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3461 - val_loss: 0.7527\n",
            "Epoch 13/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3440 - val_loss: 0.7507\n",
            "Epoch 14/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3637 - val_loss: 0.4622\n",
            "Epoch 15/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3417 - val_loss: 0.5459\n",
            "Epoch 16/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3370 - val_loss: 0.6454\n",
            "Epoch 17/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3356 - val_loss: 0.3154\n",
            "Epoch 18/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3331 - val_loss: 0.3336\n",
            "Epoch 19/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3293 - val_loss: 0.7688\n",
            "Epoch 20/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3266 - val_loss: 0.3209\n",
            "Epoch 21/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3266 - val_loss: 0.3383\n",
            "Epoch 22/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3239 - val_loss: 0.3570\n",
            "Epoch 23/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3199 - val_loss: 0.4193\n",
            "Epoch 24/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3205 - val_loss: 0.4355\n",
            "Epoch 25/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3162 - val_loss: 0.5337\n",
            "Epoch 26/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3147 - val_loss: 0.3297\n",
            "Epoch 27/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3124 - val_loss: 0.3660\n",
            "121/121 [==============================] - 0s 888us/step - loss: 0.3269\n",
            "[CV]  learning_rate=0.01573990360087585, n_hidden=2, n_neurons=21, total=  10.9s\n",
            "[CV] learning_rate=0.01573990360087585, n_hidden=2, n_neurons=21 .....\n",
            "Epoch 1/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.8681 - val_loss: 78.2036\n",
            "Epoch 2/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5490 - val_loss: 0.6790\n",
            "Epoch 3/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4278 - val_loss: 0.4839\n",
            "Epoch 4/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4079 - val_loss: 2.8441\n",
            "Epoch 5/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4293 - val_loss: 1.1948\n",
            "Epoch 6/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4033 - val_loss: 1.5213\n",
            "Epoch 7/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3980 - val_loss: 1.2546\n",
            "Epoch 8/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3870 - val_loss: 0.3561\n",
            "Epoch 9/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3658 - val_loss: 0.3494\n",
            "Epoch 10/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3636 - val_loss: 0.3644\n",
            "Epoch 11/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3541 - val_loss: 0.3989\n",
            "Epoch 12/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3507 - val_loss: 0.3455\n",
            "Epoch 13/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3459 - val_loss: 0.3347\n",
            "Epoch 14/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3413 - val_loss: 0.4436\n",
            "Epoch 15/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3399 - val_loss: 0.3232\n",
            "Epoch 16/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3350 - val_loss: 0.4249\n",
            "Epoch 17/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3343 - val_loss: 0.3207\n",
            "Epoch 18/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3297 - val_loss: 0.3392\n",
            "Epoch 19/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3285 - val_loss: 0.3633\n",
            "Epoch 20/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3272 - val_loss: 0.3619\n",
            "Epoch 21/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3259 - val_loss: 0.3313\n",
            "Epoch 22/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3247 - val_loss: 0.3067\n",
            "Epoch 23/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3236 - val_loss: 0.3247\n",
            "Epoch 24/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3202 - val_loss: 0.3493\n",
            "Epoch 25/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3187 - val_loss: 0.3061\n",
            "Epoch 26/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3176 - val_loss: 0.3050\n",
            "Epoch 27/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3155 - val_loss: 0.3861\n",
            "Epoch 28/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3157 - val_loss: 0.2994\n",
            "Epoch 29/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3165 - val_loss: 0.4576\n",
            "Epoch 30/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3178 - val_loss: 0.3246\n",
            "Epoch 31/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3137 - val_loss: 0.3071\n",
            "Epoch 32/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3117 - val_loss: 0.3594\n",
            "Epoch 33/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3125 - val_loss: 0.3316\n",
            "Epoch 34/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3101 - val_loss: 0.3616\n",
            "Epoch 35/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3096 - val_loss: 0.3058\n",
            "Epoch 36/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3070 - val_loss: 0.3400\n",
            "Epoch 37/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3072 - val_loss: 0.3017\n",
            "Epoch 38/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3078 - val_loss: 0.4242\n",
            "121/121 [==============================] - 0s 906us/step - loss: 0.3115\n",
            "[CV]  learning_rate=0.01573990360087585, n_hidden=2, n_neurons=21, total=  15.0s\n",
            "[CV] learning_rate=0.002388469823418883, n_hidden=1, n_neurons=4 .....\n",
            "Epoch 1/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 3.0543 - val_loss: 16.8552\n",
            "Epoch 2/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.9701 - val_loss: 21.8505\n",
            "Epoch 3/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.8498 - val_loss: 0.6058\n",
            "Epoch 4/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5725 - val_loss: 0.5598\n",
            "Epoch 5/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5474 - val_loss: 0.5318\n",
            "Epoch 6/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.5242 - val_loss: 0.5020\n",
            "Epoch 7/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5049 - val_loss: 0.4793\n",
            "Epoch 8/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4918 - val_loss: 0.4655\n",
            "Epoch 9/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4805 - val_loss: 0.4517\n",
            "Epoch 10/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4707 - val_loss: 0.4398\n",
            "Epoch 11/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4650 - val_loss: 0.4358\n",
            "Epoch 12/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4596 - val_loss: 0.4278\n",
            "Epoch 13/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4534 - val_loss: 0.4209\n",
            "Epoch 14/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4512 - val_loss: 0.4181\n",
            "Epoch 15/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4473 - val_loss: 0.4131\n",
            "Epoch 16/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4454 - val_loss: 0.4103\n",
            "Epoch 17/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4434 - val_loss: 0.4103\n",
            "Epoch 18/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4421 - val_loss: 0.4061\n",
            "Epoch 19/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4400 - val_loss: 0.4059\n",
            "Epoch 20/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4393 - val_loss: 0.4037\n",
            "Epoch 21/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4384 - val_loss: 0.4037\n",
            "Epoch 22/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4368 - val_loss: 0.4017\n",
            "Epoch 23/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4360 - val_loss: 0.4025\n",
            "Epoch 24/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4350 - val_loss: 0.4011\n",
            "Epoch 25/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4343 - val_loss: 0.3997\n",
            "Epoch 26/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4337 - val_loss: 0.3994\n",
            "Epoch 27/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4329 - val_loss: 0.3976\n",
            "Epoch 28/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4325 - val_loss: 0.3964\n",
            "Epoch 29/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4313 - val_loss: 0.3963\n",
            "Epoch 30/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4310 - val_loss: 0.3958\n",
            "Epoch 31/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4306 - val_loss: 0.3948\n",
            "Epoch 32/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4298 - val_loss: 0.3956\n",
            "Epoch 33/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4296 - val_loss: 0.3938\n",
            "Epoch 34/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4287 - val_loss: 0.3929\n",
            "Epoch 35/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4282 - val_loss: 0.3928\n",
            "Epoch 36/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4279 - val_loss: 0.3929\n",
            "Epoch 37/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4273 - val_loss: 0.3914\n",
            "Epoch 38/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4267 - val_loss: 0.3922\n",
            "Epoch 39/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4264 - val_loss: 0.3904\n",
            "Epoch 40/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4259 - val_loss: 0.3909\n",
            "Epoch 41/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4252 - val_loss: 0.3919\n",
            "Epoch 42/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4258 - val_loss: 0.3903\n",
            "Epoch 43/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4244 - val_loss: 0.3900\n",
            "Epoch 44/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4238 - val_loss: 0.3895\n",
            "Epoch 45/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4236 - val_loss: 0.3889\n",
            "Epoch 46/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4229 - val_loss: 0.3899\n",
            "Epoch 47/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4229 - val_loss: 0.3880\n",
            "Epoch 48/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4221 - val_loss: 0.3884\n",
            "Epoch 49/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4217 - val_loss: 0.3876\n",
            "Epoch 50/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4212 - val_loss: 0.3871\n",
            "Epoch 51/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4211 - val_loss: 0.3876\n",
            "Epoch 52/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4205 - val_loss: 0.3865\n",
            "Epoch 53/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4200 - val_loss: 0.3861\n",
            "Epoch 54/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4190 - val_loss: 0.3858\n",
            "Epoch 55/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4184 - val_loss: 0.3857\n",
            "Epoch 56/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4177 - val_loss: 0.3862\n",
            "Epoch 57/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4177 - val_loss: 0.3850\n",
            "Epoch 58/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4173 - val_loss: 0.3845\n",
            "Epoch 59/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4171 - val_loss: 0.3838\n",
            "Epoch 60/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4161 - val_loss: 0.3847\n",
            "Epoch 61/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4162 - val_loss: 0.3836\n",
            "Epoch 62/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4151 - val_loss: 0.3845\n",
            "Epoch 63/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4147 - val_loss: 0.3832\n",
            "Epoch 64/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4140 - val_loss: 0.3820\n",
            "Epoch 65/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4133 - val_loss: 0.3814\n",
            "Epoch 66/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4122 - val_loss: 0.3821\n",
            "Epoch 67/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4114 - val_loss: 0.3819\n",
            "Epoch 68/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4107 - val_loss: 0.3811\n",
            "Epoch 69/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4096 - val_loss: 0.3819\n",
            "Epoch 70/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4090 - val_loss: 0.3814\n",
            "Epoch 71/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4088 - val_loss: 0.3797\n",
            "Epoch 72/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4070 - val_loss: 0.3794\n",
            "Epoch 73/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4070 - val_loss: 0.3787\n",
            "Epoch 74/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4055 - val_loss: 0.3796\n",
            "Epoch 75/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4054 - val_loss: 0.3786\n",
            "Epoch 76/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4044 - val_loss: 0.3783\n",
            "Epoch 77/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4043 - val_loss: 0.3775\n",
            "Epoch 78/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4037 - val_loss: 0.3786\n",
            "Epoch 79/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4035 - val_loss: 0.3766\n",
            "Epoch 80/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4019 - val_loss: 0.3777\n",
            "Epoch 81/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4013 - val_loss: 0.3773\n",
            "Epoch 82/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4011 - val_loss: 0.3760\n",
            "Epoch 83/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4002 - val_loss: 0.3770\n",
            "Epoch 84/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3997 - val_loss: 0.3753\n",
            "Epoch 85/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3995 - val_loss: 0.3754\n",
            "Epoch 86/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3984 - val_loss: 0.3751\n",
            "Epoch 87/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3985 - val_loss: 0.3745\n",
            "Epoch 88/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3982 - val_loss: 0.3756\n",
            "Epoch 89/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3978 - val_loss: 0.3745\n",
            "Epoch 90/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3970 - val_loss: 0.3740\n",
            "Epoch 91/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3973 - val_loss: 0.3741\n",
            "Epoch 92/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3970 - val_loss: 0.3732\n",
            "Epoch 93/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3965 - val_loss: 0.3736\n",
            "Epoch 94/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3962 - val_loss: 0.3733\n",
            "Epoch 95/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3957 - val_loss: 0.3736\n",
            "Epoch 96/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3958 - val_loss: 0.3736\n",
            "Epoch 97/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3953 - val_loss: 0.3730\n",
            "Epoch 98/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3954 - val_loss: 0.3729\n",
            "Epoch 99/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3946 - val_loss: 0.3736\n",
            "Epoch 100/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3950 - val_loss: 0.3719\n",
            "121/121 [==============================] - 0s 839us/step - loss: 0.4163\n",
            "[CV]  learning_rate=0.002388469823418883, n_hidden=1, n_neurons=4, total=  38.0s\n",
            "[CV] learning_rate=0.002388469823418883, n_hidden=1, n_neurons=4 .....\n",
            "Epoch 1/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 3.2763 - val_loss: 1.3750\n",
            "Epoch 2/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 1.0241 - val_loss: 1.8482\n",
            "Epoch 3/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.7643 - val_loss: 1.9196\n",
            "Epoch 4/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.7015 - val_loss: 1.7124\n",
            "Epoch 5/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.6713 - val_loss: 1.5121\n",
            "Epoch 6/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.6483 - val_loss: 1.3173\n",
            "Epoch 7/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.6282 - val_loss: 1.1694\n",
            "Epoch 8/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.6093 - val_loss: 1.0393\n",
            "Epoch 9/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5914 - val_loss: 0.9321\n",
            "Epoch 10/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5740 - val_loss: 0.8467\n",
            "Epoch 11/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5566 - val_loss: 0.7704\n",
            "Epoch 12/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5408 - val_loss: 0.7055\n",
            "Epoch 13/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5273 - val_loss: 0.6569\n",
            "Epoch 14/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5152 - val_loss: 0.6266\n",
            "Epoch 15/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5048 - val_loss: 0.5902\n",
            "Epoch 16/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4954 - val_loss: 0.5718\n",
            "Epoch 17/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4871 - val_loss: 0.5465\n",
            "Epoch 18/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4819 - val_loss: 0.5305\n",
            "Epoch 19/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4752 - val_loss: 0.5206\n",
            "Epoch 20/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4710 - val_loss: 0.5165\n",
            "Epoch 21/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4672 - val_loss: 0.5055\n",
            "Epoch 22/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4634 - val_loss: 0.4991\n",
            "Epoch 23/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4599 - val_loss: 0.4922\n",
            "Epoch 24/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4573 - val_loss: 0.4861\n",
            "Epoch 25/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4551 - val_loss: 0.4809\n",
            "Epoch 26/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4521 - val_loss: 0.4751\n",
            "Epoch 27/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4505 - val_loss: 0.4745\n",
            "Epoch 28/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4481 - val_loss: 0.4691\n",
            "Epoch 29/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4452 - val_loss: 0.4681\n",
            "Epoch 30/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4442 - val_loss: 0.4601\n",
            "Epoch 31/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4424 - val_loss: 0.4502\n",
            "Epoch 32/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4409 - val_loss: 0.4470\n",
            "Epoch 33/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4401 - val_loss: 0.4446\n",
            "Epoch 34/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4391 - val_loss: 0.4424\n",
            "Epoch 35/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4383 - val_loss: 0.4373\n",
            "Epoch 36/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4377 - val_loss: 0.4321\n",
            "Epoch 37/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4370 - val_loss: 0.4362\n",
            "Epoch 38/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4365 - val_loss: 0.4307\n",
            "Epoch 39/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4358 - val_loss: 0.4250\n",
            "Epoch 40/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4356 - val_loss: 0.4244\n",
            "Epoch 41/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4349 - val_loss: 0.4232\n",
            "Epoch 42/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4345 - val_loss: 0.4243\n",
            "Epoch 43/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4340 - val_loss: 0.4182\n",
            "Epoch 44/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4337 - val_loss: 0.4177\n",
            "Epoch 45/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4332 - val_loss: 0.4206\n",
            "Epoch 46/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4329 - val_loss: 0.4144\n",
            "Epoch 47/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4325 - val_loss: 0.4142\n",
            "Epoch 48/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4323 - val_loss: 0.4130\n",
            "Epoch 49/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4322 - val_loss: 0.4135\n",
            "Epoch 50/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4320 - val_loss: 0.4106\n",
            "Epoch 51/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4319 - val_loss: 0.4078\n",
            "Epoch 52/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4314 - val_loss: 0.4091\n",
            "Epoch 53/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4310 - val_loss: 0.4066\n",
            "Epoch 54/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4307 - val_loss: 0.4079\n",
            "Epoch 55/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4305 - val_loss: 0.4078\n",
            "Epoch 56/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4303 - val_loss: 0.4059\n",
            "Epoch 57/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4301 - val_loss: 0.4037\n",
            "Epoch 58/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4301 - val_loss: 0.4048\n",
            "Epoch 59/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4297 - val_loss: 0.4044\n",
            "Epoch 60/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4291 - val_loss: 0.4026\n",
            "Epoch 61/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4293 - val_loss: 0.4035\n",
            "Epoch 62/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4291 - val_loss: 0.4051\n",
            "Epoch 63/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4288 - val_loss: 0.4024\n",
            "Epoch 64/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4288 - val_loss: 0.4026\n",
            "Epoch 65/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4284 - val_loss: 0.4059\n",
            "Epoch 66/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4284 - val_loss: 0.4060\n",
            "Epoch 67/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4282 - val_loss: 0.4024\n",
            "Epoch 68/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4277 - val_loss: 0.4038\n",
            "Epoch 69/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4279 - val_loss: 0.4029\n",
            "Epoch 70/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4274 - val_loss: 0.3998\n",
            "Epoch 71/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4272 - val_loss: 0.4001\n",
            "Epoch 72/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4269 - val_loss: 0.4030\n",
            "Epoch 73/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4268 - val_loss: 0.4006\n",
            "Epoch 74/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4267 - val_loss: 0.4016\n",
            "Epoch 75/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4266 - val_loss: 0.4025\n",
            "Epoch 76/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4261 - val_loss: 0.4013\n",
            "Epoch 77/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4258 - val_loss: 0.4001\n",
            "Epoch 78/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4259 - val_loss: 0.4038\n",
            "Epoch 79/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4256 - val_loss: 0.4035\n",
            "Epoch 80/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4255 - val_loss: 0.4023\n",
            "121/121 [==============================] - 0s 950us/step - loss: 0.4395\n",
            "[CV]  learning_rate=0.002388469823418883, n_hidden=1, n_neurons=4, total=  29.4s\n",
            "[CV] learning_rate=0.002388469823418883, n_hidden=1, n_neurons=4 .....\n",
            "Epoch 1/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 3.5439 - val_loss: 1.3805\n",
            "Epoch 2/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 1.1621 - val_loss: 1.0903\n",
            "Epoch 3/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.8805 - val_loss: 0.7900\n",
            "Epoch 4/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.7781 - val_loss: 0.7074\n",
            "Epoch 5/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.7241 - val_loss: 0.6592\n",
            "Epoch 6/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.6846 - val_loss: 0.6255\n",
            "Epoch 7/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.6479 - val_loss: 0.5948\n",
            "Epoch 8/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.6163 - val_loss: 0.6086\n",
            "Epoch 9/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5898 - val_loss: 0.7122\n",
            "Epoch 10/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5824 - val_loss: 0.9397\n",
            "Epoch 11/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5783 - val_loss: 0.5225\n",
            "Epoch 12/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5590 - val_loss: 0.5679\n",
            "Epoch 13/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5469 - val_loss: 0.6636\n",
            "Epoch 14/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5396 - val_loss: 0.7508\n",
            "Epoch 15/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5473 - val_loss: 1.2276\n",
            "Epoch 16/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5418 - val_loss: 0.5363\n",
            "Epoch 17/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5360 - val_loss: 0.7425\n",
            "Epoch 18/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5239 - val_loss: 0.6267\n",
            "Epoch 19/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5207 - val_loss: 0.7098\n",
            "Epoch 20/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5293 - val_loss: 0.9684\n",
            "Epoch 21/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5116 - val_loss: 0.6090\n",
            "121/121 [==============================] - 0s 818us/step - loss: 0.5135\n",
            "[CV]  learning_rate=0.002388469823418883, n_hidden=1, n_neurons=4, total=   8.1s\n",
            "[CV] learning_rate=0.02298924804076755, n_hidden=1, n_neurons=9 ......\n",
            "Epoch 1/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.8667 - val_loss: 6.1076\n",
            "Epoch 2/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5506 - val_loss: 0.4522\n",
            "Epoch 3/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4521 - val_loss: 0.3991\n",
            "Epoch 4/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4301 - val_loss: 0.3860\n",
            "Epoch 5/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4191 - val_loss: 0.3820\n",
            "Epoch 6/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4119 - val_loss: 0.3832\n",
            "Epoch 7/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4064 - val_loss: 0.3768\n",
            "Epoch 8/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4001 - val_loss: 0.3706\n",
            "Epoch 9/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3943 - val_loss: 0.3721\n",
            "Epoch 10/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3898 - val_loss: 0.3585\n",
            "Epoch 11/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3855 - val_loss: 0.3706\n",
            "Epoch 12/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3806 - val_loss: 0.3791\n",
            "Epoch 13/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3777 - val_loss: 0.4793\n",
            "Epoch 14/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3774 - val_loss: 0.3536\n",
            "Epoch 15/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3747 - val_loss: 0.3513\n",
            "Epoch 16/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3750 - val_loss: 0.3507\n",
            "Epoch 17/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3726 - val_loss: 0.3544\n",
            "Epoch 18/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3716 - val_loss: 0.3540\n",
            "Epoch 19/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3689 - val_loss: 0.3661\n",
            "Epoch 20/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3683 - val_loss: 0.7589\n",
            "Epoch 21/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3741 - val_loss: 0.3525\n",
            "Epoch 22/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3684 - val_loss: 0.3502\n",
            "Epoch 23/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3664 - val_loss: 0.3636\n",
            "Epoch 24/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3694 - val_loss: 0.3617\n",
            "Epoch 25/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3643 - val_loss: 0.3698\n",
            "Epoch 26/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3641 - val_loss: 0.3526\n",
            "Epoch 27/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3618 - val_loss: 0.3498\n",
            "Epoch 28/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3609 - val_loss: 0.3485\n",
            "Epoch 29/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3598 - val_loss: 0.3412\n",
            "Epoch 30/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3589 - val_loss: 0.3429\n",
            "Epoch 31/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3572 - val_loss: 0.3416\n",
            "Epoch 32/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3601 - val_loss: 0.3390\n",
            "Epoch 33/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3569 - val_loss: 0.3397\n",
            "Epoch 34/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3555 - val_loss: 0.3382\n",
            "Epoch 35/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3545 - val_loss: 0.3396\n",
            "Epoch 36/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3545 - val_loss: 0.3431\n",
            "Epoch 37/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3562 - val_loss: 0.3370\n",
            "Epoch 38/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3527 - val_loss: 0.3432\n",
            "Epoch 39/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3612 - val_loss: 0.3457\n",
            "Epoch 40/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3570 - val_loss: 0.3423\n",
            "Epoch 41/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3588 - val_loss: 0.3782\n",
            "Epoch 42/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3589 - val_loss: 0.3375\n",
            "Epoch 43/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3552 - val_loss: 0.3379\n",
            "Epoch 44/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3507 - val_loss: 0.3393\n",
            "Epoch 45/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3577 - val_loss: 0.3361\n",
            "Epoch 46/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3545 - val_loss: 0.5786\n",
            "Epoch 47/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3521 - val_loss: 0.3335\n",
            "Epoch 48/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3542 - val_loss: 0.3395\n",
            "Epoch 49/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3507 - val_loss: 0.3339\n",
            "Epoch 50/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3512 - val_loss: 0.3360\n",
            "Epoch 51/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3529 - val_loss: 0.3853\n",
            "Epoch 52/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3490 - val_loss: 0.5797\n",
            "Epoch 53/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3520 - val_loss: 0.3335\n",
            "Epoch 54/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3505 - val_loss: 0.3422\n",
            "Epoch 55/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3485 - val_loss: 0.3381\n",
            "Epoch 56/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3494 - val_loss: 0.3538\n",
            "Epoch 57/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3487 - val_loss: 0.4913\n",
            "121/121 [==============================] - 0s 808us/step - loss: 0.3808\n",
            "[CV]  learning_rate=0.02298924804076755, n_hidden=1, n_neurons=9, total=  21.6s\n",
            "[CV] learning_rate=0.02298924804076755, n_hidden=1, n_neurons=9 ......\n",
            "Epoch 1/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.6350 - val_loss: 0.4199\n",
            "Epoch 2/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4333 - val_loss: 0.4190\n",
            "Epoch 3/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4163 - val_loss: 0.3890\n",
            "Epoch 4/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4101 - val_loss: 0.4000\n",
            "Epoch 5/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4062 - val_loss: 0.4391\n",
            "Epoch 6/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4006 - val_loss: 0.5933\n",
            "Epoch 7/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4004 - val_loss: 0.4875\n",
            "Epoch 8/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3965 - val_loss: 0.5758\n",
            "Epoch 9/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3949 - val_loss: 0.5464\n",
            "Epoch 10/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3954 - val_loss: 0.5368\n",
            "Epoch 11/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3882 - val_loss: 0.4691\n",
            "Epoch 12/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3991 - val_loss: 0.5633\n",
            "Epoch 13/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3908 - val_loss: 0.5702\n",
            "121/121 [==============================] - 0s 910us/step - loss: 0.4234\n",
            "[CV]  learning_rate=0.02298924804076755, n_hidden=1, n_neurons=9, total=   5.3s\n",
            "[CV] learning_rate=0.02298924804076755, n_hidden=1, n_neurons=9 ......\n",
            "Epoch 1/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 1.1250 - val_loss: 2.3067\n",
            "Epoch 2/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4706 - val_loss: 4.9597\n",
            "Epoch 3/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5124 - val_loss: 34.6558\n",
            "Epoch 4/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.9303 - val_loss: 0.4013\n",
            "Epoch 5/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4370 - val_loss: 0.3890\n",
            "Epoch 6/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4272 - val_loss: 0.3849\n",
            "Epoch 7/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4258 - val_loss: 0.3743\n",
            "Epoch 8/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4330 - val_loss: 0.3966\n",
            "Epoch 9/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4240 - val_loss: 0.3799\n",
            "Epoch 10/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4146 - val_loss: 0.3696\n",
            "Epoch 11/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4130 - val_loss: 0.3701\n",
            "Epoch 12/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4076 - val_loss: 0.3729\n",
            "Epoch 13/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4050 - val_loss: 0.3736\n",
            "Epoch 14/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4030 - val_loss: 0.3685\n",
            "Epoch 15/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4009 - val_loss: 0.3658\n",
            "Epoch 16/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3968 - val_loss: 0.3603\n",
            "Epoch 17/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3948 - val_loss: 0.3646\n",
            "Epoch 18/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3917 - val_loss: 0.3630\n",
            "Epoch 19/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3906 - val_loss: 0.3547\n",
            "Epoch 20/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3898 - val_loss: 0.3556\n",
            "Epoch 21/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3878 - val_loss: 0.3536\n",
            "Epoch 22/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3874 - val_loss: 0.3514\n",
            "Epoch 23/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3868 - val_loss: 0.3539\n",
            "Epoch 24/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3846 - val_loss: 0.3529\n",
            "Epoch 25/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3850 - val_loss: 0.3497\n",
            "Epoch 26/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3841 - val_loss: 0.3535\n",
            "Epoch 27/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3822 - val_loss: 0.3507\n",
            "Epoch 28/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3817 - val_loss: 0.3582\n",
            "Epoch 29/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4354 - val_loss: 0.3632\n",
            "Epoch 30/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3933 - val_loss: 0.3550\n",
            "Epoch 31/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3860 - val_loss: 0.3571\n",
            "Epoch 32/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3814 - val_loss: 0.3497\n",
            "Epoch 33/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3798 - val_loss: 0.3574\n",
            "Epoch 34/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3775 - val_loss: 0.3540\n",
            "Epoch 35/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3773 - val_loss: 0.3500\n",
            "121/121 [==============================] - 0s 811us/step - loss: 0.3692\n",
            "[CV]  learning_rate=0.02298924804076755, n_hidden=1, n_neurons=9, total=  13.5s\n",
            "[CV] learning_rate=0.00032288937857243905, n_hidden=1, n_neurons=20 ..\n",
            "Epoch 1/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 4.9841 - val_loss: 4.8184\n",
            "Epoch 2/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 3.0559 - val_loss: 4.3045\n",
            "Epoch 3/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 2.1295 - val_loss: 3.3703\n",
            "Epoch 4/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 1.6133 - val_loss: 2.3733\n",
            "Epoch 5/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 1.3126 - val_loss: 1.6283\n",
            "Epoch 6/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 1.1356 - val_loss: 1.2966\n",
            "Epoch 7/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 1.0297 - val_loss: 1.0523\n",
            "Epoch 8/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.9625 - val_loss: 0.9042\n",
            "Epoch 9/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.9183 - val_loss: 0.8433\n",
            "Epoch 10/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.8864 - val_loss: 0.8160\n",
            "Epoch 11/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.8622 - val_loss: 0.8072\n",
            "Epoch 12/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.8435 - val_loss: 0.7939\n",
            "Epoch 13/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.8262 - val_loss: 0.8011\n",
            "Epoch 14/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.8119 - val_loss: 0.8047\n",
            "Epoch 15/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.7990 - val_loss: 0.7890\n",
            "Epoch 16/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.7869 - val_loss: 0.7920\n",
            "Epoch 17/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.7755 - val_loss: 0.7855\n",
            "Epoch 18/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.7649 - val_loss: 0.7890\n",
            "Epoch 19/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.7548 - val_loss: 0.7875\n",
            "Epoch 20/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.7452 - val_loss: 0.7844\n",
            "Epoch 21/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.7362 - val_loss: 0.7527\n",
            "Epoch 22/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.7271 - val_loss: 0.7573\n",
            "Epoch 23/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.7185 - val_loss: 0.7515\n",
            "Epoch 24/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.7102 - val_loss: 0.7487\n",
            "Epoch 25/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.7025 - val_loss: 0.7299\n",
            "Epoch 26/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.6950 - val_loss: 0.7071\n",
            "Epoch 27/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.6870 - val_loss: 0.7181\n",
            "Epoch 28/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.6801 - val_loss: 0.6913\n",
            "Epoch 29/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.6729 - val_loss: 0.6927\n",
            "Epoch 30/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.6663 - val_loss: 0.6839\n",
            "Epoch 31/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.6593 - val_loss: 0.6761\n",
            "Epoch 32/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.6526 - val_loss: 0.6594\n",
            "Epoch 33/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.6463 - val_loss: 0.6581\n",
            "Epoch 34/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.6398 - val_loss: 0.6518\n",
            "Epoch 35/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.6339 - val_loss: 0.6356\n",
            "Epoch 36/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.6278 - val_loss: 0.6404\n",
            "Epoch 37/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.6223 - val_loss: 0.6122\n",
            "Epoch 38/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.6161 - val_loss: 0.6249\n",
            "Epoch 39/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.6106 - val_loss: 0.6118\n",
            "Epoch 40/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.6051 - val_loss: 0.6038\n",
            "Epoch 41/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5994 - val_loss: 0.6074\n",
            "Epoch 42/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5946 - val_loss: 0.5926\n",
            "Epoch 43/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5893 - val_loss: 0.5833\n",
            "Epoch 44/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5840 - val_loss: 0.5865\n",
            "Epoch 45/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5792 - val_loss: 0.5847\n",
            "Epoch 46/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5746 - val_loss: 0.5757\n",
            "Epoch 47/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5698 - val_loss: 0.5742\n",
            "Epoch 48/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5651 - val_loss: 0.5679\n",
            "Epoch 49/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5606 - val_loss: 0.5640\n",
            "Epoch 50/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5562 - val_loss: 0.5655\n",
            "Epoch 51/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5524 - val_loss: 0.5587\n",
            "Epoch 52/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5476 - val_loss: 0.5466\n",
            "Epoch 53/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5435 - val_loss: 0.5403\n",
            "Epoch 54/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5394 - val_loss: 0.5339\n",
            "Epoch 55/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5354 - val_loss: 0.5330\n",
            "Epoch 56/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5317 - val_loss: 0.5222\n",
            "Epoch 57/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5278 - val_loss: 0.5227\n",
            "Epoch 58/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5241 - val_loss: 0.5169\n",
            "Epoch 59/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5206 - val_loss: 0.5096\n",
            "Epoch 60/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5171 - val_loss: 0.5028\n",
            "Epoch 61/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5139 - val_loss: 0.5009\n",
            "Epoch 62/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5105 - val_loss: 0.4993\n",
            "Epoch 63/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5072 - val_loss: 0.4972\n",
            "Epoch 64/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5044 - val_loss: 0.4910\n",
            "Epoch 65/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5014 - val_loss: 0.4878\n",
            "Epoch 66/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4983 - val_loss: 0.4862\n",
            "Epoch 67/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4956 - val_loss: 0.4827\n",
            "Epoch 68/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4928 - val_loss: 0.4733\n",
            "Epoch 69/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4902 - val_loss: 0.4733\n",
            "Epoch 70/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4877 - val_loss: 0.4707\n",
            "Epoch 71/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4854 - val_loss: 0.4683\n",
            "Epoch 72/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4831 - val_loss: 0.4647\n",
            "Epoch 73/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4810 - val_loss: 0.4626\n",
            "Epoch 74/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4785 - val_loss: 0.4597\n",
            "Epoch 75/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4766 - val_loss: 0.4566\n",
            "Epoch 76/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4743 - val_loss: 0.4556\n",
            "Epoch 77/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4724 - val_loss: 0.4514\n",
            "Epoch 78/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4707 - val_loss: 0.4501\n",
            "Epoch 79/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4689 - val_loss: 0.4478\n",
            "Epoch 80/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4671 - val_loss: 0.4457\n",
            "Epoch 81/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4654 - val_loss: 0.4441\n",
            "Epoch 82/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4638 - val_loss: 0.4414\n",
            "Epoch 83/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4625 - val_loss: 0.4398\n",
            "Epoch 84/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4608 - val_loss: 0.4383\n",
            "Epoch 85/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4595 - val_loss: 0.4370\n",
            "Epoch 86/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4580 - val_loss: 0.4358\n",
            "Epoch 87/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4566 - val_loss: 0.4347\n",
            "Epoch 88/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4556 - val_loss: 0.4334\n",
            "Epoch 89/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4542 - val_loss: 0.4322\n",
            "Epoch 90/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4530 - val_loss: 0.4312\n",
            "Epoch 91/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4520 - val_loss: 0.4301\n",
            "Epoch 92/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4508 - val_loss: 0.4291\n",
            "Epoch 93/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4496 - val_loss: 0.4281\n",
            "Epoch 94/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4486 - val_loss: 0.4272\n",
            "Epoch 95/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4477 - val_loss: 0.4265\n",
            "Epoch 96/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4467 - val_loss: 0.4262\n",
            "Epoch 97/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4457 - val_loss: 0.4252\n",
            "Epoch 98/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4448 - val_loss: 0.4243\n",
            "Epoch 99/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4441 - val_loss: 0.4245\n",
            "Epoch 100/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4431 - val_loss: 0.4237\n",
            "121/121 [==============================] - 0s 831us/step - loss: 0.4571\n",
            "[CV]  learning_rate=0.00032288937857243905, n_hidden=1, n_neurons=20, total=  37.3s\n",
            "[CV] learning_rate=0.00032288937857243905, n_hidden=1, n_neurons=20 ..\n",
            "Epoch 1/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 4.0600 - val_loss: 6.9698\n",
            "Epoch 2/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 2.1607 - val_loss: 9.1152\n",
            "Epoch 3/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 1.4064 - val_loss: 10.0311\n",
            "Epoch 4/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 1.0897 - val_loss: 9.7394\n",
            "Epoch 5/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.9411 - val_loss: 8.8096\n",
            "Epoch 6/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.8597 - val_loss: 7.6886\n",
            "Epoch 7/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.8092 - val_loss: 6.5657\n",
            "Epoch 8/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.7742 - val_loss: 5.5603\n",
            "Epoch 9/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.7486 - val_loss: 4.8625\n",
            "Epoch 10/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.7279 - val_loss: 4.2950\n",
            "Epoch 11/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.7109 - val_loss: 3.8070\n",
            "Epoch 12/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.6967 - val_loss: 3.3700\n",
            "Epoch 13/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.6838 - val_loss: 2.9924\n",
            "Epoch 14/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.6727 - val_loss: 2.6708\n",
            "Epoch 15/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.6625 - val_loss: 2.3825\n",
            "Epoch 16/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.6530 - val_loss: 2.1295\n",
            "Epoch 17/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.6445 - val_loss: 1.9044\n",
            "Epoch 18/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.6363 - val_loss: 1.7056\n",
            "Epoch 19/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.6287 - val_loss: 1.5445\n",
            "Epoch 20/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.6213 - val_loss: 1.4260\n",
            "Epoch 21/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.6143 - val_loss: 1.3170\n",
            "Epoch 22/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.6075 - val_loss: 1.2227\n",
            "Epoch 23/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.6014 - val_loss: 1.1374\n",
            "Epoch 24/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5952 - val_loss: 1.0602\n",
            "Epoch 25/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5895 - val_loss: 0.9898\n",
            "Epoch 26/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5840 - val_loss: 0.9258\n",
            "Epoch 27/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5786 - val_loss: 0.8671\n",
            "Epoch 28/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5735 - val_loss: 0.8142\n",
            "Epoch 29/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5686 - val_loss: 0.7730\n",
            "Epoch 30/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5641 - val_loss: 0.7305\n",
            "Epoch 31/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5593 - val_loss: 0.6907\n",
            "Epoch 32/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5547 - val_loss: 0.6573\n",
            "Epoch 33/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5506 - val_loss: 0.6273\n",
            "Epoch 34/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5462 - val_loss: 0.6031\n",
            "Epoch 35/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5423 - val_loss: 0.5818\n",
            "Epoch 36/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5384 - val_loss: 0.5607\n",
            "Epoch 37/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5347 - val_loss: 0.5445\n",
            "Epoch 38/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5312 - val_loss: 0.5294\n",
            "Epoch 39/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5274 - val_loss: 0.5171\n",
            "Epoch 40/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5243 - val_loss: 0.5071\n",
            "Epoch 41/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5207 - val_loss: 0.4989\n",
            "Epoch 42/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5177 - val_loss: 0.4920\n",
            "Epoch 43/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5146 - val_loss: 0.4860\n",
            "Epoch 44/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5114 - val_loss: 0.4820\n",
            "Epoch 45/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5086 - val_loss: 0.4791\n",
            "Epoch 46/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5057 - val_loss: 0.4772\n",
            "Epoch 47/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5030 - val_loss: 0.4764\n",
            "Epoch 48/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5002 - val_loss: 0.4762\n",
            "Epoch 49/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4979 - val_loss: 0.4764\n",
            "Epoch 50/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4953 - val_loss: 0.4777\n",
            "Epoch 51/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4933 - val_loss: 0.4798\n",
            "Epoch 52/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4903 - val_loss: 0.4820\n",
            "Epoch 53/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4881 - val_loss: 0.4843\n",
            "Epoch 54/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4858 - val_loss: 0.4875\n",
            "Epoch 55/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4836 - val_loss: 0.4902\n",
            "Epoch 56/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4814 - val_loss: 0.4938\n",
            "Epoch 57/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4794 - val_loss: 0.4982\n",
            "Epoch 58/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4773 - val_loss: 0.5008\n",
            "121/121 [==============================] - 0s 878us/step - loss: 0.4955\n",
            "[CV]  learning_rate=0.00032288937857243905, n_hidden=1, n_neurons=20, total=  21.7s\n",
            "[CV] learning_rate=0.00032288937857243905, n_hidden=1, n_neurons=20 ..\n",
            "Epoch 1/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 5.3883 - val_loss: 3.4339\n",
            "Epoch 2/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 2.5615 - val_loss: 2.9095\n",
            "Epoch 3/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 1.5082 - val_loss: 1.7524\n",
            "Epoch 4/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 1.0632 - val_loss: 1.2056\n",
            "Epoch 5/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.8769 - val_loss: 0.8811\n",
            "Epoch 6/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.7957 - val_loss: 0.7500\n",
            "Epoch 7/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.7542 - val_loss: 0.6919\n",
            "Epoch 8/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.7283 - val_loss: 0.6701\n",
            "Epoch 9/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.7094 - val_loss: 0.6548\n",
            "Epoch 10/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.6934 - val_loss: 0.6391\n",
            "Epoch 11/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.6786 - val_loss: 0.6391\n",
            "Epoch 12/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.6662 - val_loss: 0.6188\n",
            "Epoch 13/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.6538 - val_loss: 0.6048\n",
            "Epoch 14/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.6426 - val_loss: 0.5939\n",
            "Epoch 15/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.6315 - val_loss: 0.5834\n",
            "Epoch 16/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.6209 - val_loss: 0.5839\n",
            "Epoch 17/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.6118 - val_loss: 0.5687\n",
            "Epoch 18/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.6025 - val_loss: 0.5573\n",
            "Epoch 19/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5937 - val_loss: 0.5567\n",
            "Epoch 20/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5860 - val_loss: 0.5475\n",
            "Epoch 21/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5782 - val_loss: 0.5389\n",
            "Epoch 22/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5708 - val_loss: 0.5332\n",
            "Epoch 23/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5643 - val_loss: 0.5203\n",
            "Epoch 24/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5575 - val_loss: 0.5133\n",
            "Epoch 25/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5514 - val_loss: 0.5075\n",
            "Epoch 26/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5456 - val_loss: 0.5021\n",
            "Epoch 27/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5398 - val_loss: 0.4992\n",
            "Epoch 28/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5353 - val_loss: 0.4922\n",
            "Epoch 29/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5298 - val_loss: 0.4888\n",
            "Epoch 30/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5257 - val_loss: 0.4835\n",
            "Epoch 31/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5207 - val_loss: 0.4809\n",
            "Epoch 32/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5168 - val_loss: 0.4763\n",
            "Epoch 33/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5128 - val_loss: 0.4726\n",
            "Epoch 34/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5093 - val_loss: 0.4682\n",
            "Epoch 35/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5058 - val_loss: 0.4649\n",
            "Epoch 36/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5024 - val_loss: 0.4622\n",
            "Epoch 37/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4996 - val_loss: 0.4631\n",
            "Epoch 38/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4963 - val_loss: 0.4579\n",
            "Epoch 39/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4933 - val_loss: 0.4544\n",
            "Epoch 40/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4907 - val_loss: 0.4516\n",
            "Epoch 41/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4880 - val_loss: 0.4496\n",
            "Epoch 42/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4857 - val_loss: 0.4495\n",
            "Epoch 43/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4833 - val_loss: 0.4468\n",
            "Epoch 44/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4810 - val_loss: 0.4456\n",
            "Epoch 45/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4786 - val_loss: 0.4415\n",
            "Epoch 46/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4767 - val_loss: 0.4410\n",
            "Epoch 47/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4747 - val_loss: 0.4425\n",
            "Epoch 48/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4726 - val_loss: 0.4390\n",
            "Epoch 49/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4711 - val_loss: 0.4397\n",
            "Epoch 50/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4693 - val_loss: 0.4397\n",
            "Epoch 51/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4676 - val_loss: 0.4387\n",
            "Epoch 52/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4660 - val_loss: 0.4375\n",
            "Epoch 53/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4644 - val_loss: 0.4389\n",
            "Epoch 54/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4631 - val_loss: 0.4408\n",
            "Epoch 55/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4614 - val_loss: 0.4380\n",
            "Epoch 56/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4599 - val_loss: 0.4387\n",
            "Epoch 57/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4588 - val_loss: 0.4343\n",
            "Epoch 58/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4574 - val_loss: 0.4367\n",
            "Epoch 59/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4562 - val_loss: 0.4403\n",
            "Epoch 60/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4547 - val_loss: 0.4313\n",
            "Epoch 61/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4538 - val_loss: 0.4340\n",
            "Epoch 62/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4525 - val_loss: 0.4415\n",
            "Epoch 63/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4515 - val_loss: 0.4371\n",
            "Epoch 64/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4504 - val_loss: 0.4340\n",
            "Epoch 65/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4495 - val_loss: 0.4357\n",
            "Epoch 66/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4484 - val_loss: 0.4365\n",
            "Epoch 67/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4475 - val_loss: 0.4289\n",
            "Epoch 68/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4465 - val_loss: 0.4320\n",
            "Epoch 69/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4458 - val_loss: 0.4400\n",
            "Epoch 70/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4449 - val_loss: 0.4352\n",
            "Epoch 71/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4437 - val_loss: 0.4303\n",
            "Epoch 72/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4431 - val_loss: 0.4342\n",
            "Epoch 73/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4422 - val_loss: 0.4325\n",
            "Epoch 74/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4413 - val_loss: 0.4345\n",
            "Epoch 75/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4408 - val_loss: 0.4409\n",
            "Epoch 76/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4398 - val_loss: 0.4416\n",
            "Epoch 77/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4390 - val_loss: 0.4389\n",
            "121/121 [==============================] - 0s 899us/step - loss: 0.4379\n",
            "[CV]  learning_rate=0.00032288937857243905, n_hidden=1, n_neurons=20, total=  29.0s\n",
            "[CV] learning_rate=0.000910274101743386, n_hidden=2, n_neurons=44 ....\n",
            "Epoch 1/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 2.6153 - val_loss: 1.3616\n",
            "Epoch 2/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.8785 - val_loss: 0.7161\n",
            "Epoch 3/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.6817 - val_loss: 0.6232\n",
            "Epoch 4/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.6282 - val_loss: 0.5842\n",
            "Epoch 5/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5953 - val_loss: 0.5478\n",
            "Epoch 6/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5689 - val_loss: 0.5453\n",
            "Epoch 7/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5467 - val_loss: 0.5037\n",
            "Epoch 8/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5266 - val_loss: 0.4998\n",
            "Epoch 9/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5095 - val_loss: 0.4817\n",
            "Epoch 10/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4939 - val_loss: 0.4652\n",
            "Epoch 11/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4806 - val_loss: 0.4451\n",
            "Epoch 12/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4689 - val_loss: 0.4353\n",
            "Epoch 13/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4582 - val_loss: 0.4322\n",
            "Epoch 14/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4493 - val_loss: 0.4327\n",
            "Epoch 15/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4413 - val_loss: 0.4159\n",
            "Epoch 16/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4341 - val_loss: 0.4076\n",
            "Epoch 17/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4275 - val_loss: 0.4093\n",
            "Epoch 18/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4223 - val_loss: 0.3983\n",
            "Epoch 19/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4170 - val_loss: 0.4039\n",
            "Epoch 20/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4130 - val_loss: 0.3896\n",
            "Epoch 21/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4088 - val_loss: 0.3875\n",
            "Epoch 22/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4054 - val_loss: 0.3905\n",
            "Epoch 23/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4021 - val_loss: 0.3854\n",
            "Epoch 24/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3991 - val_loss: 0.3762\n",
            "Epoch 25/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3965 - val_loss: 0.4037\n",
            "Epoch 26/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3943 - val_loss: 0.3715\n",
            "Epoch 27/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3915 - val_loss: 0.4187\n",
            "Epoch 28/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3896 - val_loss: 0.3879\n",
            "Epoch 29/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3873 - val_loss: 0.3952\n",
            "Epoch 30/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3858 - val_loss: 0.3738\n",
            "Epoch 31/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3837 - val_loss: 0.3707\n",
            "Epoch 32/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3819 - val_loss: 0.3685\n",
            "Epoch 33/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3803 - val_loss: 0.3980\n",
            "Epoch 34/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3787 - val_loss: 0.3775\n",
            "Epoch 35/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3773 - val_loss: 0.3619\n",
            "Epoch 36/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3758 - val_loss: 0.4170\n",
            "Epoch 37/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3750 - val_loss: 0.3605\n",
            "Epoch 38/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3735 - val_loss: 0.4606\n",
            "Epoch 39/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3729 - val_loss: 0.3555\n",
            "Epoch 40/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3711 - val_loss: 0.4115\n",
            "Epoch 41/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3700 - val_loss: 0.3602\n",
            "Epoch 42/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3690 - val_loss: 0.3590\n",
            "Epoch 43/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3680 - val_loss: 0.3581\n",
            "Epoch 44/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3667 - val_loss: 0.4065\n",
            "Epoch 45/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3663 - val_loss: 0.3525\n",
            "Epoch 46/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3650 - val_loss: 0.3768\n",
            "Epoch 47/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3642 - val_loss: 0.3850\n",
            "Epoch 48/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3632 - val_loss: 0.3590\n",
            "Epoch 49/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3624 - val_loss: 0.3646\n",
            "Epoch 50/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3617 - val_loss: 0.3938\n",
            "Epoch 51/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3612 - val_loss: 0.3711\n",
            "Epoch 52/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3602 - val_loss: 0.3607\n",
            "Epoch 53/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3593 - val_loss: 0.3916\n",
            "Epoch 54/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3588 - val_loss: 0.3452\n",
            "Epoch 55/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3576 - val_loss: 0.4114\n",
            "Epoch 56/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3576 - val_loss: 0.3453\n",
            "Epoch 57/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3565 - val_loss: 0.4087\n",
            "Epoch 58/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3559 - val_loss: 0.3691\n",
            "Epoch 59/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3551 - val_loss: 0.3508\n",
            "Epoch 60/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3546 - val_loss: 0.3722\n",
            "Epoch 61/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3541 - val_loss: 0.3768\n",
            "Epoch 62/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3533 - val_loss: 0.4012\n",
            "Epoch 63/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3528 - val_loss: 0.3725\n",
            "Epoch 64/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3523 - val_loss: 0.4155\n",
            "121/121 [==============================] - 0s 855us/step - loss: 0.3714\n",
            "[CV]  learning_rate=0.000910274101743386, n_hidden=2, n_neurons=44, total=  25.6s\n",
            "[CV] learning_rate=0.000910274101743386, n_hidden=2, n_neurons=44 ....\n",
            "Epoch 1/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 2.3119 - val_loss: 16.1697\n",
            "Epoch 2/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.8894 - val_loss: 14.3734\n",
            "Epoch 3/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.7784 - val_loss: 10.6526\n",
            "Epoch 4/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.7245 - val_loss: 7.7318\n",
            "Epoch 5/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.6854 - val_loss: 5.8959\n",
            "Epoch 6/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.6525 - val_loss: 4.5990\n",
            "Epoch 7/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.6235 - val_loss: 3.5118\n",
            "Epoch 8/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5967 - val_loss: 2.6725\n",
            "Epoch 9/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5725 - val_loss: 2.1701\n",
            "Epoch 10/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5498 - val_loss: 1.7968\n",
            "Epoch 11/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5288 - val_loss: 1.5335\n",
            "Epoch 12/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5103 - val_loss: 1.2373\n",
            "Epoch 13/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4937 - val_loss: 1.0479\n",
            "Epoch 14/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4794 - val_loss: 0.9404\n",
            "Epoch 15/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4670 - val_loss: 0.8256\n",
            "Epoch 16/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4559 - val_loss: 0.7691\n",
            "Epoch 17/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4469 - val_loss: 0.7082\n",
            "Epoch 18/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4393 - val_loss: 0.6792\n",
            "Epoch 19/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4325 - val_loss: 0.6362\n",
            "Epoch 20/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4268 - val_loss: 0.6294\n",
            "Epoch 21/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4222 - val_loss: 0.6080\n",
            "Epoch 22/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4177 - val_loss: 0.6069\n",
            "Epoch 23/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4140 - val_loss: 0.6151\n",
            "Epoch 24/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4108 - val_loss: 0.6004\n",
            "Epoch 25/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4079 - val_loss: 0.6024\n",
            "Epoch 26/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4053 - val_loss: 0.6089\n",
            "Epoch 27/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4025 - val_loss: 0.6220\n",
            "Epoch 28/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4004 - val_loss: 0.6274\n",
            "Epoch 29/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3979 - val_loss: 0.6640\n",
            "Epoch 30/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3966 - val_loss: 0.6568\n",
            "Epoch 31/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3944 - val_loss: 0.6581\n",
            "Epoch 32/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3923 - val_loss: 0.6713\n",
            "Epoch 33/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3911 - val_loss: 0.6919\n",
            "Epoch 34/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3893 - val_loss: 0.7188\n",
            "121/121 [==============================] - 0s 949us/step - loss: 0.4069\n",
            "[CV]  learning_rate=0.000910274101743386, n_hidden=2, n_neurons=44, total=  13.7s\n",
            "[CV] learning_rate=0.000910274101743386, n_hidden=2, n_neurons=44 ....\n",
            "Epoch 1/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 3.1730 - val_loss: 1.5446\n",
            "Epoch 2/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 1.0175 - val_loss: 0.8902\n",
            "Epoch 3/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.7561 - val_loss: 0.6977\n",
            "Epoch 4/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.6811 - val_loss: 0.6293\n",
            "Epoch 5/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.6415 - val_loss: 0.5948\n",
            "Epoch 6/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.6134 - val_loss: 0.5701\n",
            "Epoch 7/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5906 - val_loss: 0.5495\n",
            "Epoch 8/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5705 - val_loss: 0.5409\n",
            "Epoch 9/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5526 - val_loss: 0.5124\n",
            "Epoch 10/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5366 - val_loss: 0.4957\n",
            "Epoch 11/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5215 - val_loss: 0.4825\n",
            "Epoch 12/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5081 - val_loss: 0.4698\n",
            "Epoch 13/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4963 - val_loss: 0.4583\n",
            "Epoch 14/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4855 - val_loss: 0.4485\n",
            "Epoch 15/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4756 - val_loss: 0.4396\n",
            "Epoch 16/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4670 - val_loss: 0.4308\n",
            "Epoch 17/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4591 - val_loss: 0.4239\n",
            "Epoch 18/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4523 - val_loss: 0.4181\n",
            "Epoch 19/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4461 - val_loss: 0.4123\n",
            "Epoch 20/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4405 - val_loss: 0.4072\n",
            "Epoch 21/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4353 - val_loss: 0.4024\n",
            "Epoch 22/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4304 - val_loss: 0.3988\n",
            "Epoch 23/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4263 - val_loss: 0.3967\n",
            "Epoch 24/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4222 - val_loss: 0.3983\n",
            "Epoch 25/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4187 - val_loss: 0.3921\n",
            "Epoch 26/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4152 - val_loss: 0.3926\n",
            "Epoch 27/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4123 - val_loss: 0.3946\n",
            "Epoch 28/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4096 - val_loss: 0.3948\n",
            "Epoch 29/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4065 - val_loss: 0.4015\n",
            "Epoch 30/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4047 - val_loss: 0.3958\n",
            "Epoch 31/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4021 - val_loss: 0.3915\n",
            "Epoch 32/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3998 - val_loss: 0.4000\n",
            "Epoch 33/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3977 - val_loss: 0.3939\n",
            "Epoch 34/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3959 - val_loss: 0.4041\n",
            "Epoch 35/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3943 - val_loss: 0.3976\n",
            "Epoch 36/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3923 - val_loss: 0.3933\n",
            "Epoch 37/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3912 - val_loss: 0.3922\n",
            "Epoch 38/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3894 - val_loss: 0.3958\n",
            "Epoch 39/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3878 - val_loss: 0.3946\n",
            "Epoch 40/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3866 - val_loss: 0.3938\n",
            "Epoch 41/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3849 - val_loss: 0.4053\n",
            "121/121 [==============================] - 0s 892us/step - loss: 0.3810\n",
            "[CV]  learning_rate=0.000910274101743386, n_hidden=2, n_neurons=44, total=  16.5s\n",
            "[CV] learning_rate=0.002934101104997821, n_hidden=2, n_neurons=35 ....\n",
            "Epoch 1/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 1.4650 - val_loss: 0.6598\n",
            "Epoch 2/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.6178 - val_loss: 0.5656\n",
            "Epoch 3/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5403 - val_loss: 0.5105\n",
            "Epoch 4/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4918 - val_loss: 0.4692\n",
            "Epoch 5/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4576 - val_loss: 0.4453\n",
            "Epoch 6/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4346 - val_loss: 0.4573\n",
            "Epoch 7/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4192 - val_loss: 0.4513\n",
            "Epoch 8/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4075 - val_loss: 0.4665\n",
            "Epoch 9/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3998 - val_loss: 0.4745\n",
            "Epoch 10/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3935 - val_loss: 0.4703\n",
            "Epoch 11/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3879 - val_loss: 0.4503\n",
            "Epoch 12/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3829 - val_loss: 0.4301\n",
            "Epoch 13/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3781 - val_loss: 0.4619\n",
            "Epoch 14/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3753 - val_loss: 0.4812\n",
            "Epoch 15/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3719 - val_loss: 0.4561\n",
            "Epoch 16/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3689 - val_loss: 0.4715\n",
            "Epoch 17/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3659 - val_loss: 0.4751\n",
            "Epoch 18/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3636 - val_loss: 0.4672\n",
            "Epoch 19/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3608 - val_loss: 0.4644\n",
            "Epoch 20/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3587 - val_loss: 0.4563\n",
            "Epoch 21/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3567 - val_loss: 0.4559\n",
            "Epoch 22/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3553 - val_loss: 0.4491\n",
            "121/121 [==============================] - 0s 854us/step - loss: 0.3760\n",
            "[CV]  learning_rate=0.002934101104997821, n_hidden=2, n_neurons=35, total=   9.0s\n",
            "[CV] learning_rate=0.002934101104997821, n_hidden=2, n_neurons=35 ....\n",
            "Epoch 1/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 1.2109 - val_loss: 0.9771\n",
            "Epoch 2/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5570 - val_loss: 0.6358\n",
            "Epoch 3/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4929 - val_loss: 0.4815\n",
            "Epoch 4/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4548 - val_loss: 0.4269\n",
            "Epoch 5/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4314 - val_loss: 0.4052\n",
            "Epoch 6/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4172 - val_loss: 0.4000\n",
            "Epoch 7/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4086 - val_loss: 0.3933\n",
            "Epoch 8/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4012 - val_loss: 0.3911\n",
            "Epoch 9/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3950 - val_loss: 0.4028\n",
            "Epoch 10/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3907 - val_loss: 0.4106\n",
            "Epoch 11/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3860 - val_loss: 0.4176\n",
            "Epoch 12/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3820 - val_loss: 0.4293\n",
            "Epoch 13/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3793 - val_loss: 0.4412\n",
            "Epoch 14/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3766 - val_loss: 0.4895\n",
            "Epoch 15/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3742 - val_loss: 0.4817\n",
            "Epoch 16/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3712 - val_loss: 0.5120\n",
            "Epoch 17/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3690 - val_loss: 0.5241\n",
            "Epoch 18/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3673 - val_loss: 0.5607\n",
            "121/121 [==============================] - 0s 840us/step - loss: 0.3815\n",
            "[CV]  learning_rate=0.002934101104997821, n_hidden=2, n_neurons=35, total=   7.4s\n",
            "[CV] learning_rate=0.002934101104997821, n_hidden=2, n_neurons=35 ....\n",
            "Epoch 1/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 1.5439 - val_loss: 6.2959\n",
            "Epoch 2/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.7189 - val_loss: 1.7209\n",
            "Epoch 3/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5980 - val_loss: 0.5835\n",
            "Epoch 4/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5383 - val_loss: 0.5978\n",
            "Epoch 5/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4996 - val_loss: 0.4667\n",
            "Epoch 6/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4744 - val_loss: 0.5174\n",
            "Epoch 7/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4558 - val_loss: 0.4234\n",
            "Epoch 8/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4416 - val_loss: 0.4194\n",
            "Epoch 9/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4305 - val_loss: 0.5191\n",
            "Epoch 10/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4236 - val_loss: 0.3902\n",
            "Epoch 11/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4154 - val_loss: 0.5707\n",
            "Epoch 12/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4088 - val_loss: 0.4391\n",
            "Epoch 13/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4019 - val_loss: 0.3835\n",
            "Epoch 14/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3987 - val_loss: 0.5121\n",
            "Epoch 15/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3943 - val_loss: 0.3664\n",
            "Epoch 16/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3899 - val_loss: 0.4291\n",
            "Epoch 17/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3874 - val_loss: 0.3632\n",
            "Epoch 18/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3840 - val_loss: 0.4212\n",
            "Epoch 19/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3825 - val_loss: 0.4422\n",
            "Epoch 20/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3783 - val_loss: 0.4015\n",
            "Epoch 21/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3763 - val_loss: 0.3883\n",
            "Epoch 22/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3750 - val_loss: 0.3585\n",
            "Epoch 23/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3727 - val_loss: 0.3960\n",
            "Epoch 24/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3696 - val_loss: 0.4159\n",
            "Epoch 25/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3684 - val_loss: 0.3482\n",
            "Epoch 26/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3662 - val_loss: 0.3680\n",
            "Epoch 27/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3639 - val_loss: 0.4621\n",
            "Epoch 28/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3637 - val_loss: 0.4073\n",
            "Epoch 29/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3748 - val_loss: 0.5437\n",
            "Epoch 30/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3630 - val_loss: 0.3762\n",
            "Epoch 31/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3581 - val_loss: 0.3859\n",
            "Epoch 32/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3570 - val_loss: 0.4180\n",
            "Epoch 33/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3547 - val_loss: 0.3983\n",
            "Epoch 34/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3542 - val_loss: 0.4507\n",
            "Epoch 35/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3537 - val_loss: 0.3642\n",
            "121/121 [==============================] - 0s 931us/step - loss: 0.3506\n",
            "[CV]  learning_rate=0.002934101104997821, n_hidden=2, n_neurons=35, total=  14.4s\n",
            "[CV] learning_rate=0.01975886795274932, n_hidden=1, n_neurons=40 .....\n",
            "Epoch 1/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.7279 - val_loss: 20.8050\n",
            "Epoch 2/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.7077 - val_loss: 17.7764\n",
            "Epoch 3/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.7168 - val_loss: 45.3061\n",
            "Epoch 4/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.6362 - val_loss: 283.6451\n",
            "Epoch 5/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4893 - val_loss: 25.2984\n",
            "Epoch 6/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 3.4063 - val_loss: 0.4012\n",
            "Epoch 7/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4469 - val_loss: 0.3507\n",
            "Epoch 8/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3735 - val_loss: 0.3444\n",
            "Epoch 9/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3563 - val_loss: 0.3633\n",
            "Epoch 10/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3522 - val_loss: 0.3267\n",
            "Epoch 11/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3501 - val_loss: 0.3361\n",
            "Epoch 12/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3435 - val_loss: 0.3402\n",
            "Epoch 13/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3411 - val_loss: 0.3321\n",
            "Epoch 14/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3362 - val_loss: 0.3199\n",
            "Epoch 15/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3326 - val_loss: 0.3148\n",
            "Epoch 16/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3318 - val_loss: 0.3144\n",
            "Epoch 17/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3292 - val_loss: 0.3127\n",
            "Epoch 18/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3279 - val_loss: 0.3185\n",
            "Epoch 19/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3269 - val_loss: 0.3150\n",
            "Epoch 20/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3252 - val_loss: 0.3120\n",
            "Epoch 21/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3245 - val_loss: 0.3121\n",
            "Epoch 22/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3225 - val_loss: 0.3229\n",
            "Epoch 23/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3212 - val_loss: 0.3164\n",
            "Epoch 24/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3224 - val_loss: 0.3143\n",
            "Epoch 25/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3319 - val_loss: 0.3386\n",
            "Epoch 26/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3247 - val_loss: 0.3198\n",
            "Epoch 27/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3223 - val_loss: 0.3149\n",
            "Epoch 28/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3229 - val_loss: 0.3096\n",
            "Epoch 29/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3190 - val_loss: 0.3077\n",
            "Epoch 30/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3151 - val_loss: 0.3082\n",
            "Epoch 31/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3153 - val_loss: 0.3038\n",
            "Epoch 32/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3146 - val_loss: 0.3036\n",
            "Epoch 33/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3131 - val_loss: 0.3061\n",
            "Epoch 34/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3109 - val_loss: 0.3033\n",
            "Epoch 35/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3111 - val_loss: 0.3187\n",
            "Epoch 36/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3104 - val_loss: 0.3134\n",
            "Epoch 37/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3090 - val_loss: 0.3100\n",
            "Epoch 38/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3075 - val_loss: 0.3149\n",
            "Epoch 39/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3082 - val_loss: 0.3198\n",
            "Epoch 40/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3058 - val_loss: 0.3130\n",
            "Epoch 41/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3090 - val_loss: 0.3233\n",
            "Epoch 42/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3055 - val_loss: 0.3563\n",
            "Epoch 43/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3069 - val_loss: 1.0882\n",
            "Epoch 44/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3091 - val_loss: 0.3204\n",
            "121/121 [==============================] - 0s 818us/step - loss: 0.3382\n",
            "[CV]  learning_rate=0.01975886795274932, n_hidden=1, n_neurons=40, total=  16.6s\n",
            "[CV] learning_rate=0.01975886795274932, n_hidden=1, n_neurons=40 .....\n",
            "Epoch 1/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.7461 - val_loss: 1.1687\n",
            "Epoch 2/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4550 - val_loss: 0.4220\n",
            "Epoch 3/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4143 - val_loss: 1.1498\n",
            "Epoch 4/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4037 - val_loss: 0.9809\n",
            "Epoch 5/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3945 - val_loss: 0.6671\n",
            "Epoch 6/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3838 - val_loss: 1.1980\n",
            "Epoch 7/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3836 - val_loss: 1.5918\n",
            "Epoch 8/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3746 - val_loss: 1.1712\n",
            "Epoch 9/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3709 - val_loss: 1.1462\n",
            "Epoch 10/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3676 - val_loss: 0.4978\n",
            "Epoch 11/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3635 - val_loss: 1.0030\n",
            "Epoch 12/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3629 - val_loss: 1.2161\n",
            "121/121 [==============================] - 0s 989us/step - loss: 0.3916\n",
            "[CV]  learning_rate=0.01975886795274932, n_hidden=1, n_neurons=40, total=   4.9s\n",
            "[CV] learning_rate=0.01975886795274932, n_hidden=1, n_neurons=40 .....\n",
            "Epoch 1/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.9208 - val_loss: 56.4137\n",
            "Epoch 2/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.6207 - val_loss: 8.0176\n",
            "Epoch 3/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4429 - val_loss: 1.0555\n",
            "Epoch 4/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4448 - val_loss: 1.6325\n",
            "Epoch 5/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4160 - val_loss: 1.3003\n",
            "Epoch 6/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4075 - val_loss: 0.4546\n",
            "Epoch 7/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4063 - val_loss: 0.3749\n",
            "Epoch 8/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3837 - val_loss: 0.3643\n",
            "Epoch 9/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3888 - val_loss: 0.3811\n",
            "Epoch 10/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3760 - val_loss: 0.3448\n",
            "Epoch 11/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3698 - val_loss: 0.3999\n",
            "Epoch 12/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3713 - val_loss: 0.3511\n",
            "Epoch 13/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3659 - val_loss: 0.3549\n",
            "Epoch 14/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3620 - val_loss: 0.4154\n",
            "Epoch 15/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3584 - val_loss: 0.3362\n",
            "Epoch 16/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3557 - val_loss: 0.3879\n",
            "Epoch 17/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3535 - val_loss: 0.3371\n",
            "Epoch 18/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3554 - val_loss: 0.3614\n",
            "Epoch 19/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3510 - val_loss: 0.3956\n",
            "Epoch 20/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3484 - val_loss: 0.3425\n",
            "Epoch 21/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3476 - val_loss: 0.3660\n",
            "Epoch 22/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3511 - val_loss: 0.3367\n",
            "Epoch 23/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3455 - val_loss: 0.3349\n",
            "Epoch 24/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3433 - val_loss: 0.3493\n",
            "Epoch 25/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3399 - val_loss: 0.3331\n",
            "Epoch 26/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3410 - val_loss: 0.3300\n",
            "Epoch 27/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3376 - val_loss: 0.3445\n",
            "Epoch 28/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3362 - val_loss: 0.3373\n",
            "Epoch 29/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3697 - val_loss: 0.3481\n",
            "Epoch 30/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3405 - val_loss: 0.3609\n",
            "Epoch 31/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3366 - val_loss: 0.3394\n",
            "Epoch 32/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3337 - val_loss: 0.3664\n",
            "Epoch 33/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3352 - val_loss: 0.3175\n",
            "Epoch 34/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3303 - val_loss: 0.3697\n",
            "Epoch 35/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3316 - val_loss: 0.3246\n",
            "Epoch 36/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3279 - val_loss: 0.3552\n",
            "Epoch 37/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3273 - val_loss: 0.3201\n",
            "Epoch 38/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3264 - val_loss: 0.3792\n",
            "Epoch 39/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3263 - val_loss: 0.3452\n",
            "Epoch 40/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3253 - val_loss: 0.4026\n",
            "Epoch 41/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3264 - val_loss: 0.3144\n",
            "Epoch 42/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3232 - val_loss: 0.3634\n",
            "Epoch 43/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3228 - val_loss: 0.3074\n",
            "Epoch 44/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3208 - val_loss: 0.3589\n",
            "Epoch 45/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3286 - val_loss: 0.3163\n",
            "Epoch 46/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3232 - val_loss: 0.4127\n",
            "Epoch 47/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3202 - val_loss: 0.3086\n",
            "Epoch 48/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3195 - val_loss: 0.3266\n",
            "Epoch 49/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3163 - val_loss: 0.3672\n",
            "Epoch 50/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3177 - val_loss: 0.3255\n",
            "Epoch 51/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3172 - val_loss: 0.3907\n",
            "Epoch 52/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3164 - val_loss: 0.3356\n",
            "Epoch 53/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3182 - val_loss: 0.4746\n",
            "121/121 [==============================] - 0s 962us/step - loss: 0.3314\n",
            "[CV]  learning_rate=0.01975886795274932, n_hidden=1, n_neurons=40, total=  20.8s\n",
            "[CV] learning_rate=0.009723980412586881, n_hidden=1, n_neurons=6 .....\n",
            "Epoch 1/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 1.1193 - val_loss: 0.5167\n",
            "Epoch 2/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5350 - val_loss: 0.4696\n",
            "Epoch 3/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4930 - val_loss: 0.4343\n",
            "Epoch 4/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4657 - val_loss: 0.4156\n",
            "Epoch 5/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4490 - val_loss: 0.4081\n",
            "Epoch 6/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4359 - val_loss: 0.4015\n",
            "Epoch 7/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4267 - val_loss: 0.3927\n",
            "Epoch 8/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4259 - val_loss: 0.3952\n",
            "Epoch 9/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4216 - val_loss: 0.3937\n",
            "Epoch 10/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4151 - val_loss: 0.3795\n",
            "Epoch 11/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4176 - val_loss: 0.4017\n",
            "Epoch 12/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4171 - val_loss: 0.3879\n",
            "Epoch 13/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4128 - val_loss: 0.3752\n",
            "Epoch 14/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4102 - val_loss: 0.3777\n",
            "Epoch 15/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4012 - val_loss: 0.3713\n",
            "Epoch 16/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4033 - val_loss: 0.3716\n",
            "Epoch 17/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4087 - val_loss: 0.3861\n",
            "Epoch 18/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4066 - val_loss: 0.3680\n",
            "Epoch 19/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3954 - val_loss: 0.3686\n",
            "Epoch 20/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3949 - val_loss: 0.3652\n",
            "Epoch 21/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3965 - val_loss: 0.3694\n",
            "Epoch 22/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3912 - val_loss: 0.3633\n",
            "Epoch 23/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3937 - val_loss: 0.4908\n",
            "Epoch 24/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4079 - val_loss: 0.3675\n",
            "Epoch 25/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3894 - val_loss: 0.3689\n",
            "Epoch 26/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4073 - val_loss: 0.3610\n",
            "Epoch 27/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4083 - val_loss: 0.3661\n",
            "Epoch 28/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3899 - val_loss: 0.3560\n",
            "Epoch 29/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3906 - val_loss: 0.3548\n",
            "Epoch 30/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3851 - val_loss: 0.3559\n",
            "Epoch 31/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3785 - val_loss: 0.3512\n",
            "Epoch 32/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3882 - val_loss: 0.3527\n",
            "Epoch 33/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3753 - val_loss: 0.3533\n",
            "Epoch 34/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3823 - val_loss: 0.3495\n",
            "Epoch 35/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3736 - val_loss: 0.3491\n",
            "Epoch 36/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3730 - val_loss: 0.3494\n",
            "Epoch 37/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3822 - val_loss: 0.3485\n",
            "Epoch 38/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3732 - val_loss: 0.3469\n",
            "Epoch 39/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3758 - val_loss: 0.3483\n",
            "Epoch 40/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3769 - val_loss: 0.3458\n",
            "Epoch 41/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3795 - val_loss: 0.3503\n",
            "Epoch 42/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3687 - val_loss: 0.3454\n",
            "Epoch 43/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3740 - val_loss: 0.3458\n",
            "Epoch 44/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3681 - val_loss: 0.3450\n",
            "Epoch 45/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3702 - val_loss: 0.3437\n",
            "Epoch 46/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3677 - val_loss: 0.3443\n",
            "Epoch 47/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3678 - val_loss: 0.3431\n",
            "Epoch 48/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3732 - val_loss: 0.3506\n",
            "Epoch 49/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3644 - val_loss: 0.3422\n",
            "Epoch 50/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3660 - val_loss: 0.3420\n",
            "Epoch 51/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3659 - val_loss: 0.3503\n",
            "Epoch 52/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3633 - val_loss: 0.3418\n",
            "Epoch 53/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3648 - val_loss: 0.3411\n",
            "Epoch 54/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3694 - val_loss: 0.3534\n",
            "Epoch 55/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3638 - val_loss: 0.3449\n",
            "Epoch 56/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3644 - val_loss: 0.3417\n",
            "Epoch 57/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3649 - val_loss: 0.3580\n",
            "Epoch 58/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3727 - val_loss: 0.3430\n",
            "Epoch 59/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3635 - val_loss: 0.3433\n",
            "Epoch 60/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3632 - val_loss: 0.3431\n",
            "Epoch 61/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3615 - val_loss: 0.3426\n",
            "Epoch 62/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3613 - val_loss: 0.3532\n",
            "Epoch 63/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3646 - val_loss: 0.3429\n",
            "121/121 [==============================] - 0s 823us/step - loss: 0.3827\n",
            "[CV]  learning_rate=0.009723980412586881, n_hidden=1, n_neurons=6, total=  23.6s\n",
            "[CV] learning_rate=0.009723980412586881, n_hidden=1, n_neurons=6 .....\n",
            "Epoch 1/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.8900 - val_loss: 0.5059\n",
            "Epoch 2/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4707 - val_loss: 0.4247\n",
            "Epoch 3/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4439 - val_loss: 0.5628\n",
            "Epoch 4/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4358 - val_loss: 0.7221\n",
            "Epoch 5/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4288 - val_loss: 0.8717\n",
            "Epoch 6/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4246 - val_loss: 1.1210\n",
            "Epoch 7/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4233 - val_loss: 1.2496\n",
            "Epoch 8/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4204 - val_loss: 1.3653\n",
            "Epoch 9/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4176 - val_loss: 1.6424\n",
            "Epoch 10/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4175 - val_loss: 1.4802\n",
            "Epoch 11/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4147 - val_loss: 1.5841\n",
            "Epoch 12/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4138 - val_loss: 1.5523\n",
            "121/121 [==============================] - 0s 898us/step - loss: 0.4504\n",
            "[CV]  learning_rate=0.009723980412586881, n_hidden=1, n_neurons=6, total=   4.9s\n",
            "[CV] learning_rate=0.009723980412586881, n_hidden=1, n_neurons=6 .....\n",
            "Epoch 1/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 1.9896 - val_loss: 32.2889\n",
            "Epoch 2/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 1.1634 - val_loss: 12.1504\n",
            "Epoch 3/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.8018 - val_loss: 0.5456\n",
            "Epoch 4/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5145 - val_loss: 0.4949\n",
            "Epoch 5/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4853 - val_loss: 0.5069\n",
            "Epoch 6/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4904 - val_loss: 0.4262\n",
            "Epoch 7/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4432 - val_loss: 0.3955\n",
            "Epoch 8/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4214 - val_loss: 0.3811\n",
            "Epoch 9/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4117 - val_loss: 0.3746\n",
            "Epoch 10/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4069 - val_loss: 0.3692\n",
            "Epoch 11/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4026 - val_loss: 0.3678\n",
            "Epoch 12/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4021 - val_loss: 0.3710\n",
            "Epoch 13/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3982 - val_loss: 0.3644\n",
            "Epoch 14/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4001 - val_loss: 0.3643\n",
            "Epoch 15/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3975 - val_loss: 0.3581\n",
            "Epoch 16/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3944 - val_loss: 0.3567\n",
            "Epoch 17/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3985 - val_loss: 0.3587\n",
            "Epoch 18/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3948 - val_loss: 0.3614\n",
            "Epoch 19/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3929 - val_loss: 0.3569\n",
            "Epoch 20/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3891 - val_loss: 0.3531\n",
            "Epoch 21/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3877 - val_loss: 0.3518\n",
            "Epoch 22/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3932 - val_loss: 0.3522\n",
            "Epoch 23/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3934 - val_loss: 0.3528\n",
            "Epoch 24/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3861 - val_loss: 0.3541\n",
            "Epoch 25/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3848 - val_loss: 0.3510\n",
            "Epoch 26/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3854 - val_loss: 0.3521\n",
            "Epoch 27/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3834 - val_loss: 0.3503\n",
            "Epoch 28/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3838 - val_loss: 0.3607\n",
            "Epoch 29/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4074 - val_loss: 0.3564\n",
            "Epoch 30/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4000 - val_loss: 0.3538\n",
            "Epoch 31/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3921 - val_loss: 0.3545\n",
            "Epoch 32/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3848 - val_loss: 0.3499\n",
            "Epoch 33/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3838 - val_loss: 0.3503\n",
            "Epoch 34/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3827 - val_loss: 0.3519\n",
            "Epoch 35/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3841 - val_loss: 0.3490\n",
            "Epoch 36/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3811 - val_loss: 0.3484\n",
            "Epoch 37/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3814 - val_loss: 0.3507\n",
            "Epoch 38/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3817 - val_loss: 0.3536\n",
            "Epoch 39/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3818 - val_loss: 0.3467\n",
            "Epoch 40/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3804 - val_loss: 0.3479\n",
            "Epoch 41/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3810 - val_loss: 0.3471\n",
            "Epoch 42/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3806 - val_loss: 0.3486\n",
            "Epoch 43/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3807 - val_loss: 0.3476\n",
            "Epoch 44/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3787 - val_loss: 0.3462\n",
            "Epoch 45/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3855 - val_loss: 0.3479\n",
            "Epoch 46/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3795 - val_loss: 0.3456\n",
            "Epoch 47/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3775 - val_loss: 0.3455\n",
            "Epoch 48/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3782 - val_loss: 0.3451\n",
            "Epoch 49/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3774 - val_loss: 0.3503\n",
            "Epoch 50/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3773 - val_loss: 0.3459\n",
            "Epoch 51/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3768 - val_loss: 0.3487\n",
            "Epoch 52/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3773 - val_loss: 0.3458\n",
            "Epoch 53/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3767 - val_loss: 0.3461\n",
            "Epoch 54/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3773 - val_loss: 0.3479\n",
            "Epoch 55/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3825 - val_loss: 0.3511\n",
            "Epoch 56/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3757 - val_loss: 0.3465\n",
            "Epoch 57/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3770 - val_loss: 0.3470\n",
            "Epoch 58/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3787 - val_loss: 0.3457\n",
            "121/121 [==============================] - 0s 953us/step - loss: 0.3692\n",
            "[CV]  learning_rate=0.009723980412586881, n_hidden=1, n_neurons=6, total=  21.5s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done  30 out of  30 | elapsed:  9.4min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-74-035bf192124e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m rnd_search_cv.fit(X_train, y_train, epochs=100,\n\u001b[1;32m     12\u001b[0m                   \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m                   callbacks=[keras.callbacks.EarlyStopping(patience=10)])\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    734\u001b[0m             \u001b[0;31m# of the params are estimators as well.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m             self.best_estimator_ = clone(clone(base_estimator).set_params(\n\u001b[0;32m--> 736\u001b[0;31m                 **self.best_params_))\n\u001b[0m\u001b[1;32m    737\u001b[0m             \u001b[0mrefit_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mclone\u001b[0;34m(estimator, safe)\u001b[0m\n\u001b[1;32m     80\u001b[0m             raise RuntimeError('Cannot clone object %s, as the constructor '\n\u001b[1;32m     81\u001b[0m                                \u001b[0;34m'either does not set or modifies parameter %s'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m                                (estimator, name))\n\u001b[0m\u001b[1;32m     83\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnew_object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Cannot clone object <tensorflow.python.keras.wrappers.scikit_learn.KerasRegressor object at 0x7f3cd9f0a4a8>, as the constructor either does not set or modifies parameter learning_rate"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smt9Na4aKCG8",
        "colab_type": "code",
        "outputId": "bcb02ae2-019f-47f8-e483-4d2a31354996",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "rnd_search_cv.best_params_"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'learning_rate': 0.008339092654580042, 'n_hidden': 1, 'n_neurons': 38}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxsjiRxuKCG-",
        "colab_type": "code",
        "outputId": "686b7ca7-8404-4c09-eb66-f763f5f0707f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "rnd_search_cv.best_score_"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.35238636533419293"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RpUnXlmHKCHA",
        "colab_type": "code",
        "outputId": "b4f777fc-03a7-4347-aeb0-8eb78548c5b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "source": [
        "rnd_search_cv.best_estimator_"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-73-3b40e07bb81d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrnd_search_cv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'RandomizedSearchCV' object has no attribute 'best_estimator_'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kY5eCH8KCHE",
        "colab_type": "code",
        "outputId": "71649471-171b-43f4-9436-4ae5230bdb03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "rnd_search_cv.score(X_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-68-cbbd136c11f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrnd_search_cv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    441\u001b[0m         \"\"\"\n\u001b[1;32m    442\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'score'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscorer_\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m             raise ValueError(\"No score function explicitly defined, \"\n\u001b[1;32m    445\u001b[0m                              \u001b[0;34m\"and the estimator doesn't provide one %s\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'RandomizedSearchCV' object has no attribute 'scorer_'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yE2ULE3KCHH",
        "colab_type": "code",
        "outputId": "ff628383-bf99-4579-ba23-a8447b3a0ca0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        }
      },
      "source": [
        "model = rnd_search_cv.best_estimator_.model\n",
        "model"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-69-f89828bd4b63>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnd_search_cv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'RandomizedSearchCV' object has no attribute 'best_estimator_'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "Qq3zR-raKCHJ",
        "colab_type": "code",
        "outputId": "e42ea1c8-57c4-475a-b149-ff726a1b4f14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "model.evaluate(X_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "162/162 [==============================] - 0s 878us/step - loss: 0.3295\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.32945728302001953"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rTRK1l4JUJB",
        "colab_type": "text"
      },
      "source": [
        "# EXERCISE 1: using the NN power in MNIST [In class and home exercise]\n",
        "\n",
        "UUID - #S2E3\n",
        "\n",
        "I know, I know... you are tired of it... but it is a good standard dataset to play with hyperparameters. We will help the community with COVID19 later on when we know how to do simple NNs.\n",
        "\n",
        "Try to train a deep MLP on the MNIST dataset (you can load it using `keras.datasets.mnist.load_data()`). See if you can get over 98% precision. \n",
        "\n",
        "Try searching for the optimal learning rate by using the approach presented in this chapter (i.e., by growing the learning rate exponentially, plotting the loss, and finding the point where the loss shoots up). \n",
        "\n",
        "Try adding all the bells and whistles—save checkpoints, use early stopping, and plot learning curves using TensorBoard. \n",
        "\n",
        "Also, try hyperparameter tuning (without RandomSearchCV... it is broken at the moment...)\n",
        "\n",
        "**You will need to bring me an h5 file that I will run to check the accuracy and a screenshot of TensorBoard**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83zQ0KV8KCHQ",
        "colab_type": "text"
      },
      "source": [
        "Let's load the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yIVY2ivKCHQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Me8rn51pLNJu",
        "colab_type": "text"
      },
      "source": [
        "# EXERCISE 2: keep on going with Covid19\n",
        "\n",
        "Try to apply what you learned to the Xray dataset \n",
        "\n",
        "*   Option 1: try to apply what you learned to the Xray dataset presented in last session. This will be a compound exercise across many sessions. It will be graded when we present how to create a CNN in Session 3-4 \n",
        "*   Option 2: build an MLP regressor to predict the spread of coronavirus in [this Kaggle dataset](https://www.kaggle.com/c/covid19-global-forecasting-week-1), use all the artillery learned today\n",
        "\n",
        "Do both for extra points."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUGy7jqpLQJT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}