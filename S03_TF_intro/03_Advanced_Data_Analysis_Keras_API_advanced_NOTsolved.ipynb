{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "03_Advanced_Data_Analysis_Keras_API_advanced_NOTsolved.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "b54vRt8jT1JC",
        "hzDxyCXqXlt_",
        "LDuDkUGEQfGt",
        "oUF67BcWQfHC",
        "TE_VxvKIQfH4",
        "AyqBScRhQfIG"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aB_ck5OSE3TU",
        "colab_type": "text"
      },
      "source": [
        "![BTS](https://github.com/vfp1/bts-mbds-data-science-foundations-2019/raw/master/sessions/img/Logo-BTS.jpg)\n",
        "\n",
        "# Session 03: Keras API advanced\n",
        "### Victor F. Pajuelo Madrigal <victor.pajuelo@bts.tech> - Advanced Data Analysis (30-03-2020)\n",
        "\n",
        "Open this notebook in Google Colaboratory: [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vfp1/bts-advanced-data-analysis-2020/blob/master/S03_TF_intro/03_Advanced_Data_Analysis_Keras_API_advanced_NOTsolved.ipynb)\n",
        "\n",
        "**Resources (code patched and updated from):**\n",
        "* Sklearn\n",
        "* TensorFlow Authors\n",
        "* Talos documentation\n",
        "* Aurelien Geron's O'Reilly's \"Hands-On Machine Learning with Scikit-Learn, Keras & Tensorflow\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "b54vRt8jT1JC"
      },
      "source": [
        "## Installing and using Talos\n",
        "\n",
        "**WARNING!!** Don't do `!pip install talos`, with the change to tensorflow 2.0 we need to ensure that we install Talos 1.0\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "6b6330c0-2775-415b-8264-586ca3df5513",
        "id": "-BNhvT_jT1JD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Install in your environment at your convenience\n",
        "!pip install git+https://github.com/autonomio/talos@1.0"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/autonomio/talos@1.0\n",
            "  Cloning https://github.com/autonomio/talos (to revision 1.0) to /tmp/pip-req-build-k1m2786y\n",
            "  Running command git clone -q https://github.com/autonomio/talos /tmp/pip-req-build-k1m2786y\n",
            "  Running command git checkout -b 1.0 --track origin/1.0\n",
            "  Switched to a new branch '1.0'\n",
            "  Branch '1.0' set up to track remote branch '1.0' from 'origin'.\n",
            "Requirement already satisfied: tensorflow>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from talos==1.0) (2.2.0rc1)\n",
            "Collecting statsmodels>=0.11.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cb/83/540fd83238a18abe6c2d280fa8e489ac5fcefa1f370f0ca1acd16ae1b860/statsmodels-0.11.1-cp36-cp36m-manylinux1_x86_64.whl (8.7MB)\n",
            "\u001b[K     |████████████████████████████████| 8.7MB 1.4MB/s \n",
            "\u001b[?25hCollecting wrangle\n",
            "  Downloading https://files.pythonhosted.org/packages/85/35/bc729e377417613f2d062a890faea5d649ef1a554df21499e9c3a4a5501a/wrangle-0.6.7.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from talos==1.0) (1.18.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from talos==1.0) (0.25.3)\n",
            "Collecting astetik\n",
            "  Downloading https://files.pythonhosted.org/packages/3c/ba/f8622951da73d9b47b45bb847112c388651f9c6e413e712954f260301d9f/astetik-1.9.9.tar.gz\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (from talos==1.0) (0.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from talos==1.0) (4.38.0)\n",
            "Collecting chances\n",
            "  Downloading https://files.pythonhosted.org/packages/fa/d8/d61112d7476dc3074b855f1edd8556cde9b49b7106853f0b060109dd4c82/chances-0.1.9.tar.gz\n",
            "Collecting kerasplotlib\n",
            "  Downloading https://files.pythonhosted.org/packages/7b/b7/31663d3b5ea9afd8c2c6ffa06d3c4e118ef363e12dc75b7c49fb6a2d22aa/kerasplotlib-0.1.6.tar.gz\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from talos==1.0) (2.21.0)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0.0->talos==1.0) (1.6.3)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0.0->talos==1.0) (2.10.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0.0->talos==1.0) (1.12.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.3.0,>=2.2.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0.0->talos==1.0) (2.2.0rc0)\n",
            "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0.0->talos==1.0) (1.4.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0.0->talos==1.0) (0.9.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0.0->talos==1.0) (1.27.2)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0.0->talos==1.0) (0.34.2)\n",
            "Requirement already satisfied: tensorboard<2.2.0,>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0.0->talos==1.0) (2.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0.0->talos==1.0) (3.2.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0.0->talos==1.0) (3.10.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0.0->talos==1.0) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0.0->talos==1.0) (1.12.1)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0.0->talos==1.0) (0.3.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0.0->talos==1.0) (0.2.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0.0->talos==1.0) (1.1.0)\n",
            "Requirement already satisfied: patsy>=0.5 in /usr/local/lib/python3.6/dist-packages (from statsmodels>=0.11.0->talos==1.0) (0.5.1)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (from wrangle->talos==1.0) (2.2.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->talos==1.0) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->talos==1.0) (2.8.1)\n",
            "Collecting geonamescache\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/c1/efb823270c8526b2f4f3eb8c804c5a0a55277267ad2312f5eb47bd9cc370/geonamescache-1.1.0-py3-none-any.whl (830kB)\n",
            "\u001b[K     |████████████████████████████████| 839kB 40.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn->talos==1.0) (0.22.2.post1)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.6/dist-packages (from kerasplotlib->talos==1.0) (5.5.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->talos==1.0) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->talos==1.0) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->talos==1.0) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->talos==1.0) (3.0.4)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow>=2.0.0->talos==1.0) (0.4.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow>=2.0.0->talos==1.0) (3.2.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow>=2.0.0->talos==1.0) (1.0.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow>=2.0.0->talos==1.0) (46.0.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow>=2.0.0->talos==1.0) (1.7.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras->wrangle->talos==1.0) (3.13)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from keras->wrangle->talos==1.0) (1.0.8)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn->talos==1.0) (0.14.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipython->kerasplotlib->talos==1.0) (4.4.2)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython->kerasplotlib->talos==1.0) (1.0.18)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython->kerasplotlib->talos==1.0) (2.1.3)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython->kerasplotlib->talos==1.0) (0.8.1)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from ipython->kerasplotlib->talos==1.0) (4.3.3)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython->kerasplotlib->talos==1.0) (0.7.5)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython->kerasplotlib->talos==1.0) (4.8.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow>=2.0.0->talos==1.0) (1.3.0)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow>=2.0.0->talos==1.0) (4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow>=2.0.0->talos==1.0) (0.2.8)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow>=2.0.0->talos==1.0) (3.1.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->kerasplotlib->talos==1.0) (0.1.9)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->ipython->kerasplotlib->talos==1.0) (0.2.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython->kerasplotlib->talos==1.0) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow>=2.0.0->talos==1.0) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow>=2.0.0->talos==1.0) (0.4.8)\n",
            "Building wheels for collected packages: talos, wrangle, astetik, chances, kerasplotlib\n",
            "  Building wheel for talos (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for talos: filename=talos-1.0-cp36-none-any.whl size=53700 sha256=f32efeaa0d8066208a385e7c8e16145edfcef78d77e3fe5440b54ac47074681b\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-c8ihavdc/wheels/9f/14/d1/e58012a97c43cf2148959890b171f49e4a5e1c82a9946b2c22\n",
            "  Building wheel for wrangle (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wrangle: filename=wrangle-0.6.7-cp36-none-any.whl size=49894 sha256=a63a3efe894472113f1b701094abf6889512c0f87bce59d0c696869f9cbc84cb\n",
            "  Stored in directory: /root/.cache/pip/wheels/bf/1b/50/d0403ce6ef269e364894da7b50db68db14c4ac62c577561e2d\n",
            "  Building wheel for astetik (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for astetik: filename=astetik-1.9.9-cp36-none-any.whl size=56960 sha256=aa9e447ffc09605890d4a0b3004a32742246da6284801bea62f1b88533501534\n",
            "  Stored in directory: /root/.cache/pip/wheels/ae/70/21/c475cd079ec401dd6e1b9b1d42b4c38554ce12679bfb214aad\n",
            "  Building wheel for chances (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for chances: filename=chances-0.1.9-cp36-none-any.whl size=41609 sha256=480f76da48ade530bc5ca107d1e18c7f3622074ae25fe6b3a07c7f03f8d93eab\n",
            "  Stored in directory: /root/.cache/pip/wheels/75/33/46/c871b94249bd57d17797d049b3dff8e3a09c315afb67eb14c6\n",
            "  Building wheel for kerasplotlib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kerasplotlib: filename=kerasplotlib-0.1.6-cp36-none-any.whl size=3601 sha256=8950db380ca7b9d7a89d39e0657c9a600e787253e320d23c175cd839d5e6f15e\n",
            "  Stored in directory: /root/.cache/pip/wheels/9d/d3/8c/9503a22b0a38e8b21c70ad834e4606d209193443e5c709305d\n",
            "Successfully built talos wrangle astetik chances kerasplotlib\n",
            "\u001b[31mERROR: wrangle 0.6.7 has requirement scipy==1.2, but you'll have scipy 1.4.1 which is incompatible.\u001b[0m\n",
            "Installing collected packages: statsmodels, wrangle, geonamescache, astetik, chances, kerasplotlib, talos\n",
            "  Found existing installation: statsmodels 0.10.2\n",
            "    Uninstalling statsmodels-0.10.2:\n",
            "      Successfully uninstalled statsmodels-0.10.2\n",
            "Successfully installed astetik-1.9.9 chances-0.1.9 geonamescache-1.1.0 kerasplotlib-0.1.6 statsmodels-0.11.1 talos-1.0 wrangle-0.6.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hzDxyCXqXlt_"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "reTbVTMhXluB"
      },
      "source": [
        "First, let's import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures. We also check that Python 3.5 or later is installed (although Python 2.x may work, it is deprecated so we strongly recommend you use Python 3 instead), as well as Scikit-Learn ≥0.20 and TensorFlow ≥2.0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3QX_xNLrXluB",
        "outputId": "a29e2f43-cb73-4ebb-bd75-734b7aaaeca6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        }
      },
      "source": [
        "# Python ≥3.5 is required\n",
        "import sys\n",
        "assert sys.version_info >= (3, 5)\n",
        "\n",
        "# Scikit-Learn ≥0.20 is required\n",
        "import sklearn\n",
        "assert sklearn.__version__ >= \"0.20\"\n",
        "\n",
        "try:\n",
        "    # %tensorflow_version only exists in Colab.\n",
        "    %tensorflow_version 2.x\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# TensorFlow ≥2.0 is required\n",
        "import tensorflow as tf\n",
        "assert tf.__version__ >= \"2.0\"\n",
        "\n",
        "# Import Keras from TensorFlow\n",
        "from tensorflow import keras\n",
        "\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# to make this notebook's output stable across runs\n",
        "np.random.seed(42)\n",
        "\n",
        "# To plot pretty figures\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)\n",
        "\n",
        "# Where to save the figures\n",
        "PROJECT_ROOT_DIR = \".\"\n",
        "CHAPTER_ID = \"ann\"\n",
        "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
        "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
        "\n",
        "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
        "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
        "    print(\"Saving figure\", fig_id)\n",
        "    if tight_layout:\n",
        "        plt.tight_layout()\n",
        "    plt.savefig(path, format=fig_extension, dpi=resolution)\n",
        "\n",
        "# Ignore useless warnings (see SciPy issue #5998)\n",
        "import warnings\n",
        "warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")\n",
        "\n",
        "# Import talos after all\n",
        "import talos"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-02016a4ac472>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;31m# Import talos after all\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtalos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'talos'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IdSlVLm-KCGs",
        "colab_type": "text"
      },
      "source": [
        "# Hyperparameter Tuning\n",
        "\n",
        "UUID - #S3C1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KU029kLKCED",
        "colab_type": "text"
      },
      "source": [
        "Let's load, split and scale the California housing dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwBhgQ02KCEE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "housing = fetch_california_housing()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(housing.data, housing.target, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PavMhzNKCEK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QpeJjzJRFM5",
        "colab_type": "text"
      },
      "source": [
        "## Using Talos\n",
        "\n",
        "Talos allows us to use Keras model as is, but introducing the whole pipeline in a function as such.\n",
        "\n",
        "Talos works with any Keras model, without changing the structure of the model in anyway, or without introducing any new syntax. The below example shows clearly how this works.\n",
        "\n",
        "For this example, we have to import two helper functions from Talos, one for early stopping callout, and the other for using normalized learning rate values. Because we might want to work on trying out several optimizers in a single scan, without normalization, inputting of the values would become cumbersome.\n",
        "\n",
        "Note that the only difference in the model below is how instead of using a label or value to define a given model parameter, we do it using a dictionary label. Also for optimizer we are using a learning rate parameter, which involves the use of two dictionary labels.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwr28SwbZdmX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from talos.utils import lr_normalizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XgnwV6VKW8xD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# then we can go ahead and set the parameter space\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam, Nadam, SGD\n",
        "\n",
        "\n",
        "p = {'lr': (0.001, 0.01, 0.1, 10, 10),\n",
        "     'activation':['relu', 'elu'], \n",
        "     'optimizer': [Adam, Nadam, SGD], \n",
        "     'losses': ['mse', 'mae'], \n",
        "     'shapes': ['brick'], \n",
        "     'first_neuron': [32, 64], \n",
        "     'hidden_layers':[1, 2, 3], \n",
        "     'dropout': [0], \n",
        "     'batch_size': [20],\n",
        "     'epochs': [100]}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yc_w-ui8vhFq",
        "colab_type": "code",
        "outputId": "d7a6acc8-9334-46a9-a028-55480f9c0eb3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_train_scaled.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(15480, 8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 380
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Rp6u4Bi7pnu",
        "colab_type": "code",
        "outputId": "2586428d-fdf4-4654-ee28-0805b8c0c792",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_valid.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3870, 8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 381
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3LJFFQuKCGu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from talos.utils import hidden_layers\n",
        "from talos.utils import lr_normalizer\n",
        "\n",
        "def houses_model(x_train, y_train, x_val, y_val, params):\n",
        "\n",
        "    #Starting our Sequential model\n",
        "    model = keras.models.Sequential()\n",
        "\n",
        "    # Add the first layer with the input shape\n",
        "    #model.add(keras.layers.Dense(units=x_train.shape[0], input_shape=(8,)))\n",
        "    model.add(keras.layers.InputLayer(input_shape=x_train.shape[1:]))\n",
        "\n",
        "    # Add the Talos hidden layers method\n",
        "    hidden_layers(model, params, 1)\n",
        "\n",
        "    model.add(keras.layers.Dense(1, name='Output'))\n",
        "\n",
        "    keras.utils.plot_model(model, \"my_multipleinput_model.png\", show_shapes=True)\n",
        "\n",
        "    # Create a model compilation \n",
        "    model.compile(optimizer=params['optimizer'](lr=lr_normalizer(params['lr'], params['optimizer'])),\n",
        "                  loss=params['losses'],\n",
        "                  metrics=['mse'])\n",
        "    \n",
        "    print(model.summary())\n",
        "\n",
        "    # Use a validation split here since it seems that passing validation to Scan object is broken\n",
        "    out = model.fit(x_train, y_train, validation_split=0.3, \n",
        "                    batch_size=params['batch_size'],\n",
        "                    epochs=params['epochs'],\n",
        "                    verbose=2,\n",
        "                    callbacks=[talos.utils.early_stopper(params['epochs'], \n",
        "                                                         monitor='mse', \n",
        "                                                         patience='10')])\n",
        "    \n",
        "    return out, model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxgmV-7KaqtB",
        "colab_type": "text"
      },
      "source": [
        "### Run the Hyperparameter search\n",
        "\n",
        "Now we are ready to run the model based on the parameters and the layer configuration above. The exact same process would apply with any other model, just make sure to pass the model function name in the Scan() command as in the below example. To get started quickly, we're going to invoke the 'grid_downsample' parameter to 1/100 of the entire permutations.\n",
        "\n",
        "**BEWARE!!!** Since Sklearn is not stable... it seems that Talos is not doing well. The whole transfer to TF 2.x is costing quite a bit cumbersome for many libraries... in this specific case , do not pass `x_val` and `y_val` to the Scan object. Rather pass a `validation.split` in the build model function above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8HQ8fyATtQL",
        "colab_type": "code",
        "outputId": "feff2af4-ce65-44a2-c4e4-4b3cdfba9308",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "scan_object = talos.Scan(x=X_train_scaled,\n",
        "                         y=y_train, \n",
        "                         params=p, \n",
        "                         model=houses_model, \n",
        "                         experiment_name='my_exp', \n",
        "                         fraction_limit=.3, \n",
        "                         reduction_metric='val_mse')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  0%|          | 0/21 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_2 (Dense)              (None, 32)                288       \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 32)                1056      \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 32)                1056      \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "Output (Dense)               (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 2,433\n",
            "Trainable params: 2,433\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/100\n",
            "380/380 - 1s - loss: 2.0476 - mse: 5.3373 - val_loss: 2.0623 - val_mse: 5.4463\n",
            "Epoch 2/100\n",
            "380/380 - 1s - loss: 2.0326 - mse: 5.2763 - val_loss: 2.0483 - val_mse: 5.3845\n",
            "Epoch 3/100\n",
            "380/380 - 1s - loss: 2.0187 - mse: 5.2151 - val_loss: 2.0343 - val_mse: 5.3230\n",
            "Epoch 4/100\n",
            "380/380 - 1s - loss: 2.0037 - mse: 5.1549 - val_loss: 2.0203 - val_mse: 5.2622\n",
            "Epoch 5/100\n",
            "380/380 - 1s - loss: 1.9908 - mse: 5.0945 - val_loss: 2.0063 - val_mse: 5.2018\n",
            "Epoch 6/100\n",
            "380/380 - 1s - loss: 1.9766 - mse: 5.0352 - val_loss: 1.9924 - val_mse: 5.1421\n",
            "Epoch 7/100\n",
            "380/380 - 1s - loss: 1.9618 - mse: 4.9763 - val_loss: 1.9784 - val_mse: 5.0827\n",
            "Epoch 8/100\n",
            "380/380 - 1s - loss: 1.9485 - mse: 4.9181 - val_loss: 1.9644 - val_mse: 5.0238\n",
            "Epoch 9/100\n",
            "380/380 - 1s - loss: 1.9337 - mse: 4.8597 - val_loss: 1.9504 - val_mse: 4.9653\n",
            "Epoch 10/100\n",
            "380/380 - 1s - loss: 1.9206 - mse: 4.8021 - val_loss: 1.9363 - val_mse: 4.9076\n",
            "Epoch 11/100\n",
            "380/380 - 1s - loss: 1.9078 - mse: 4.7450 - val_loss: 1.9223 - val_mse: 4.8505\n",
            "Epoch 12/100\n",
            "380/380 - 1s - loss: 1.8929 - mse: 4.6889 - val_loss: 1.9083 - val_mse: 4.7934\n",
            "Epoch 13/100\n",
            "380/380 - 1s - loss: 1.8773 - mse: 4.6332 - val_loss: 1.8943 - val_mse: 4.7371\n",
            "Epoch 14/100\n",
            "380/380 - 1s - loss: 1.8650 - mse: 4.5774 - val_loss: 1.8802 - val_mse: 4.6807\n",
            "Epoch 15/100\n",
            "380/380 - 1s - loss: 1.8480 - mse: 4.5222 - val_loss: 1.8661 - val_mse: 4.6247\n",
            "Epoch 16/100\n",
            "380/380 - 1s - loss: 1.8350 - mse: 4.4675 - val_loss: 1.8519 - val_mse: 4.5693\n",
            "Epoch 17/100\n",
            "380/380 - 1s - loss: 1.8207 - mse: 4.4127 - val_loss: 1.8378 - val_mse: 4.5143\n",
            "Epoch 18/100\n",
            "380/380 - 1s - loss: 1.8060 - mse: 4.3589 - val_loss: 1.8236 - val_mse: 4.4596\n",
            "Epoch 19/100\n",
            "380/380 - 1s - loss: 1.7923 - mse: 4.3049 - val_loss: 1.8094 - val_mse: 4.4051\n",
            "Epoch 20/100\n",
            "380/380 - 1s - loss: 1.7771 - mse: 4.2520 - val_loss: 1.7951 - val_mse: 4.3512\n",
            "Epoch 21/100\n",
            "380/380 - 1s - loss: 1.7626 - mse: 4.1984 - val_loss: 1.7808 - val_mse: 4.2973\n",
            "Epoch 22/100\n",
            "380/380 - 1s - loss: 1.7489 - mse: 4.1457 - val_loss: 1.7664 - val_mse: 4.2440\n",
            "Epoch 23/100\n",
            "380/380 - 1s - loss: 1.7352 - mse: 4.0938 - val_loss: 1.7520 - val_mse: 4.1910\n",
            "Epoch 24/100\n",
            "380/380 - 1s - loss: 1.7208 - mse: 4.0415 - val_loss: 1.7375 - val_mse: 4.1378\n",
            "Epoch 25/100\n",
            "380/380 - 1s - loss: 1.7059 - mse: 3.9897 - val_loss: 1.7230 - val_mse: 4.0851\n",
            "Epoch 26/100\n",
            "380/380 - 1s - loss: 1.6907 - mse: 3.9378 - val_loss: 1.7084 - val_mse: 4.0327\n",
            "Epoch 27/100\n",
            "380/380 - 1s - loss: 1.6780 - mse: 3.8866 - val_loss: 1.6937 - val_mse: 3.9805\n",
            "Epoch 28/100\n",
            "380/380 - 1s - loss: 1.6617 - mse: 3.8355 - val_loss: 1.6791 - val_mse: 3.9288\n",
            "Epoch 29/100\n",
            "380/380 - 1s - loss: 1.6461 - mse: 3.7854 - val_loss: 1.6644 - val_mse: 3.8775\n",
            "Epoch 30/100\n",
            "380/380 - 1s - loss: 1.6336 - mse: 3.7348 - val_loss: 1.6496 - val_mse: 3.8263\n",
            "Epoch 31/100\n",
            "380/380 - 1s - loss: 1.6179 - mse: 3.6847 - val_loss: 1.6349 - val_mse: 3.7754\n",
            "Epoch 32/100\n",
            "380/380 - 1s - loss: 1.6035 - mse: 3.6351 - val_loss: 1.6201 - val_mse: 3.7247\n",
            "Epoch 33/100\n",
            "380/380 - 1s - loss: 1.5871 - mse: 3.5856 - val_loss: 1.6053 - val_mse: 3.6743\n",
            "Epoch 34/100\n",
            "380/380 - 1s - loss: 1.5725 - mse: 3.5360 - val_loss: 1.5905 - val_mse: 3.6239\n",
            "Epoch 35/100\n",
            "380/380 - 1s - loss: 1.5578 - mse: 3.4871 - val_loss: 1.5757 - val_mse: 3.5741\n",
            "Epoch 36/100\n",
            "380/380 - 1s - loss: 1.5426 - mse: 3.4385 - val_loss: 1.5608 - val_mse: 3.5244\n",
            "Epoch 37/100\n",
            "380/380 - 1s - loss: 1.5278 - mse: 3.3896 - val_loss: 1.5459 - val_mse: 3.4749\n",
            "Epoch 38/100\n",
            "380/380 - 1s - loss: 1.5145 - mse: 3.3415 - val_loss: 1.5310 - val_mse: 3.4256\n",
            "Epoch 39/100\n",
            "380/380 - 1s - loss: 1.4980 - mse: 3.2934 - val_loss: 1.5159 - val_mse: 3.3766\n",
            "Epoch 40/100\n",
            "380/380 - 1s - loss: 1.4810 - mse: 3.2460 - val_loss: 1.5009 - val_mse: 3.3278\n",
            "Epoch 41/100\n",
            "380/380 - 1s - loss: 1.4671 - mse: 3.1987 - val_loss: 1.4859 - val_mse: 3.2794\n",
            "Epoch 42/100\n",
            "380/380 - 1s - loss: 1.4536 - mse: 3.1515 - val_loss: 1.4709 - val_mse: 3.2311\n",
            "Epoch 43/100\n",
            "380/380 - 1s - loss: 1.4383 - mse: 3.1045 - val_loss: 1.4559 - val_mse: 3.1832\n",
            "Epoch 44/100\n",
            "380/380 - 1s - loss: 1.4219 - mse: 3.0575 - val_loss: 1.4408 - val_mse: 3.1350\n",
            "Epoch 45/100\n",
            "380/380 - 1s - loss: 1.4082 - mse: 3.0113 - val_loss: 1.4257 - val_mse: 3.0874\n",
            "Epoch 46/100\n",
            "380/380 - 1s - loss: 1.3919 - mse: 2.9647 - val_loss: 1.4106 - val_mse: 3.0398\n",
            "Epoch 47/100\n",
            "380/380 - 1s - loss: 1.3761 - mse: 2.9183 - val_loss: 1.3954 - val_mse: 2.9921\n",
            "Epoch 48/100\n",
            "380/380 - 1s - loss: 1.3623 - mse: 2.8720 - val_loss: 1.3802 - val_mse: 2.9447\n",
            "Epoch 49/100\n",
            "380/380 - 1s - loss: 1.3477 - mse: 2.8264 - val_loss: 1.3651 - val_mse: 2.8976\n",
            "Epoch 50/100\n",
            "380/380 - 1s - loss: 1.3316 - mse: 2.7802 - val_loss: 1.3499 - val_mse: 2.8505\n",
            "Epoch 51/100\n",
            "380/380 - 1s - loss: 1.3163 - mse: 2.7350 - val_loss: 1.3347 - val_mse: 2.8036\n",
            "Epoch 52/100\n",
            "380/380 - 1s - loss: 1.3008 - mse: 2.6899 - val_loss: 1.3197 - val_mse: 2.7569\n",
            "Epoch 53/100\n",
            "380/380 - 1s - loss: 1.2849 - mse: 2.6447 - val_loss: 1.3046 - val_mse: 2.7105\n",
            "Epoch 54/100\n",
            "380/380 - 1s - loss: 1.2715 - mse: 2.6001 - val_loss: 1.2897 - val_mse: 2.6645\n",
            "Epoch 55/100\n",
            "380/380 - 1s - loss: 1.2566 - mse: 2.5558 - val_loss: 1.2747 - val_mse: 2.6185\n",
            "Epoch 56/100\n",
            "380/380 - 1s - loss: 1.2416 - mse: 2.5114 - val_loss: 1.2598 - val_mse: 2.5731\n",
            "Epoch 57/100\n",
            "380/380 - 1s - loss: 1.2257 - mse: 2.4676 - val_loss: 1.2448 - val_mse: 2.5275\n",
            "Epoch 58/100\n",
            "380/380 - 1s - loss: 1.2114 - mse: 2.4240 - val_loss: 1.2300 - val_mse: 2.4825\n",
            "Epoch 59/100\n",
            "380/380 - 1s - loss: 1.1973 - mse: 2.3804 - val_loss: 1.2150 - val_mse: 2.4373\n",
            "Epoch 60/100\n",
            "380/380 - 1s - loss: 1.1814 - mse: 2.3374 - val_loss: 1.2001 - val_mse: 2.3923\n",
            "Epoch 61/100\n",
            "380/380 - 1s - loss: 1.1662 - mse: 2.2944 - val_loss: 1.1853 - val_mse: 2.3476\n",
            "Epoch 62/100\n",
            "380/380 - 1s - loss: 1.1538 - mse: 2.2518 - val_loss: 1.1705 - val_mse: 2.3035\n",
            "Epoch 63/100\n",
            "380/380 - 1s - loss: 1.1375 - mse: 2.2093 - val_loss: 1.1558 - val_mse: 2.2590\n",
            "Epoch 64/100\n",
            "380/380 - 1s - loss: 1.1253 - mse: 2.1673 - val_loss: 1.1412 - val_mse: 2.2154\n",
            "Epoch 65/100\n",
            "380/380 - 1s - loss: 1.1100 - mse: 2.1251 - val_loss: 1.1266 - val_mse: 2.1714\n",
            "Epoch 66/100\n",
            "380/380 - 1s - loss: 1.0941 - mse: 2.0836 - val_loss: 1.1120 - val_mse: 2.1279\n",
            "Epoch 67/100\n",
            "380/380 - 1s - loss: 1.0818 - mse: 2.0426 - val_loss: 1.0975 - val_mse: 2.0847\n",
            "Epoch 68/100\n",
            "380/380 - 1s - loss: 1.0660 - mse: 2.0015 - val_loss: 1.0831 - val_mse: 2.0419\n",
            "Epoch 69/100\n",
            "380/380 - 1s - loss: 1.0541 - mse: 1.9610 - val_loss: 1.0687 - val_mse: 1.9995\n",
            "Epoch 70/100\n",
            "380/380 - 1s - loss: 1.0394 - mse: 1.9206 - val_loss: 1.0544 - val_mse: 1.9574\n",
            "Epoch 71/100\n",
            "380/380 - 1s - loss: 1.0277 - mse: 1.8807 - val_loss: 1.0401 - val_mse: 1.9158\n",
            "Epoch 72/100\n",
            "380/380 - 1s - loss: 1.0123 - mse: 1.8420 - val_loss: 1.0261 - val_mse: 1.8747\n",
            "Epoch 73/100\n",
            "380/380 - 1s - loss: 0.9970 - mse: 1.8027 - val_loss: 1.0121 - val_mse: 1.8338\n",
            "Epoch 74/100\n",
            "380/380 - 1s - loss: 0.9835 - mse: 1.7646 - val_loss: 0.9983 - val_mse: 1.7938\n",
            "Epoch 75/100\n",
            "380/380 - 1s - loss: 0.9717 - mse: 1.7264 - val_loss: 0.9847 - val_mse: 1.7540\n",
            "Epoch 76/100\n",
            "380/380 - 1s - loss: 0.9579 - mse: 1.6891 - val_loss: 0.9711 - val_mse: 1.7147\n",
            "Epoch 77/100\n",
            "380/380 - 1s - loss: 0.9448 - mse: 1.6523 - val_loss: 0.9577 - val_mse: 1.6761\n",
            "Epoch 78/100\n",
            "380/380 - 1s - loss: 0.9332 - mse: 1.6159 - val_loss: 0.9444 - val_mse: 1.6382\n",
            "Epoch 79/100\n",
            "380/380 - 1s - loss: 0.9178 - mse: 1.5799 - val_loss: 0.9311 - val_mse: 1.6004\n",
            "Epoch 80/100\n",
            "380/380 - 1s - loss: 0.9054 - mse: 1.5447 - val_loss: 0.9180 - val_mse: 1.5634\n",
            "Epoch 81/100\n",
            "380/380 - 1s - loss: 0.8941 - mse: 1.5100 - val_loss: 0.9051 - val_mse: 1.5275\n",
            "Epoch 82/100\n",
            "380/380 - 1s - loss: 0.8808 - mse: 1.4761 - val_loss: 0.8924 - val_mse: 1.4918\n",
            "Epoch 83/100\n",
            "380/380 - 1s - loss: 0.8679 - mse: 1.4429 - val_loss: 0.8798 - val_mse: 1.4572\n",
            "Epoch 84/100\n",
            "380/380 - 1s - loss: 0.8560 - mse: 1.4108 - val_loss: 0.8675 - val_mse: 1.4236\n",
            "Epoch 85/100\n",
            "380/380 - 1s - loss: 0.8468 - mse: 1.3793 - val_loss: 0.8556 - val_mse: 1.3909\n",
            "Epoch 86/100\n",
            "380/380 - 1s - loss: 0.8330 - mse: 1.3487 - val_loss: 0.8439 - val_mse: 1.3589\n",
            "Epoch 87/100\n",
            "380/380 - 1s - loss: 0.8227 - mse: 1.3188 - val_loss: 0.8325 - val_mse: 1.3278\n",
            "Epoch 88/100\n",
            "380/380 - 1s - loss: 0.8124 - mse: 1.2895 - val_loss: 0.8212 - val_mse: 1.2972\n",
            "Epoch 89/100\n",
            "380/380 - 1s - loss: 0.8011 - mse: 1.2609 - val_loss: 0.8101 - val_mse: 1.2676\n",
            "Epoch 90/100\n",
            "380/380 - 1s - loss: 0.7907 - mse: 1.2332 - val_loss: 0.7994 - val_mse: 1.2390\n",
            "Epoch 91/100\n",
            "380/380 - 1s - loss: 0.7790 - mse: 1.2067 - val_loss: 0.7887 - val_mse: 1.2111\n",
            "Epoch 92/100\n",
            "380/380 - 1s - loss: 0.7700 - mse: 1.1807 - val_loss: 0.7784 - val_mse: 1.1842\n",
            "Epoch 93/100\n",
            "380/380 - 1s - loss: 0.7595 - mse: 1.1556 - val_loss: 0.7684 - val_mse: 1.1583\n",
            "Epoch 94/100\n",
            "380/380 - 1s - loss: 0.7501 - mse: 1.1313 - val_loss: 0.7587 - val_mse: 1.1334\n",
            "Epoch 95/100\n",
            "380/380 - 1s - loss: 0.7416 - mse: 1.1083 - val_loss: 0.7494 - val_mse: 1.1093\n",
            "Epoch 96/100\n",
            "380/380 - 1s - loss: 0.7327 - mse: 1.0858 - val_loss: 0.7404 - val_mse: 1.0863\n",
            "Epoch 97/100\n",
            "380/380 - 1s - loss: 0.7265 - mse: 1.0645 - val_loss: 0.7318 - val_mse: 1.0644\n",
            "Epoch 98/100\n",
            "380/380 - 1s - loss: 0.7177 - mse: 1.0440 - val_loss: 0.7235 - val_mse: 1.0433\n",
            "Epoch 99/100\n",
            "380/380 - 1s - loss: 0.7098 - mse: 1.0246 - val_loss: 0.7154 - val_mse: 1.0231\n",
            "Epoch 100/100\n",
            "380/380 - 1s - loss: 0.7019 - mse: 1.0056 - val_loss: 0.7077 - val_mse: 1.0037\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  5%|▍         | 1/21 [01:14<24:48, 74.41s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 64)                576       \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "Output (Dense)               (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 641\n",
            "Trainable params: 641\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/100\n",
            "380/380 - 1s - loss: 5.5920 - mse: 5.5899 - val_loss: 5.7163 - val_mse: 5.7165\n",
            "Epoch 2/100\n",
            "380/380 - 1s - loss: 5.4012 - mse: 5.4035 - val_loss: 5.5275 - val_mse: 5.5277\n",
            "Epoch 3/100\n",
            "380/380 - 1s - loss: 5.2209 - mse: 5.2243 - val_loss: 5.3460 - val_mse: 5.3462\n",
            "Epoch 4/100\n",
            "380/380 - 1s - loss: 5.0442 - mse: 5.0522 - val_loss: 5.1715 - val_mse: 5.1718\n",
            "Epoch 5/100\n",
            "380/380 - 1s - loss: 4.8867 - mse: 4.8868 - val_loss: 5.0034 - val_mse: 5.0037\n",
            "Epoch 6/100\n",
            "380/380 - 1s - loss: 4.7279 - mse: 4.7275 - val_loss: 4.8416 - val_mse: 4.8419\n",
            "Epoch 7/100\n",
            "380/380 - 1s - loss: 4.5686 - mse: 4.5741 - val_loss: 4.6859 - val_mse: 4.6862\n",
            "Epoch 8/100\n",
            "380/380 - 1s - loss: 4.4243 - mse: 4.4265 - val_loss: 4.5357 - val_mse: 4.5361\n",
            "Epoch 9/100\n",
            "380/380 - 1s - loss: 4.2796 - mse: 4.2843 - val_loss: 4.3911 - val_mse: 4.3915\n",
            "Epoch 10/100\n",
            "380/380 - 1s - loss: 4.1479 - mse: 4.1473 - val_loss: 4.2514 - val_mse: 4.2519\n",
            "Epoch 11/100\n",
            "380/380 - 1s - loss: 4.0209 - mse: 4.0152 - val_loss: 4.1167 - val_mse: 4.1172\n",
            "Epoch 12/100\n",
            "380/380 - 1s - loss: 3.8888 - mse: 3.8878 - val_loss: 3.9867 - val_mse: 3.9873\n",
            "Epoch 13/100\n",
            "380/380 - 1s - loss: 3.7609 - mse: 3.7648 - val_loss: 3.8615 - val_mse: 3.8620\n",
            "Epoch 14/100\n",
            "380/380 - 1s - loss: 3.6534 - mse: 3.6464 - val_loss: 3.7404 - val_mse: 3.7410\n",
            "Epoch 15/100\n",
            "380/380 - 1s - loss: 3.5267 - mse: 3.5320 - val_loss: 3.6238 - val_mse: 3.6244\n",
            "Epoch 16/100\n",
            "380/380 - 1s - loss: 3.4186 - mse: 3.4218 - val_loss: 3.5112 - val_mse: 3.5118\n",
            "Epoch 17/100\n",
            "380/380 - 1s - loss: 3.3134 - mse: 3.3155 - val_loss: 3.4025 - val_mse: 3.4031\n",
            "Epoch 18/100\n",
            "380/380 - 1s - loss: 3.2117 - mse: 3.2129 - val_loss: 3.2975 - val_mse: 3.2982\n",
            "Epoch 19/100\n",
            "380/380 - 1s - loss: 3.1128 - mse: 3.1138 - val_loss: 3.1961 - val_mse: 3.1968\n",
            "Epoch 20/100\n",
            "380/380 - 1s - loss: 3.0134 - mse: 3.0182 - val_loss: 3.0984 - val_mse: 3.0991\n",
            "Epoch 21/100\n",
            "380/380 - 1s - loss: 2.9223 - mse: 2.9261 - val_loss: 3.0041 - val_mse: 3.0048\n",
            "Epoch 22/100\n",
            "380/380 - 1s - loss: 2.8349 - mse: 2.8373 - val_loss: 2.9129 - val_mse: 2.9137\n",
            "Epoch 23/100\n",
            "380/380 - 1s - loss: 2.7532 - mse: 2.7515 - val_loss: 2.8249 - val_mse: 2.8257\n",
            "Epoch 24/100\n",
            "380/380 - 1s - loss: 2.6702 - mse: 2.6686 - val_loss: 2.7400 - val_mse: 2.7408\n",
            "Epoch 25/100\n",
            "380/380 - 1s - loss: 2.5884 - mse: 2.5887 - val_loss: 2.6580 - val_mse: 2.6589\n",
            "Epoch 26/100\n",
            "380/380 - 1s - loss: 2.5097 - mse: 2.5117 - val_loss: 2.5791 - val_mse: 2.5800\n",
            "Epoch 27/100\n",
            "380/380 - 1s - loss: 2.4430 - mse: 2.4374 - val_loss: 2.5026 - val_mse: 2.5035\n",
            "Epoch 28/100\n",
            "380/380 - 1s - loss: 2.3678 - mse: 2.3656 - val_loss: 2.4292 - val_mse: 2.4302\n",
            "Epoch 29/100\n",
            "380/380 - 1s - loss: 2.2929 - mse: 2.2967 - val_loss: 2.3585 - val_mse: 2.3595\n",
            "Epoch 30/100\n",
            "380/380 - 1s - loss: 2.2345 - mse: 2.2304 - val_loss: 2.2901 - val_mse: 2.2911\n",
            "Epoch 31/100\n",
            "380/380 - 1s - loss: 2.1669 - mse: 2.1662 - val_loss: 2.2243 - val_mse: 2.2253\n",
            "Epoch 32/100\n",
            "380/380 - 1s - loss: 2.1089 - mse: 2.1045 - val_loss: 2.1609 - val_mse: 2.1619\n",
            "Epoch 33/100\n",
            "380/380 - 1s - loss: 2.0416 - mse: 2.0451 - val_loss: 2.0999 - val_mse: 2.1009\n",
            "Epoch 34/100\n",
            "380/380 - 1s - loss: 1.9882 - mse: 1.9879 - val_loss: 2.0411 - val_mse: 2.0422\n",
            "Epoch 35/100\n",
            "380/380 - 1s - loss: 1.9322 - mse: 1.9329 - val_loss: 1.9845 - val_mse: 1.9855\n",
            "Epoch 36/100\n",
            "380/380 - 1s - loss: 1.8781 - mse: 1.8799 - val_loss: 1.9300 - val_mse: 1.9311\n",
            "Epoch 37/100\n",
            "380/380 - 1s - loss: 1.8289 - mse: 1.8290 - val_loss: 1.8775 - val_mse: 1.8786\n",
            "Epoch 38/100\n",
            "380/380 - 1s - loss: 1.7832 - mse: 1.7799 - val_loss: 1.8269 - val_mse: 1.8280\n",
            "Epoch 39/100\n",
            "380/380 - 1s - loss: 1.7379 - mse: 1.7327 - val_loss: 1.7783 - val_mse: 1.7795\n",
            "Epoch 40/100\n",
            "380/380 - 1s - loss: 1.6840 - mse: 1.6873 - val_loss: 1.7318 - val_mse: 1.7329\n",
            "Epoch 41/100\n",
            "380/380 - 1s - loss: 1.6413 - mse: 1.6438 - val_loss: 1.6870 - val_mse: 1.6881\n",
            "Epoch 42/100\n",
            "380/380 - 1s - loss: 1.6054 - mse: 1.6020 - val_loss: 1.6438 - val_mse: 1.6449\n",
            "Epoch 43/100\n",
            "380/380 - 1s - loss: 1.5630 - mse: 1.5618 - val_loss: 1.6023 - val_mse: 1.6035\n",
            "Epoch 44/100\n",
            "380/380 - 1s - loss: 1.5213 - mse: 1.5232 - val_loss: 1.5625 - val_mse: 1.5637\n",
            "Epoch 45/100\n",
            "380/380 - 1s - loss: 1.4890 - mse: 1.4861 - val_loss: 1.5243 - val_mse: 1.5255\n",
            "Epoch 46/100\n",
            "380/380 - 1s - loss: 1.4484 - mse: 1.4505 - val_loss: 1.4876 - val_mse: 1.4888\n",
            "Epoch 47/100\n",
            "380/380 - 1s - loss: 1.4141 - mse: 1.4164 - val_loss: 1.4525 - val_mse: 1.4537\n",
            "Epoch 48/100\n",
            "380/380 - 1s - loss: 1.3844 - mse: 1.3837 - val_loss: 1.4186 - val_mse: 1.4199\n",
            "Epoch 49/100\n",
            "380/380 - 1s - loss: 1.3530 - mse: 1.3523 - val_loss: 1.3862 - val_mse: 1.3874\n",
            "Epoch 50/100\n",
            "380/380 - 1s - loss: 1.3247 - mse: 1.3221 - val_loss: 1.3551 - val_mse: 1.3563\n",
            "Epoch 51/100\n",
            "380/380 - 1s - loss: 1.2932 - mse: 1.2932 - val_loss: 1.3253 - val_mse: 1.3266\n",
            "Epoch 52/100\n",
            "380/380 - 1s - loss: 1.2641 - mse: 1.2656 - val_loss: 1.2968 - val_mse: 1.2980\n",
            "Epoch 53/100\n",
            "380/380 - 1s - loss: 1.2371 - mse: 1.2391 - val_loss: 1.2694 - val_mse: 1.2707\n",
            "Epoch 54/100\n",
            "380/380 - 1s - loss: 1.2135 - mse: 1.2138 - val_loss: 1.2432 - val_mse: 1.2445\n",
            "Epoch 55/100\n",
            "380/380 - 1s - loss: 1.1885 - mse: 1.1894 - val_loss: 1.2181 - val_mse: 1.2194\n",
            "Epoch 56/100\n",
            "380/380 - 1s - loss: 1.1651 - mse: 1.1662 - val_loss: 1.1940 - val_mse: 1.1953\n",
            "Epoch 57/100\n",
            "380/380 - 1s - loss: 1.1428 - mse: 1.1439 - val_loss: 1.1710 - val_mse: 1.1723\n",
            "Epoch 58/100\n",
            "380/380 - 1s - loss: 1.1212 - mse: 1.1226 - val_loss: 1.1490 - val_mse: 1.1503\n",
            "Epoch 59/100\n",
            "380/380 - 1s - loss: 1.1032 - mse: 1.1022 - val_loss: 1.1279 - val_mse: 1.1292\n",
            "Epoch 60/100\n",
            "380/380 - 1s - loss: 1.0807 - mse: 1.0827 - val_loss: 1.1078 - val_mse: 1.1091\n",
            "Epoch 61/100\n",
            "380/380 - 1s - loss: 1.0623 - mse: 1.0641 - val_loss: 1.0885 - val_mse: 1.0898\n",
            "Epoch 62/100\n",
            "380/380 - 1s - loss: 1.0459 - mse: 1.0463 - val_loss: 1.0700 - val_mse: 1.0714\n",
            "Epoch 63/100\n",
            "380/380 - 1s - loss: 1.0283 - mse: 1.0292 - val_loss: 1.0524 - val_mse: 1.0537\n",
            "Epoch 64/100\n",
            "380/380 - 1s - loss: 1.0137 - mse: 1.0129 - val_loss: 1.0354 - val_mse: 1.0368\n",
            "Epoch 65/100\n",
            "380/380 - 1s - loss: 0.9979 - mse: 0.9972 - val_loss: 1.0193 - val_mse: 1.0207\n",
            "Epoch 66/100\n",
            "380/380 - 1s - loss: 0.9810 - mse: 0.9823 - val_loss: 1.0039 - val_mse: 1.0053\n",
            "Epoch 67/100\n",
            "380/380 - 1s - loss: 0.9691 - mse: 0.9681 - val_loss: 0.9891 - val_mse: 0.9905\n",
            "Epoch 68/100\n",
            "380/380 - 1s - loss: 0.9528 - mse: 0.9545 - val_loss: 0.9751 - val_mse: 0.9764\n",
            "Epoch 69/100\n",
            "380/380 - 1s - loss: 0.9427 - mse: 0.9415 - val_loss: 0.9616 - val_mse: 0.9629\n",
            "Epoch 70/100\n",
            "380/380 - 1s - loss: 0.9287 - mse: 0.9291 - val_loss: 0.9487 - val_mse: 0.9501\n",
            "Epoch 71/100\n",
            "380/380 - 1s - loss: 0.9221 - mse: 0.9172 - val_loss: 0.9364 - val_mse: 0.9377\n",
            "Epoch 72/100\n",
            "380/380 - 1s - loss: 0.9052 - mse: 0.9058 - val_loss: 0.9246 - val_mse: 0.9260\n",
            "Epoch 73/100\n",
            "380/380 - 1s - loss: 0.8940 - mse: 0.8950 - val_loss: 0.9134 - val_mse: 0.9148\n",
            "Epoch 74/100\n",
            "380/380 - 1s - loss: 0.8835 - mse: 0.8847 - val_loss: 0.9027 - val_mse: 0.9041\n",
            "Epoch 75/100\n",
            "380/380 - 1s - loss: 0.8779 - mse: 0.8748 - val_loss: 0.8925 - val_mse: 0.8938\n",
            "Epoch 76/100\n",
            "380/380 - 1s - loss: 0.8640 - mse: 0.8654 - val_loss: 0.8827 - val_mse: 0.8841\n",
            "Epoch 77/100\n",
            "380/380 - 1s - loss: 0.8565 - mse: 0.8564 - val_loss: 0.8734 - val_mse: 0.8747\n",
            "Epoch 78/100\n",
            "380/380 - 1s - loss: 0.8497 - mse: 0.8478 - val_loss: 0.8644 - val_mse: 0.8658\n",
            "Epoch 79/100\n",
            "380/380 - 1s - loss: 0.8390 - mse: 0.8396 - val_loss: 0.8559 - val_mse: 0.8573\n",
            "Epoch 80/100\n",
            "380/380 - 1s - loss: 0.8303 - mse: 0.8317 - val_loss: 0.8478 - val_mse: 0.8492\n",
            "Epoch 81/100\n",
            "380/380 - 1s - loss: 0.8240 - mse: 0.8243 - val_loss: 0.8400 - val_mse: 0.8414\n",
            "Epoch 82/100\n",
            "380/380 - 1s - loss: 0.8161 - mse: 0.8171 - val_loss: 0.8326 - val_mse: 0.8340\n",
            "Epoch 83/100\n",
            "380/380 - 1s - loss: 0.8090 - mse: 0.8103 - val_loss: 0.8255 - val_mse: 0.8269\n",
            "Epoch 84/100\n",
            "380/380 - 1s - loss: 0.8024 - mse: 0.8038 - val_loss: 0.8188 - val_mse: 0.8202\n",
            "Epoch 85/100\n",
            "380/380 - 1s - loss: 0.8011 - mse: 0.7976 - val_loss: 0.8123 - val_mse: 0.8137\n",
            "Epoch 86/100\n",
            "380/380 - 1s - loss: 0.7904 - mse: 0.7916 - val_loss: 0.8061 - val_mse: 0.8075\n",
            "Epoch 87/100\n",
            "380/380 - 1s - loss: 0.7851 - mse: 0.7859 - val_loss: 0.8002 - val_mse: 0.8016\n",
            "Epoch 88/100\n",
            "380/380 - 1s - loss: 0.7823 - mse: 0.7805 - val_loss: 0.7945 - val_mse: 0.7959\n",
            "Epoch 89/100\n",
            "380/380 - 1s - loss: 0.7748 - mse: 0.7753 - val_loss: 0.7891 - val_mse: 0.7905\n",
            "Epoch 90/100\n",
            "380/380 - 1s - loss: 0.7698 - mse: 0.7703 - val_loss: 0.7839 - val_mse: 0.7853\n",
            "Epoch 91/100\n",
            "380/380 - 1s - loss: 0.7651 - mse: 0.7655 - val_loss: 0.7790 - val_mse: 0.7804\n",
            "Epoch 92/100\n",
            "380/380 - 1s - loss: 0.7612 - mse: 0.7610 - val_loss: 0.7743 - val_mse: 0.7757\n",
            "Epoch 93/100\n",
            "380/380 - 1s - loss: 0.7556 - mse: 0.7566 - val_loss: 0.7698 - val_mse: 0.7712\n",
            "Epoch 94/100\n",
            "380/380 - 1s - loss: 0.7512 - mse: 0.7525 - val_loss: 0.7655 - val_mse: 0.7668\n",
            "Epoch 95/100\n",
            "380/380 - 1s - loss: 0.7474 - mse: 0.7485 - val_loss: 0.7613 - val_mse: 0.7627\n",
            "Epoch 96/100\n",
            "380/380 - 1s - loss: 0.7443 - mse: 0.7447 - val_loss: 0.7574 - val_mse: 0.7587\n",
            "Epoch 97/100\n",
            "380/380 - 1s - loss: 0.7426 - mse: 0.7410 - val_loss: 0.7535 - val_mse: 0.7549\n",
            "Epoch 98/100\n",
            "380/380 - 1s - loss: 0.7407 - mse: 0.7375 - val_loss: 0.7499 - val_mse: 0.7513\n",
            "Epoch 99/100\n",
            "380/380 - 1s - loss: 0.7332 - mse: 0.7342 - val_loss: 0.7464 - val_mse: 0.7478\n",
            "Epoch 100/100\n",
            "380/380 - 1s - loss: 0.7299 - mse: 0.7309 - val_loss: 0.7431 - val_mse: 0.7444\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 10%|▉         | 2/21 [02:18<22:33, 71.26s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 64)                576       \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "Output (Dense)               (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 8,961\n",
            "Trainable params: 8,961\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/100\n",
            "380/380 - 1s - loss: 5.1916 - mse: 5.1890 - val_loss: 5.3754 - val_mse: 5.3757\n",
            "Epoch 2/100\n",
            "380/380 - 1s - loss: 5.0738 - mse: 5.0779 - val_loss: 5.2618 - val_mse: 5.2621\n",
            "Epoch 3/100\n",
            "380/380 - 1s - loss: 4.9658 - mse: 4.9695 - val_loss: 5.1493 - val_mse: 5.1497\n",
            "Epoch 4/100\n",
            "380/380 - 1s - loss: 4.8566 - mse: 4.8632 - val_loss: 5.0403 - val_mse: 5.0408\n",
            "Epoch 5/100\n",
            "380/380 - 1s - loss: 4.7575 - mse: 4.7593 - val_loss: 4.9333 - val_mse: 4.9338\n",
            "Epoch 6/100\n",
            "380/380 - 1s - loss: 4.6555 - mse: 4.6574 - val_loss: 4.8289 - val_mse: 4.8295\n",
            "Epoch 7/100\n",
            "380/380 - 1s - loss: 4.5520 - mse: 4.5577 - val_loss: 4.7261 - val_mse: 4.7267\n",
            "Epoch 8/100\n",
            "380/380 - 1s - loss: 4.4570 - mse: 4.4598 - val_loss: 4.6254 - val_mse: 4.6261\n",
            "Epoch 9/100\n",
            "380/380 - 1s - loss: 4.3569 - mse: 4.3636 - val_loss: 4.5258 - val_mse: 4.5266\n",
            "Epoch 10/100\n",
            "380/380 - 1s - loss: 4.2707 - mse: 4.2690 - val_loss: 4.4285 - val_mse: 4.4294\n",
            "Epoch 11/100\n",
            "380/380 - 1s - loss: 4.1827 - mse: 4.1760 - val_loss: 4.3331 - val_mse: 4.3340\n",
            "Epoch 12/100\n",
            "380/380 - 1s - loss: 4.0854 - mse: 4.0850 - val_loss: 4.2399 - val_mse: 4.2408\n",
            "Epoch 13/100\n",
            "380/380 - 1s - loss: 3.9929 - mse: 3.9960 - val_loss: 4.1485 - val_mse: 4.1495\n",
            "Epoch 14/100\n",
            "380/380 - 1s - loss: 3.9179 - mse: 3.9089 - val_loss: 4.0587 - val_mse: 4.0598\n",
            "Epoch 15/100\n",
            "380/380 - 1s - loss: 3.8172 - mse: 3.8232 - val_loss: 3.9708 - val_mse: 3.9719\n",
            "Epoch 16/100\n",
            "380/380 - 1s - loss: 3.7371 - mse: 3.7397 - val_loss: 3.8852 - val_mse: 3.8863\n",
            "Epoch 17/100\n",
            "380/380 - 1s - loss: 3.6552 - mse: 3.6579 - val_loss: 3.8011 - val_mse: 3.8023\n",
            "Epoch 18/100\n",
            "380/380 - 1s - loss: 3.5774 - mse: 3.5784 - val_loss: 3.7199 - val_mse: 3.7211\n",
            "Epoch 19/100\n",
            "380/380 - 1s - loss: 3.4990 - mse: 3.5011 - val_loss: 3.6407 - val_mse: 3.6420\n",
            "Epoch 20/100\n",
            "380/380 - 1s - loss: 3.4205 - mse: 3.4257 - val_loss: 3.5634 - val_mse: 3.5647\n",
            "Epoch 21/100\n",
            "380/380 - 1s - loss: 3.3475 - mse: 3.3523 - val_loss: 3.4883 - val_mse: 3.4896\n",
            "Epoch 22/100\n",
            "380/380 - 1s - loss: 3.2784 - mse: 3.2810 - val_loss: 3.4154 - val_mse: 3.4168\n",
            "Epoch 23/100\n",
            "380/380 - 1s - loss: 3.2146 - mse: 3.2115 - val_loss: 3.3444 - val_mse: 3.3459\n",
            "Epoch 24/100\n",
            "380/380 - 1s - loss: 3.1469 - mse: 3.1443 - val_loss: 3.2759 - val_mse: 3.2774\n",
            "Epoch 25/100\n",
            "380/380 - 1s - loss: 3.0793 - mse: 3.0792 - val_loss: 3.2090 - val_mse: 3.2105\n",
            "Epoch 26/100\n",
            "380/380 - 1s - loss: 3.0127 - mse: 3.0161 - val_loss: 3.1454 - val_mse: 3.1469\n",
            "Epoch 27/100\n",
            "380/380 - 1s - loss: 2.9629 - mse: 2.9552 - val_loss: 3.0822 - val_mse: 3.0838\n",
            "Epoch 28/100\n",
            "380/380 - 1s - loss: 2.8969 - mse: 2.8960 - val_loss: 3.0233 - val_mse: 3.0249\n",
            "Epoch 29/100\n",
            "380/380 - 1s - loss: 2.8354 - mse: 2.8393 - val_loss: 2.9653 - val_mse: 2.9669\n",
            "Epoch 30/100\n",
            "380/380 - 1s - loss: 2.7892 - mse: 2.7844 - val_loss: 2.9093 - val_mse: 2.9110\n",
            "Epoch 31/100\n",
            "380/380 - 1s - loss: 2.7301 - mse: 2.7314 - val_loss: 2.8559 - val_mse: 2.8576\n",
            "Epoch 32/100\n",
            "380/380 - 1s - loss: 2.6806 - mse: 2.6805 - val_loss: 2.8043 - val_mse: 2.8060\n",
            "Epoch 33/100\n",
            "380/380 - 1s - loss: 2.6288 - mse: 2.6314 - val_loss: 2.7544 - val_mse: 2.7561\n",
            "Epoch 34/100\n",
            "380/380 - 1s - loss: 2.5826 - mse: 2.5841 - val_loss: 2.7065 - val_mse: 2.7082\n",
            "Epoch 35/100\n",
            "380/380 - 1s - loss: 2.5375 - mse: 2.5388 - val_loss: 2.6604 - val_mse: 2.6621\n",
            "Epoch 36/100\n",
            "380/380 - 1s - loss: 2.4925 - mse: 2.4952 - val_loss: 2.6158 - val_mse: 2.6175\n",
            "Epoch 37/100\n",
            "380/380 - 1s - loss: 2.4527 - mse: 2.4532 - val_loss: 2.5728 - val_mse: 2.5745\n",
            "Epoch 38/100\n",
            "380/380 - 1s - loss: 2.4170 - mse: 2.4125 - val_loss: 2.5311 - val_mse: 2.5329\n",
            "Epoch 39/100\n",
            "380/380 - 1s - loss: 2.3739 - mse: 2.3734 - val_loss: 2.4913 - val_mse: 2.4931\n",
            "Epoch 40/100\n",
            "380/380 - 1s - loss: 2.3318 - mse: 2.3361 - val_loss: 2.4530 - val_mse: 2.4547\n",
            "Epoch 41/100\n",
            "380/380 - 1s - loss: 2.2968 - mse: 2.3001 - val_loss: 2.4159 - val_mse: 2.4177\n",
            "Epoch 42/100\n",
            "380/380 - 1s - loss: 2.2691 - mse: 2.2650 - val_loss: 2.3797 - val_mse: 2.3815\n",
            "Epoch 43/100\n",
            "380/380 - 1s - loss: 2.2342 - mse: 2.2309 - val_loss: 2.3451 - val_mse: 2.3469\n",
            "Epoch 44/100\n",
            "380/380 - 1s - loss: 2.1942 - mse: 2.1980 - val_loss: 2.3114 - val_mse: 2.3132\n",
            "Epoch 45/100\n",
            "380/380 - 1s - loss: 2.1700 - mse: 2.1662 - val_loss: 2.2787 - val_mse: 2.2805\n",
            "Epoch 46/100\n",
            "380/380 - 1s - loss: 2.1338 - mse: 2.1352 - val_loss: 2.2469 - val_mse: 2.2487\n",
            "Epoch 47/100\n",
            "380/380 - 1s - loss: 2.1025 - mse: 2.1052 - val_loss: 2.2160 - val_mse: 2.2178\n",
            "Epoch 48/100\n",
            "380/380 - 1s - loss: 2.0754 - mse: 2.0761 - val_loss: 2.1861 - val_mse: 2.1879\n",
            "Epoch 49/100\n",
            "380/380 - 1s - loss: 2.0505 - mse: 2.0479 - val_loss: 2.1570 - val_mse: 2.1588\n",
            "Epoch 50/100\n",
            "380/380 - 1s - loss: 2.0213 - mse: 2.0203 - val_loss: 2.1287 - val_mse: 2.1305\n",
            "Epoch 51/100\n",
            "380/380 - 1s - loss: 1.9917 - mse: 1.9935 - val_loss: 2.1011 - val_mse: 2.1028\n",
            "Epoch 52/100\n",
            "380/380 - 1s - loss: 1.9648 - mse: 1.9675 - val_loss: 2.0742 - val_mse: 2.0760\n",
            "Epoch 53/100\n",
            "380/380 - 1s - loss: 1.9388 - mse: 1.9422 - val_loss: 2.0481 - val_mse: 2.0499\n",
            "Epoch 54/100\n",
            "380/380 - 1s - loss: 1.9181 - mse: 1.9176 - val_loss: 2.0226 - val_mse: 2.0244\n",
            "Epoch 55/100\n",
            "380/380 - 1s - loss: 1.8920 - mse: 1.8935 - val_loss: 1.9978 - val_mse: 1.9996\n",
            "Epoch 56/100\n",
            "380/380 - 1s - loss: 1.8684 - mse: 1.8701 - val_loss: 1.9736 - val_mse: 1.9753\n",
            "Epoch 57/100\n",
            "380/380 - 1s - loss: 1.8450 - mse: 1.8474 - val_loss: 1.9501 - val_mse: 1.9518\n",
            "Epoch 58/100\n",
            "380/380 - 1s - loss: 1.8232 - mse: 1.8253 - val_loss: 1.9271 - val_mse: 1.9289\n",
            "Epoch 59/100\n",
            "380/380 - 1s - loss: 1.8058 - mse: 1.8037 - val_loss: 1.9046 - val_mse: 1.9063\n",
            "Epoch 60/100\n",
            "380/380 - 1s - loss: 1.7801 - mse: 1.7825 - val_loss: 1.8824 - val_mse: 1.8841\n",
            "Epoch 61/100\n",
            "380/380 - 1s - loss: 1.7589 - mse: 1.7618 - val_loss: 1.8608 - val_mse: 1.8625\n",
            "Epoch 62/100\n",
            "380/380 - 1s - loss: 1.7419 - mse: 1.7417 - val_loss: 1.8400 - val_mse: 1.8417\n",
            "Epoch 63/100\n",
            "380/380 - 1s - loss: 1.7200 - mse: 1.7220 - val_loss: 1.8192 - val_mse: 1.8209\n",
            "Epoch 64/100\n",
            "380/380 - 1s - loss: 1.7049 - mse: 1.7029 - val_loss: 1.7993 - val_mse: 1.8009\n",
            "Epoch 65/100\n",
            "380/380 - 1s - loss: 1.6826 - mse: 1.6840 - val_loss: 1.7793 - val_mse: 1.7810\n",
            "Epoch 66/100\n",
            "380/380 - 1s - loss: 1.6634 - mse: 1.6656 - val_loss: 1.7600 - val_mse: 1.7617\n",
            "Epoch 67/100\n",
            "380/380 - 1s - loss: 1.6504 - mse: 1.6476 - val_loss: 1.7411 - val_mse: 1.7427\n",
            "Epoch 68/100\n",
            "380/380 - 1s - loss: 1.6270 - mse: 1.6298 - val_loss: 1.7225 - val_mse: 1.7241\n",
            "Epoch 69/100\n",
            "380/380 - 1s - loss: 1.6145 - mse: 1.6126 - val_loss: 1.7041 - val_mse: 1.7057\n",
            "Epoch 70/100\n",
            "380/380 - 1s - loss: 1.5965 - mse: 1.5955 - val_loss: 1.6862 - val_mse: 1.6879\n",
            "Epoch 71/100\n",
            "380/380 - 1s - loss: 1.6024 - mse: 1.5790 - val_loss: 1.6687 - val_mse: 1.6703\n",
            "Epoch 72/100\n",
            "380/380 - 1s - loss: 1.5630 - mse: 1.5626 - val_loss: 1.6514 - val_mse: 1.6530\n",
            "Epoch 73/100\n",
            "380/380 - 1s - loss: 1.5451 - mse: 1.5467 - val_loss: 1.6345 - val_mse: 1.6361\n",
            "Epoch 74/100\n",
            "380/380 - 1s - loss: 1.5290 - mse: 1.5311 - val_loss: 1.6179 - val_mse: 1.6195\n",
            "Epoch 75/100\n",
            "380/380 - 1s - loss: 1.5163 - mse: 1.5157 - val_loss: 1.6016 - val_mse: 1.6032\n",
            "Epoch 76/100\n",
            "380/380 - 1s - loss: 1.5001 - mse: 1.5006 - val_loss: 1.5854 - val_mse: 1.5869\n",
            "Epoch 77/100\n",
            "380/380 - 1s - loss: 1.4843 - mse: 1.4857 - val_loss: 1.5696 - val_mse: 1.5712\n",
            "Epoch 78/100\n",
            "380/380 - 1s - loss: 1.4749 - mse: 1.4710 - val_loss: 1.5539 - val_mse: 1.5555\n",
            "Epoch 79/100\n",
            "380/380 - 1s - loss: 1.4545 - mse: 1.4565 - val_loss: 1.5385 - val_mse: 1.5400\n",
            "Epoch 80/100\n",
            "380/380 - 1s - loss: 1.4401 - mse: 1.4422 - val_loss: 1.5233 - val_mse: 1.5248\n",
            "Epoch 81/100\n",
            "380/380 - 1s - loss: 1.4296 - mse: 1.4282 - val_loss: 1.5084 - val_mse: 1.5099\n",
            "Epoch 82/100\n",
            "380/380 - 1s - loss: 1.4129 - mse: 1.4142 - val_loss: 1.4934 - val_mse: 1.4949\n",
            "Epoch 83/100\n",
            "380/380 - 1s - loss: 1.3982 - mse: 1.4005 - val_loss: 1.4786 - val_mse: 1.4801\n",
            "Epoch 84/100\n",
            "380/380 - 1s - loss: 1.3848 - mse: 1.3870 - val_loss: 1.4642 - val_mse: 1.4657\n",
            "Epoch 85/100\n",
            "380/380 - 1s - loss: 1.3760 - mse: 1.3737 - val_loss: 1.4501 - val_mse: 1.4516\n",
            "Epoch 86/100\n",
            "380/380 - 1s - loss: 1.3586 - mse: 1.3606 - val_loss: 1.4361 - val_mse: 1.4375\n",
            "Epoch 87/100\n",
            "380/380 - 1s - loss: 1.3459 - mse: 1.3476 - val_loss: 1.4222 - val_mse: 1.4236\n",
            "Epoch 88/100\n",
            "380/380 - 1s - loss: 1.3362 - mse: 1.3348 - val_loss: 1.4085 - val_mse: 1.4099\n",
            "Epoch 89/100\n",
            "380/380 - 1s - loss: 1.3225 - mse: 1.3221 - val_loss: 1.3950 - val_mse: 1.3964\n",
            "Epoch 90/100\n",
            "380/380 - 1s - loss: 1.3089 - mse: 1.3096 - val_loss: 1.3815 - val_mse: 1.3829\n",
            "Epoch 91/100\n",
            "380/380 - 1s - loss: 1.2952 - mse: 1.2972 - val_loss: 1.3683 - val_mse: 1.3697\n",
            "Epoch 92/100\n",
            "380/380 - 1s - loss: 1.2854 - mse: 1.2850 - val_loss: 1.3554 - val_mse: 1.3568\n",
            "Epoch 93/100\n",
            "380/380 - 1s - loss: 1.2711 - mse: 1.2731 - val_loss: 1.3424 - val_mse: 1.3438\n",
            "Epoch 94/100\n",
            "380/380 - 1s - loss: 1.2597 - mse: 1.2614 - val_loss: 1.3300 - val_mse: 1.3314\n",
            "Epoch 95/100\n",
            "380/380 - 1s - loss: 1.2487 - mse: 1.2498 - val_loss: 1.3177 - val_mse: 1.3190\n",
            "Epoch 96/100\n",
            "380/380 - 1s - loss: 1.2367 - mse: 1.2384 - val_loss: 1.3054 - val_mse: 1.3067\n",
            "Epoch 97/100\n",
            "380/380 - 1s - loss: 1.2280 - mse: 1.2271 - val_loss: 1.2934 - val_mse: 1.2947\n",
            "Epoch 98/100\n",
            "380/380 - 1s - loss: 1.2182 - mse: 1.2159 - val_loss: 1.2812 - val_mse: 1.2826\n",
            "Epoch 99/100\n",
            "380/380 - 1s - loss: 1.2031 - mse: 1.2049 - val_loss: 1.2695 - val_mse: 1.2708\n",
            "Epoch 100/100\n",
            "380/380 - 1s - loss: 1.1922 - mse: 1.1940 - val_loss: 1.2579 - val_mse: 1.2592\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 14%|█▍        | 3/21 [03:36<22:01, 73.44s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 32)                288       \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 32)                1056      \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "Output (Dense)               (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 1,377\n",
            "Trainable params: 1,377\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/100\n",
            "380/380 - 1s - loss: 7.8142 - mse: 7.8101 - val_loss: 7.9900 - val_mse: 7.9898\n",
            "Epoch 2/100\n",
            "380/380 - 1s - loss: 7.4269 - mse: 7.4339 - val_loss: 7.6146 - val_mse: 7.6145\n",
            "Epoch 3/100\n",
            "380/380 - 1s - loss: 7.0846 - mse: 7.0894 - val_loss: 7.2696 - val_mse: 7.2696\n",
            "Epoch 4/100\n",
            "380/380 - 1s - loss: 6.7632 - mse: 6.7720 - val_loss: 6.9510 - val_mse: 6.9512\n",
            "Epoch 5/100\n",
            "380/380 - 1s - loss: 6.4758 - mse: 6.4781 - val_loss: 6.6551 - val_mse: 6.6554\n",
            "Epoch 6/100\n",
            "380/380 - 1s - loss: 6.2016 - mse: 6.2046 - val_loss: 6.3794 - val_mse: 6.3798\n",
            "Epoch 7/100\n",
            "380/380 - 1s - loss: 5.9416 - mse: 5.9492 - val_loss: 6.1217 - val_mse: 6.1221\n",
            "Epoch 8/100\n",
            "380/380 - 1s - loss: 5.7051 - mse: 5.7100 - val_loss: 5.8796 - val_mse: 5.8801\n",
            "Epoch 9/100\n",
            "380/380 - 1s - loss: 5.4771 - mse: 5.4850 - val_loss: 5.6518 - val_mse: 5.6525\n",
            "Epoch 10/100\n",
            "380/380 - 1s - loss: 5.2740 - mse: 5.2729 - val_loss: 5.4364 - val_mse: 5.4371\n",
            "Epoch 11/100\n",
            "380/380 - 1s - loss: 5.0791 - mse: 5.0722 - val_loss: 5.2326 - val_mse: 5.2334\n",
            "Epoch 12/100\n",
            "380/380 - 1s - loss: 4.8824 - mse: 4.8820 - val_loss: 5.0396 - val_mse: 5.0404\n",
            "Epoch 13/100\n",
            "380/380 - 1s - loss: 4.6984 - mse: 4.7017 - val_loss: 4.8564 - val_mse: 4.8573\n",
            "Epoch 14/100\n",
            "380/380 - 1s - loss: 4.5393 - mse: 4.5306 - val_loss: 4.6819 - val_mse: 4.6828\n",
            "Epoch 15/100\n",
            "380/380 - 1s - loss: 4.3613 - mse: 4.3674 - val_loss: 4.5158 - val_mse: 4.5168\n",
            "Epoch 16/100\n",
            "380/380 - 1s - loss: 4.2086 - mse: 4.2121 - val_loss: 4.3573 - val_mse: 4.3583\n",
            "Epoch 17/100\n",
            "380/380 - 1s - loss: 4.0601 - mse: 4.0638 - val_loss: 4.2060 - val_mse: 4.2071\n",
            "Epoch 18/100\n",
            "380/380 - 1s - loss: 3.9206 - mse: 3.9222 - val_loss: 4.0611 - val_mse: 4.0623\n",
            "Epoch 19/100\n",
            "380/380 - 1s - loss: 3.7847 - mse: 3.7866 - val_loss: 3.9226 - val_mse: 3.9238\n",
            "Epoch 20/100\n",
            "380/380 - 1s - loss: 3.6517 - mse: 3.6570 - val_loss: 3.7900 - val_mse: 3.7912\n",
            "Epoch 21/100\n",
            "380/380 - 1s - loss: 3.5282 - mse: 3.5329 - val_loss: 3.6630 - val_mse: 3.6643\n",
            "Epoch 22/100\n",
            "380/380 - 1s - loss: 3.4106 - mse: 3.4142 - val_loss: 3.5412 - val_mse: 3.5425\n",
            "Epoch 23/100\n",
            "380/380 - 1s - loss: 3.3025 - mse: 3.3004 - val_loss: 3.4243 - val_mse: 3.4256\n",
            "Epoch 24/100\n",
            "380/380 - 1s - loss: 3.1935 - mse: 3.1912 - val_loss: 3.3123 - val_mse: 3.3136\n",
            "Epoch 25/100\n",
            "380/380 - 1s - loss: 3.0884 - mse: 3.0866 - val_loss: 3.2048 - val_mse: 3.2061\n",
            "Epoch 26/100\n",
            "380/380 - 1s - loss: 2.9831 - mse: 2.9862 - val_loss: 3.1018 - val_mse: 3.1032\n",
            "Epoch 27/100\n",
            "380/380 - 1s - loss: 2.8994 - mse: 2.8902 - val_loss: 3.0030 - val_mse: 3.0044\n",
            "Epoch 28/100\n",
            "380/380 - 1s - loss: 2.7992 - mse: 2.7980 - val_loss: 2.9084 - val_mse: 2.9099\n",
            "Epoch 29/100\n",
            "380/380 - 1s - loss: 2.7058 - mse: 2.7100 - val_loss: 2.8180 - val_mse: 2.8195\n",
            "Epoch 30/100\n",
            "380/380 - 1s - loss: 2.6298 - mse: 2.6259 - val_loss: 2.7312 - val_mse: 2.7327\n",
            "Epoch 31/100\n",
            "380/380 - 1s - loss: 2.5424 - mse: 2.5451 - val_loss: 2.6482 - val_mse: 2.6497\n",
            "Epoch 32/100\n",
            "380/380 - 1s - loss: 2.4692 - mse: 2.4681 - val_loss: 2.5687 - val_mse: 2.5701\n",
            "Epoch 33/100\n",
            "380/380 - 1s - loss: 2.3915 - mse: 2.3943 - val_loss: 2.4927 - val_mse: 2.4942\n",
            "Epoch 34/100\n",
            "380/380 - 1s - loss: 2.3241 - mse: 2.3239 - val_loss: 2.4201 - val_mse: 2.4216\n",
            "Epoch 35/100\n",
            "380/380 - 1s - loss: 2.2551 - mse: 2.2566 - val_loss: 2.3508 - val_mse: 2.3522\n",
            "Epoch 36/100\n",
            "380/380 - 1s - loss: 2.1898 - mse: 2.1925 - val_loss: 2.2845 - val_mse: 2.2860\n",
            "Epoch 37/100\n",
            "380/380 - 1s - loss: 2.1322 - mse: 2.1313 - val_loss: 2.2213 - val_mse: 2.2228\n",
            "Epoch 38/100\n",
            "380/380 - 1s - loss: 2.0765 - mse: 2.0730 - val_loss: 2.1609 - val_mse: 2.1624\n",
            "Epoch 39/100\n",
            "380/380 - 1s - loss: 2.0194 - mse: 2.0174 - val_loss: 2.1035 - val_mse: 2.1050\n",
            "Epoch 40/100\n",
            "380/380 - 1s - loss: 1.9609 - mse: 1.9645 - val_loss: 2.0489 - val_mse: 2.0503\n",
            "Epoch 41/100\n",
            "380/380 - 1s - loss: 1.9113 - mse: 1.9143 - val_loss: 1.9968 - val_mse: 1.9983\n",
            "Epoch 42/100\n",
            "380/380 - 1s - loss: 1.8705 - mse: 1.8666 - val_loss: 1.9471 - val_mse: 1.9485\n",
            "Epoch 43/100\n",
            "380/380 - 1s - loss: 1.8229 - mse: 1.8211 - val_loss: 1.8998 - val_mse: 1.9013\n",
            "Epoch 44/100\n",
            "380/380 - 1s - loss: 1.7749 - mse: 1.7778 - val_loss: 1.8549 - val_mse: 1.8564\n",
            "Epoch 45/100\n",
            "380/380 - 1s - loss: 1.7401 - mse: 1.7367 - val_loss: 1.8121 - val_mse: 1.8136\n",
            "Epoch 46/100\n",
            "380/380 - 1s - loss: 1.6962 - mse: 1.6976 - val_loss: 1.7715 - val_mse: 1.7730\n",
            "Epoch 47/100\n",
            "380/380 - 1s - loss: 1.6582 - mse: 1.6605 - val_loss: 1.7330 - val_mse: 1.7344\n",
            "Epoch 48/100\n",
            "380/380 - 1s - loss: 1.6248 - mse: 1.6254 - val_loss: 1.6963 - val_mse: 1.6977\n",
            "Epoch 49/100\n",
            "380/380 - 1s - loss: 1.5943 - mse: 1.5920 - val_loss: 1.6614 - val_mse: 1.6629\n",
            "Epoch 50/100\n",
            "380/380 - 1s - loss: 1.5620 - mse: 1.5604 - val_loss: 1.6284 - val_mse: 1.6298\n",
            "Epoch 51/100\n",
            "380/380 - 1s - loss: 1.5290 - mse: 1.5304 - val_loss: 1.5971 - val_mse: 1.5985\n",
            "Epoch 52/100\n",
            "380/380 - 1s - loss: 1.4997 - mse: 1.5019 - val_loss: 1.5673 - val_mse: 1.5687\n",
            "Epoch 53/100\n",
            "380/380 - 1s - loss: 1.4732 - mse: 1.4750 - val_loss: 1.5392 - val_mse: 1.5406\n",
            "Epoch 54/100\n",
            "380/380 - 1s - loss: 1.4499 - mse: 1.4495 - val_loss: 1.5123 - val_mse: 1.5137\n",
            "Epoch 55/100\n",
            "380/380 - 1s - loss: 1.4239 - mse: 1.4252 - val_loss: 1.4868 - val_mse: 1.4882\n",
            "Epoch 56/100\n",
            "380/380 - 1s - loss: 1.4005 - mse: 1.4022 - val_loss: 1.4626 - val_mse: 1.4640\n",
            "Epoch 57/100\n",
            "380/380 - 1s - loss: 1.3784 - mse: 1.3803 - val_loss: 1.4396 - val_mse: 1.4410\n",
            "Epoch 58/100\n",
            "380/380 - 1s - loss: 1.3581 - mse: 1.3596 - val_loss: 1.4178 - val_mse: 1.4192\n",
            "Epoch 59/100\n",
            "380/380 - 1s - loss: 1.3414 - mse: 1.3400 - val_loss: 1.3970 - val_mse: 1.3983\n",
            "Epoch 60/100\n",
            "380/380 - 1s - loss: 1.3195 - mse: 1.3213 - val_loss: 1.3772 - val_mse: 1.3786\n",
            "Epoch 61/100\n",
            "380/380 - 1s - loss: 1.3016 - mse: 1.3035 - val_loss: 1.3584 - val_mse: 1.3598\n",
            "Epoch 62/100\n",
            "380/380 - 1s - loss: 1.2867 - mse: 1.2866 - val_loss: 1.3405 - val_mse: 1.3418\n",
            "Epoch 63/100\n",
            "380/380 - 1s - loss: 1.2693 - mse: 1.2704 - val_loss: 1.3233 - val_mse: 1.3247\n",
            "Epoch 64/100\n",
            "380/380 - 1s - loss: 1.2565 - mse: 1.2550 - val_loss: 1.3069 - val_mse: 1.3082\n",
            "Epoch 65/100\n",
            "380/380 - 1s - loss: 1.2390 - mse: 1.2402 - val_loss: 1.2912 - val_mse: 1.2925\n",
            "Epoch 66/100\n",
            "380/380 - 1s - loss: 1.2247 - mse: 1.2261 - val_loss: 1.2763 - val_mse: 1.2776\n",
            "Epoch 67/100\n",
            "380/380 - 1s - loss: 1.2132 - mse: 1.2127 - val_loss: 1.2619 - val_mse: 1.2632\n",
            "Epoch 68/100\n",
            "380/380 - 1s - loss: 1.1976 - mse: 1.1998 - val_loss: 1.2482 - val_mse: 1.2495\n",
            "Epoch 69/100\n",
            "380/380 - 1s - loss: 1.1890 - mse: 1.1875 - val_loss: 1.2350 - val_mse: 1.2363\n",
            "Epoch 70/100\n",
            "380/380 - 1s - loss: 1.1754 - mse: 1.1756 - val_loss: 1.2223 - val_mse: 1.2236\n",
            "Epoch 71/100\n",
            "380/380 - 1s - loss: 1.1667 - mse: 1.1642 - val_loss: 1.2101 - val_mse: 1.2113\n",
            "Epoch 72/100\n",
            "380/380 - 1s - loss: 1.1531 - mse: 1.1533 - val_loss: 1.1984 - val_mse: 1.1996\n",
            "Epoch 73/100\n",
            "380/380 - 1s - loss: 1.1415 - mse: 1.1427 - val_loss: 1.1872 - val_mse: 1.1884\n",
            "Epoch 74/100\n",
            "380/380 - 1s - loss: 1.1314 - mse: 1.1326 - val_loss: 1.1764 - val_mse: 1.1776\n",
            "Epoch 75/100\n",
            "380/380 - 1s - loss: 1.1248 - mse: 1.1229 - val_loss: 1.1659 - val_mse: 1.1671\n",
            "Epoch 76/100\n",
            "380/380 - 1s - loss: 1.1119 - mse: 1.1135 - val_loss: 1.1558 - val_mse: 1.1570\n",
            "Epoch 77/100\n",
            "380/380 - 1s - loss: 1.1035 - mse: 1.1044 - val_loss: 1.1460 - val_mse: 1.1472\n",
            "Epoch 78/100\n",
            "380/380 - 1s - loss: 1.0973 - mse: 1.0956 - val_loss: 1.1366 - val_mse: 1.1377\n",
            "Epoch 79/100\n",
            "380/380 - 1s - loss: 1.0862 - mse: 1.0871 - val_loss: 1.1274 - val_mse: 1.1286\n",
            "Epoch 80/100\n",
            "380/380 - 1s - loss: 1.0774 - mse: 1.0789 - val_loss: 1.1186 - val_mse: 1.1197\n",
            "Epoch 81/100\n",
            "380/380 - 1s - loss: 1.0716 - mse: 1.0709 - val_loss: 1.1099 - val_mse: 1.1111\n",
            "Epoch 82/100\n",
            "380/380 - 1s - loss: 1.0618 - mse: 1.0631 - val_loss: 1.1016 - val_mse: 1.1027\n",
            "Epoch 83/100\n",
            "380/380 - 1s - loss: 1.0540 - mse: 1.0556 - val_loss: 1.0936 - val_mse: 1.0947\n",
            "Epoch 84/100\n",
            "380/380 - 1s - loss: 1.0468 - mse: 1.0483 - val_loss: 1.0857 - val_mse: 1.0868\n",
            "Epoch 85/100\n",
            "380/380 - 1s - loss: 1.0428 - mse: 1.0412 - val_loss: 1.0781 - val_mse: 1.0792\n",
            "Epoch 86/100\n",
            "380/380 - 1s - loss: 1.0330 - mse: 1.0343 - val_loss: 1.0707 - val_mse: 1.0718\n",
            "Epoch 87/100\n",
            "380/380 - 1s - loss: 1.0261 - mse: 1.0275 - val_loss: 1.0634 - val_mse: 1.0645\n",
            "Epoch 88/100\n",
            "380/380 - 1s - loss: 1.0224 - mse: 1.0210 - val_loss: 1.0564 - val_mse: 1.0575\n",
            "Epoch 89/100\n",
            "380/380 - 1s - loss: 1.0147 - mse: 1.0146 - val_loss: 1.0495 - val_mse: 1.0506\n",
            "Epoch 90/100\n",
            "380/380 - 1s - loss: 1.0082 - mse: 1.0083 - val_loss: 1.0428 - val_mse: 1.0439\n",
            "Epoch 91/100\n",
            "380/380 - 1s - loss: 1.0012 - mse: 1.0022 - val_loss: 1.0363 - val_mse: 1.0373\n",
            "Epoch 92/100\n",
            "380/380 - 1s - loss: 0.9964 - mse: 0.9962 - val_loss: 1.0299 - val_mse: 1.0310\n",
            "Epoch 93/100\n",
            "380/380 - 1s - loss: 0.9893 - mse: 0.9904 - val_loss: 1.0237 - val_mse: 1.0247\n",
            "Epoch 94/100\n",
            "380/380 - 1s - loss: 0.9844 - mse: 0.9847 - val_loss: 1.0176 - val_mse: 1.0187\n",
            "Epoch 95/100\n",
            "380/380 - 1s - loss: 0.9781 - mse: 0.9792 - val_loss: 1.0117 - val_mse: 1.0127\n",
            "Epoch 96/100\n",
            "380/380 - 1s - loss: 0.9725 - mse: 0.9737 - val_loss: 1.0059 - val_mse: 1.0069\n",
            "Epoch 97/100\n",
            "380/380 - 1s - loss: 0.9695 - mse: 0.9684 - val_loss: 1.0001 - val_mse: 1.0012\n",
            "Epoch 98/100\n",
            "380/380 - 1s - loss: 0.9663 - mse: 0.9632 - val_loss: 0.9946 - val_mse: 0.9956\n",
            "Epoch 99/100\n",
            "380/380 - 1s - loss: 0.9567 - mse: 0.9581 - val_loss: 0.9891 - val_mse: 0.9901\n",
            "Epoch 100/100\n",
            "380/380 - 1s - loss: 0.9518 - mse: 0.9530 - val_loss: 0.9838 - val_mse: 0.9848\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 19%|█▉        | 4/21 [04:43<20:13, 71.40s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 32)                288       \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "Output (Dense)               (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 321\n",
            "Trainable params: 321\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/100\n",
            "380/380 - 1s - loss: 2.7806 - mse: 9.5763 - val_loss: 2.8151 - val_mse: 9.8221\n",
            "Epoch 2/100\n",
            "380/380 - 1s - loss: 2.7680 - mse: 9.5008 - val_loss: 2.8030 - val_mse: 9.7460\n",
            "Epoch 3/100\n",
            "380/380 - 1s - loss: 2.7557 - mse: 9.4256 - val_loss: 2.7910 - val_mse: 9.6703\n",
            "Epoch 4/100\n",
            "380/380 - 1s - loss: 2.7422 - mse: 9.3513 - val_loss: 2.7790 - val_mse: 9.5950\n",
            "Epoch 5/100\n",
            "380/380 - 1s - loss: 2.7327 - mse: 9.2773 - val_loss: 2.7670 - val_mse: 9.5204\n",
            "Epoch 6/100\n",
            "380/380 - 1s - loss: 2.7203 - mse: 9.2044 - val_loss: 2.7550 - val_mse: 9.4465\n",
            "Epoch 7/100\n",
            "380/380 - 1s - loss: 2.7076 - mse: 9.1316 - val_loss: 2.7431 - val_mse: 9.3730\n",
            "Epoch 8/100\n",
            "380/380 - 1s - loss: 2.6963 - mse: 9.0596 - val_loss: 2.7312 - val_mse: 9.2999\n",
            "Epoch 9/100\n",
            "380/380 - 1s - loss: 2.6840 - mse: 8.9871 - val_loss: 2.7193 - val_mse: 9.2272\n",
            "Epoch 10/100\n",
            "380/380 - 1s - loss: 2.6731 - mse: 8.9160 - val_loss: 2.7074 - val_mse: 9.1552\n",
            "Epoch 11/100\n",
            "380/380 - 1s - loss: 2.6619 - mse: 8.8451 - val_loss: 2.6956 - val_mse: 9.0837\n",
            "Epoch 12/100\n",
            "380/380 - 1s - loss: 2.6500 - mse: 8.7752 - val_loss: 2.6838 - val_mse: 9.0128\n",
            "Epoch 13/100\n",
            "380/380 - 1s - loss: 2.6367 - mse: 8.7055 - val_loss: 2.6720 - val_mse: 8.9422\n",
            "Epoch 14/100\n",
            "380/380 - 1s - loss: 2.6276 - mse: 8.6359 - val_loss: 2.6603 - val_mse: 8.8720\n",
            "Epoch 15/100\n",
            "380/380 - 1s - loss: 2.6119 - mse: 8.5668 - val_loss: 2.6485 - val_mse: 8.8021\n",
            "Epoch 16/100\n",
            "380/380 - 1s - loss: 2.6014 - mse: 8.4983 - val_loss: 2.6368 - val_mse: 8.7328\n",
            "Epoch 17/100\n",
            "380/380 - 1s - loss: 2.5897 - mse: 8.4298 - val_loss: 2.6251 - val_mse: 8.6639\n",
            "Epoch 18/100\n",
            "380/380 - 1s - loss: 2.5784 - mse: 8.3627 - val_loss: 2.6134 - val_mse: 8.5956\n",
            "Epoch 19/100\n",
            "380/380 - 1s - loss: 2.5668 - mse: 8.2956 - val_loss: 2.6017 - val_mse: 8.5276\n",
            "Epoch 20/100\n",
            "380/380 - 1s - loss: 2.5536 - mse: 8.2287 - val_loss: 2.5901 - val_mse: 8.4600\n",
            "Epoch 21/100\n",
            "380/380 - 1s - loss: 2.5423 - mse: 8.1622 - val_loss: 2.5785 - val_mse: 8.3929\n",
            "Epoch 22/100\n",
            "380/380 - 1s - loss: 2.5315 - mse: 8.0964 - val_loss: 2.5668 - val_mse: 8.3262\n",
            "Epoch 23/100\n",
            "380/380 - 1s - loss: 2.5210 - mse: 8.0309 - val_loss: 2.5553 - val_mse: 8.2600\n",
            "Epoch 24/100\n",
            "380/380 - 1s - loss: 2.5091 - mse: 7.9659 - val_loss: 2.5437 - val_mse: 8.1940\n",
            "Epoch 25/100\n",
            "380/380 - 1s - loss: 2.4972 - mse: 7.9009 - val_loss: 2.5321 - val_mse: 8.1284\n",
            "Epoch 26/100\n",
            "380/380 - 1s - loss: 2.4852 - mse: 7.8368 - val_loss: 2.5206 - val_mse: 8.0633\n",
            "Epoch 27/100\n",
            "380/380 - 1s - loss: 2.4764 - mse: 7.7728 - val_loss: 2.5090 - val_mse: 7.9988\n",
            "Epoch 28/100\n",
            "380/380 - 1s - loss: 2.4621 - mse: 7.7096 - val_loss: 2.4975 - val_mse: 7.9346\n",
            "Epoch 29/100\n",
            "380/380 - 1s - loss: 2.4502 - mse: 7.6471 - val_loss: 2.4861 - val_mse: 7.8711\n",
            "Epoch 30/100\n",
            "380/380 - 1s - loss: 2.4409 - mse: 7.5846 - val_loss: 2.4746 - val_mse: 7.8079\n",
            "Epoch 31/100\n",
            "380/380 - 1s - loss: 2.4284 - mse: 7.5230 - val_loss: 2.4632 - val_mse: 7.7452\n",
            "Epoch 32/100\n",
            "380/380 - 1s - loss: 2.4179 - mse: 7.4614 - val_loss: 2.4518 - val_mse: 7.6829\n",
            "Epoch 33/100\n",
            "380/380 - 1s - loss: 2.4046 - mse: 7.4004 - val_loss: 2.4404 - val_mse: 7.6210\n",
            "Epoch 34/100\n",
            "380/380 - 1s - loss: 2.3936 - mse: 7.3393 - val_loss: 2.4291 - val_mse: 7.5594\n",
            "Epoch 35/100\n",
            "380/380 - 1s - loss: 2.3829 - mse: 7.2790 - val_loss: 2.4178 - val_mse: 7.4984\n",
            "Epoch 36/100\n",
            "380/380 - 1s - loss: 2.3713 - mse: 7.2195 - val_loss: 2.4064 - val_mse: 7.4378\n",
            "Epoch 37/100\n",
            "380/380 - 1s - loss: 2.3608 - mse: 7.1601 - val_loss: 2.3952 - val_mse: 7.3776\n",
            "Epoch 38/100\n",
            "380/380 - 1s - loss: 2.3503 - mse: 7.1005 - val_loss: 2.3839 - val_mse: 7.3177\n",
            "Epoch 39/100\n",
            "380/380 - 1s - loss: 2.3388 - mse: 7.0421 - val_loss: 2.3726 - val_mse: 7.2583\n",
            "Epoch 40/100\n",
            "380/380 - 1s - loss: 2.3241 - mse: 6.9840 - val_loss: 2.3614 - val_mse: 7.1993\n",
            "Epoch 41/100\n",
            "380/380 - 1s - loss: 2.3137 - mse: 6.9264 - val_loss: 2.3502 - val_mse: 7.1407\n",
            "Epoch 42/100\n",
            "380/380 - 1s - loss: 2.3056 - mse: 6.8689 - val_loss: 2.3390 - val_mse: 7.0826\n",
            "Epoch 43/100\n",
            "380/380 - 1s - loss: 2.2929 - mse: 6.8124 - val_loss: 2.3278 - val_mse: 7.0249\n",
            "Epoch 44/100\n",
            "380/380 - 1s - loss: 2.2806 - mse: 6.7556 - val_loss: 2.3167 - val_mse: 6.9676\n",
            "Epoch 45/100\n",
            "380/380 - 1s - loss: 2.2714 - mse: 6.6993 - val_loss: 2.3055 - val_mse: 6.9106\n",
            "Epoch 46/100\n",
            "380/380 - 1s - loss: 2.2581 - mse: 6.6437 - val_loss: 2.2944 - val_mse: 6.8540\n",
            "Epoch 47/100\n",
            "380/380 - 1s - loss: 2.2470 - mse: 6.5882 - val_loss: 2.2833 - val_mse: 6.7979\n",
            "Epoch 48/100\n",
            "380/380 - 1s - loss: 2.2381 - mse: 6.5332 - val_loss: 2.2722 - val_mse: 6.7421\n",
            "Epoch 49/100\n",
            "380/380 - 1s - loss: 2.2274 - mse: 6.4787 - val_loss: 2.2612 - val_mse: 6.6868\n",
            "Epoch 50/100\n",
            "380/380 - 1s - loss: 2.2151 - mse: 6.4246 - val_loss: 2.2501 - val_mse: 6.6319\n",
            "Epoch 51/100\n",
            "380/380 - 1s - loss: 2.2038 - mse: 6.3711 - val_loss: 2.2392 - val_mse: 6.5775\n",
            "Epoch 52/100\n",
            "380/380 - 1s - loss: 2.1927 - mse: 6.3178 - val_loss: 2.2282 - val_mse: 6.5234\n",
            "Epoch 53/100\n",
            "380/380 - 1s - loss: 2.1802 - mse: 6.2649 - val_loss: 2.2172 - val_mse: 6.4697\n",
            "Epoch 54/100\n",
            "380/380 - 1s - loss: 2.1716 - mse: 6.2123 - val_loss: 2.2062 - val_mse: 6.4164\n",
            "Epoch 55/100\n",
            "380/380 - 1s - loss: 2.1603 - mse: 6.1600 - val_loss: 2.1953 - val_mse: 6.3635\n",
            "Epoch 56/100\n",
            "380/380 - 1s - loss: 2.1500 - mse: 6.1081 - val_loss: 2.1844 - val_mse: 6.3111\n",
            "Epoch 57/100\n",
            "380/380 - 1s - loss: 2.1368 - mse: 6.0569 - val_loss: 2.1735 - val_mse: 6.2590\n",
            "Epoch 58/100\n",
            "380/380 - 1s - loss: 2.1269 - mse: 6.0060 - val_loss: 2.1627 - val_mse: 6.2073\n",
            "Epoch 59/100\n",
            "380/380 - 1s - loss: 2.1167 - mse: 5.9555 - val_loss: 2.1518 - val_mse: 6.1559\n",
            "Epoch 60/100\n",
            "380/380 - 1s - loss: 2.1051 - mse: 5.9054 - val_loss: 2.1410 - val_mse: 6.1050\n",
            "Epoch 61/100\n",
            "380/380 - 1s - loss: 2.0928 - mse: 5.8556 - val_loss: 2.1302 - val_mse: 6.0544\n",
            "Epoch 62/100\n",
            "380/380 - 1s - loss: 2.0849 - mse: 5.8065 - val_loss: 2.1194 - val_mse: 6.0043\n",
            "Epoch 63/100\n",
            "380/380 - 1s - loss: 2.0724 - mse: 5.7571 - val_loss: 2.1086 - val_mse: 5.9544\n",
            "Epoch 64/100\n",
            "380/380 - 1s - loss: 2.0651 - mse: 5.7086 - val_loss: 2.0979 - val_mse: 5.9050\n",
            "Epoch 65/100\n",
            "380/380 - 1s - loss: 2.0521 - mse: 5.6600 - val_loss: 2.0871 - val_mse: 5.8558\n",
            "Epoch 66/100\n",
            "380/380 - 1s - loss: 2.0393 - mse: 5.6123 - val_loss: 2.0764 - val_mse: 5.8072\n",
            "Epoch 67/100\n",
            "380/380 - 1s - loss: 2.0322 - mse: 5.5649 - val_loss: 2.0657 - val_mse: 5.7587\n",
            "Epoch 68/100\n",
            "380/380 - 1s - loss: 2.0185 - mse: 5.5177 - val_loss: 2.0550 - val_mse: 5.7108\n",
            "Epoch 69/100\n",
            "380/380 - 1s - loss: 2.0099 - mse: 5.4707 - val_loss: 2.0443 - val_mse: 5.6632\n",
            "Epoch 70/100\n",
            "380/380 - 1s - loss: 1.9989 - mse: 5.4244 - val_loss: 2.0337 - val_mse: 5.6160\n",
            "Epoch 71/100\n",
            "380/380 - 1s - loss: 1.9946 - mse: 5.3782 - val_loss: 2.0231 - val_mse: 5.5691\n",
            "Epoch 72/100\n",
            "380/380 - 1s - loss: 1.9783 - mse: 5.3326 - val_loss: 2.0124 - val_mse: 5.5226\n",
            "Epoch 73/100\n",
            "380/380 - 1s - loss: 1.9654 - mse: 5.2874 - val_loss: 2.0019 - val_mse: 5.4766\n",
            "Epoch 74/100\n",
            "380/380 - 1s - loss: 1.9546 - mse: 5.2423 - val_loss: 1.9913 - val_mse: 5.4309\n",
            "Epoch 75/100\n",
            "380/380 - 1s - loss: 1.9465 - mse: 5.1978 - val_loss: 1.9807 - val_mse: 5.3855\n",
            "Epoch 76/100\n",
            "380/380 - 1s - loss: 1.9339 - mse: 5.1536 - val_loss: 1.9702 - val_mse: 5.3404\n",
            "Epoch 77/100\n",
            "380/380 - 1s - loss: 1.9247 - mse: 5.1097 - val_loss: 1.9596 - val_mse: 5.2956\n",
            "Epoch 78/100\n",
            "380/380 - 1s - loss: 1.9157 - mse: 5.0660 - val_loss: 1.9491 - val_mse: 5.2511\n",
            "Epoch 79/100\n",
            "380/380 - 1s - loss: 1.9015 - mse: 5.0225 - val_loss: 1.9385 - val_mse: 5.2069\n",
            "Epoch 80/100\n",
            "380/380 - 1s - loss: 1.8922 - mse: 4.9796 - val_loss: 1.9280 - val_mse: 5.1631\n",
            "Epoch 81/100\n",
            "380/380 - 1s - loss: 1.8836 - mse: 4.9367 - val_loss: 1.9175 - val_mse: 5.1196\n",
            "Epoch 82/100\n",
            "380/380 - 1s - loss: 1.8716 - mse: 4.8943 - val_loss: 1.9071 - val_mse: 5.0763\n",
            "Epoch 83/100\n",
            "380/380 - 1s - loss: 1.8595 - mse: 4.8522 - val_loss: 1.8966 - val_mse: 5.0335\n",
            "Epoch 84/100\n",
            "380/380 - 1s - loss: 1.8490 - mse: 4.8106 - val_loss: 1.8862 - val_mse: 4.9909\n",
            "Epoch 85/100\n",
            "380/380 - 1s - loss: 1.8412 - mse: 4.7695 - val_loss: 1.8758 - val_mse: 4.9488\n",
            "Epoch 86/100\n",
            "380/380 - 1s - loss: 1.8280 - mse: 4.7283 - val_loss: 1.8654 - val_mse: 4.9070\n",
            "Epoch 87/100\n",
            "380/380 - 1s - loss: 1.8189 - mse: 4.6877 - val_loss: 1.8550 - val_mse: 4.8655\n",
            "Epoch 88/100\n",
            "380/380 - 1s - loss: 1.8098 - mse: 4.6472 - val_loss: 1.8447 - val_mse: 4.8242\n",
            "Epoch 89/100\n",
            "380/380 - 1s - loss: 1.7987 - mse: 4.6069 - val_loss: 1.8343 - val_mse: 4.7832\n",
            "Epoch 90/100\n",
            "380/380 - 1s - loss: 1.7890 - mse: 4.5668 - val_loss: 1.8241 - val_mse: 4.7426\n",
            "Epoch 91/100\n",
            "380/380 - 1s - loss: 1.7765 - mse: 4.5275 - val_loss: 1.8138 - val_mse: 4.7022\n",
            "Epoch 92/100\n",
            "380/380 - 1s - loss: 1.7665 - mse: 4.4883 - val_loss: 1.8035 - val_mse: 4.6621\n",
            "Epoch 93/100\n",
            "380/380 - 1s - loss: 1.7559 - mse: 4.4496 - val_loss: 1.7933 - val_mse: 4.6224\n",
            "Epoch 94/100\n",
            "380/380 - 1s - loss: 1.7458 - mse: 4.4110 - val_loss: 1.7831 - val_mse: 4.5830\n",
            "Epoch 95/100\n",
            "380/380 - 1s - loss: 1.7355 - mse: 4.3726 - val_loss: 1.7730 - val_mse: 4.5439\n",
            "Epoch 96/100\n",
            "380/380 - 1s - loss: 1.7256 - mse: 4.3345 - val_loss: 1.7629 - val_mse: 4.5050\n",
            "Epoch 97/100\n",
            "380/380 - 1s - loss: 1.7187 - mse: 4.2969 - val_loss: 1.7528 - val_mse: 4.4665\n",
            "Epoch 98/100\n",
            "380/380 - 1s - loss: 1.7063 - mse: 4.2596 - val_loss: 1.7428 - val_mse: 4.4283\n",
            "Epoch 99/100\n",
            "380/380 - 1s - loss: 1.6967 - mse: 4.2226 - val_loss: 1.7329 - val_mse: 4.3905\n",
            "Epoch 100/100\n",
            "380/380 - 1s - loss: 1.6852 - mse: 4.1861 - val_loss: 1.7230 - val_mse: 4.3530\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 24%|██▍       | 5/21 [05:50<18:43, 70.22s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 32)                288       \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "Output (Dense)               (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 321\n",
            "Trainable params: 321\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/100\n",
            "380/380 - 1s - loss: 4.3484 - mse: 4.3444 - val_loss: 4.6987 - val_mse: 4.6996\n",
            "Epoch 2/100\n",
            "380/380 - 1s - loss: 4.3243 - mse: 4.3279 - val_loss: 4.6820 - val_mse: 4.6829\n",
            "Epoch 3/100\n",
            "380/380 - 1s - loss: 4.3083 - mse: 4.3117 - val_loss: 4.6654 - val_mse: 4.6663\n",
            "Epoch 4/100\n",
            "380/380 - 1s - loss: 4.2887 - mse: 4.2955 - val_loss: 4.6487 - val_mse: 4.6497\n",
            "Epoch 5/100\n",
            "380/380 - 1s - loss: 4.2775 - mse: 4.2794 - val_loss: 4.6323 - val_mse: 4.6332\n",
            "Epoch 6/100\n",
            "380/380 - 1s - loss: 4.2613 - mse: 4.2633 - val_loss: 4.6158 - val_mse: 4.6168\n",
            "Epoch 7/100\n",
            "380/380 - 1s - loss: 4.2412 - mse: 4.2474 - val_loss: 4.5995 - val_mse: 4.6005\n",
            "Epoch 8/100\n",
            "380/380 - 1s - loss: 4.2279 - mse: 4.2316 - val_loss: 4.5833 - val_mse: 4.5843\n",
            "Epoch 9/100\n",
            "380/380 - 1s - loss: 4.2087 - mse: 4.2158 - val_loss: 4.5670 - val_mse: 4.5680\n",
            "Epoch 10/100\n",
            "380/380 - 1s - loss: 4.2003 - mse: 4.2000 - val_loss: 4.5509 - val_mse: 4.5519\n",
            "Epoch 11/100\n",
            "380/380 - 1s - loss: 4.1903 - mse: 4.1843 - val_loss: 4.5349 - val_mse: 4.5359\n",
            "Epoch 12/100\n",
            "380/380 - 1s - loss: 4.1682 - mse: 4.1687 - val_loss: 4.5187 - val_mse: 4.5197\n",
            "Epoch 13/100\n",
            "380/380 - 1s - loss: 4.1499 - mse: 4.1530 - val_loss: 4.5028 - val_mse: 4.5038\n",
            "Epoch 14/100\n",
            "380/380 - 1s - loss: 4.1462 - mse: 4.1375 - val_loss: 4.4868 - val_mse: 4.4878\n",
            "Epoch 15/100\n",
            "380/380 - 1s - loss: 4.1158 - mse: 4.1220 - val_loss: 4.4710 - val_mse: 4.4720\n",
            "Epoch 16/100\n",
            "380/380 - 1s - loss: 4.1042 - mse: 4.1067 - val_loss: 4.4551 - val_mse: 4.4562\n",
            "Epoch 17/100\n",
            "380/380 - 1s - loss: 4.0887 - mse: 4.0913 - val_loss: 4.4394 - val_mse: 4.4404\n",
            "Epoch 18/100\n",
            "380/380 - 1s - loss: 4.0757 - mse: 4.0760 - val_loss: 4.4237 - val_mse: 4.4247\n",
            "Epoch 19/100\n",
            "380/380 - 1s - loss: 4.0595 - mse: 4.0608 - val_loss: 4.4081 - val_mse: 4.4091\n",
            "Epoch 20/100\n",
            "380/380 - 1s - loss: 4.0397 - mse: 4.0457 - val_loss: 4.3925 - val_mse: 4.3936\n",
            "Epoch 21/100\n",
            "380/380 - 1s - loss: 4.0249 - mse: 4.0306 - val_loss: 4.3770 - val_mse: 4.3781\n",
            "Epoch 22/100\n",
            "380/380 - 1s - loss: 4.0133 - mse: 4.0154 - val_loss: 4.3615 - val_mse: 4.3626\n",
            "Epoch 23/100\n",
            "380/380 - 1s - loss: 4.0035 - mse: 4.0004 - val_loss: 4.3460 - val_mse: 4.3471\n",
            "Epoch 24/100\n",
            "380/380 - 1s - loss: 3.9876 - mse: 3.9853 - val_loss: 4.3306 - val_mse: 4.3316\n",
            "Epoch 25/100\n",
            "380/380 - 1s - loss: 3.9713 - mse: 3.9703 - val_loss: 4.3151 - val_mse: 4.3162\n",
            "Epoch 26/100\n",
            "380/380 - 1s - loss: 3.9524 - mse: 3.9554 - val_loss: 4.2999 - val_mse: 4.3010\n",
            "Epoch 27/100\n",
            "380/380 - 1s - loss: 3.9526 - mse: 3.9406 - val_loss: 4.2846 - val_mse: 4.2857\n",
            "Epoch 28/100\n",
            "380/380 - 1s - loss: 3.9258 - mse: 3.9258 - val_loss: 4.2695 - val_mse: 4.2706\n",
            "Epoch 29/100\n",
            "380/380 - 1s - loss: 3.9053 - mse: 3.9111 - val_loss: 4.2544 - val_mse: 4.2555\n",
            "Epoch 30/100\n",
            "380/380 - 1s - loss: 3.9033 - mse: 3.8965 - val_loss: 4.2394 - val_mse: 4.2405\n",
            "Epoch 31/100\n",
            "380/380 - 1s - loss: 3.8802 - mse: 3.8819 - val_loss: 4.2245 - val_mse: 4.2256\n",
            "Epoch 32/100\n",
            "380/380 - 1s - loss: 3.8691 - mse: 3.8674 - val_loss: 4.2097 - val_mse: 4.2108\n",
            "Epoch 33/100\n",
            "380/380 - 1s - loss: 3.8485 - mse: 3.8529 - val_loss: 4.1948 - val_mse: 4.1959\n",
            "Epoch 34/100\n",
            "380/380 - 1s - loss: 3.8367 - mse: 3.8384 - val_loss: 4.1801 - val_mse: 4.1812\n",
            "Epoch 35/100\n",
            "380/380 - 1s - loss: 3.8212 - mse: 3.8240 - val_loss: 4.1654 - val_mse: 4.1665\n",
            "Epoch 36/100\n",
            "380/380 - 1s - loss: 3.8069 - mse: 3.8097 - val_loss: 4.1508 - val_mse: 4.1519\n",
            "Epoch 37/100\n",
            "380/380 - 1s - loss: 3.7961 - mse: 3.7954 - val_loss: 4.1361 - val_mse: 4.1373\n",
            "Epoch 38/100\n",
            "380/380 - 1s - loss: 3.7861 - mse: 3.7811 - val_loss: 4.1216 - val_mse: 4.1227\n",
            "Epoch 39/100\n",
            "380/380 - 1s - loss: 3.7689 - mse: 3.7669 - val_loss: 4.1071 - val_mse: 4.1082\n",
            "Epoch 40/100\n",
            "380/380 - 1s - loss: 3.7458 - mse: 3.7528 - val_loss: 4.0927 - val_mse: 4.0939\n",
            "Epoch 41/100\n",
            "380/380 - 1s - loss: 3.7338 - mse: 3.7388 - val_loss: 4.0784 - val_mse: 4.0795\n",
            "Epoch 42/100\n",
            "380/380 - 1s - loss: 3.7308 - mse: 3.7248 - val_loss: 4.0640 - val_mse: 4.0652\n",
            "Epoch 43/100\n",
            "380/380 - 1s - loss: 3.7132 - mse: 3.7108 - val_loss: 4.0498 - val_mse: 4.0509\n",
            "Epoch 44/100\n",
            "380/380 - 1s - loss: 3.6926 - mse: 3.6969 - val_loss: 4.0355 - val_mse: 4.0367\n",
            "Epoch 45/100\n",
            "380/380 - 1s - loss: 3.6886 - mse: 3.6831 - val_loss: 4.0214 - val_mse: 4.0226\n",
            "Epoch 46/100\n",
            "380/380 - 1s - loss: 3.6663 - mse: 3.6692 - val_loss: 4.0072 - val_mse: 4.0084\n",
            "Epoch 47/100\n",
            "380/380 - 1s - loss: 3.6501 - mse: 3.6555 - val_loss: 3.9932 - val_mse: 3.9944\n",
            "Epoch 48/100\n",
            "380/380 - 1s - loss: 3.6431 - mse: 3.6418 - val_loss: 3.9792 - val_mse: 3.9804\n",
            "Epoch 49/100\n",
            "380/380 - 1s - loss: 3.6316 - mse: 3.6281 - val_loss: 3.9653 - val_mse: 3.9664\n",
            "Epoch 50/100\n",
            "380/380 - 1s - loss: 3.6168 - mse: 3.6145 - val_loss: 3.9513 - val_mse: 3.9525\n",
            "Epoch 51/100\n",
            "380/380 - 1s - loss: 3.6004 - mse: 3.6009 - val_loss: 3.9374 - val_mse: 3.9386\n",
            "Epoch 52/100\n",
            "380/380 - 1s - loss: 3.5855 - mse: 3.5874 - val_loss: 3.9236 - val_mse: 3.9248\n",
            "Epoch 53/100\n",
            "380/380 - 1s - loss: 3.5684 - mse: 3.5739 - val_loss: 3.9098 - val_mse: 3.9110\n",
            "Epoch 54/100\n",
            "380/380 - 1s - loss: 3.5621 - mse: 3.5605 - val_loss: 3.8960 - val_mse: 3.8972\n",
            "Epoch 55/100\n",
            "380/380 - 1s - loss: 3.5482 - mse: 3.5471 - val_loss: 3.8823 - val_mse: 3.8835\n",
            "Epoch 56/100\n",
            "380/380 - 1s - loss: 3.5331 - mse: 3.5337 - val_loss: 3.8687 - val_mse: 3.8699\n",
            "Epoch 57/100\n",
            "380/380 - 1s - loss: 3.5166 - mse: 3.5205 - val_loss: 3.8552 - val_mse: 3.8564\n",
            "Epoch 58/100\n",
            "380/380 - 1s - loss: 3.5043 - mse: 3.5073 - val_loss: 3.8417 - val_mse: 3.8429\n",
            "Epoch 59/100\n",
            "380/380 - 1s - loss: 3.4967 - mse: 3.4942 - val_loss: 3.8281 - val_mse: 3.8294\n",
            "Epoch 60/100\n",
            "380/380 - 1s - loss: 3.4760 - mse: 3.4810 - val_loss: 3.8147 - val_mse: 3.8159\n",
            "Epoch 61/100\n",
            "380/380 - 1s - loss: 3.4621 - mse: 3.4679 - val_loss: 3.8014 - val_mse: 3.8026\n",
            "Epoch 62/100\n",
            "380/380 - 1s - loss: 3.4571 - mse: 3.4550 - val_loss: 3.7882 - val_mse: 3.7894\n",
            "Epoch 63/100\n",
            "380/380 - 1s - loss: 3.4391 - mse: 3.4421 - val_loss: 3.7749 - val_mse: 3.7761\n",
            "Epoch 64/100\n",
            "380/380 - 1s - loss: 3.4378 - mse: 3.4292 - val_loss: 3.7618 - val_mse: 3.7630\n",
            "Epoch 65/100\n",
            "380/380 - 1s - loss: 3.4166 - mse: 3.4164 - val_loss: 3.7486 - val_mse: 3.7498\n",
            "Epoch 66/100\n",
            "380/380 - 1s - loss: 3.3991 - mse: 3.4036 - val_loss: 3.7355 - val_mse: 3.7368\n",
            "Epoch 67/100\n",
            "380/380 - 1s - loss: 3.3939 - mse: 3.3909 - val_loss: 3.7224 - val_mse: 3.7236\n",
            "Epoch 68/100\n",
            "380/380 - 1s - loss: 3.3749 - mse: 3.3781 - val_loss: 3.7093 - val_mse: 3.7106\n",
            "Epoch 69/100\n",
            "380/380 - 1s - loss: 3.3708 - mse: 3.3654 - val_loss: 3.6963 - val_mse: 3.6976\n",
            "Epoch 70/100\n",
            "380/380 - 1s - loss: 3.3550 - mse: 3.3528 - val_loss: 3.6835 - val_mse: 3.6848\n",
            "Epoch 71/100\n",
            "380/380 - 1s - loss: 3.3478 - mse: 3.3403 - val_loss: 3.6707 - val_mse: 3.6720\n",
            "Epoch 72/100\n",
            "380/380 - 1s - loss: 3.3312 - mse: 3.3278 - val_loss: 3.6578 - val_mse: 3.6591\n",
            "Epoch 73/100\n",
            "380/380 - 1s - loss: 3.3118 - mse: 3.3153 - val_loss: 3.6450 - val_mse: 3.6464\n",
            "Epoch 74/100\n",
            "380/380 - 1s - loss: 3.2987 - mse: 3.3029 - val_loss: 3.6323 - val_mse: 3.6336\n",
            "Epoch 75/100\n",
            "380/380 - 1s - loss: 3.2952 - mse: 3.2906 - val_loss: 3.6197 - val_mse: 3.6210\n",
            "Epoch 76/100\n",
            "380/380 - 1s - loss: 3.2759 - mse: 3.2783 - val_loss: 3.6071 - val_mse: 3.6084\n",
            "Epoch 77/100\n",
            "380/380 - 1s - loss: 3.2665 - mse: 3.2661 - val_loss: 3.5946 - val_mse: 3.5959\n",
            "Epoch 78/100\n",
            "380/380 - 1s - loss: 3.2642 - mse: 3.2539 - val_loss: 3.5821 - val_mse: 3.5834\n",
            "Epoch 79/100\n",
            "380/380 - 1s - loss: 3.2377 - mse: 3.2417 - val_loss: 3.5696 - val_mse: 3.5710\n",
            "Epoch 80/100\n",
            "380/380 - 1s - loss: 3.2273 - mse: 3.2296 - val_loss: 3.5572 - val_mse: 3.5586\n",
            "Epoch 81/100\n",
            "380/380 - 1s - loss: 3.2263 - mse: 3.2176 - val_loss: 3.5449 - val_mse: 3.5463\n",
            "Epoch 82/100\n",
            "380/380 - 1s - loss: 3.2063 - mse: 3.2055 - val_loss: 3.5325 - val_mse: 3.5339\n",
            "Epoch 83/100\n",
            "380/380 - 1s - loss: 3.1895 - mse: 3.1935 - val_loss: 3.5201 - val_mse: 3.5215\n",
            "Epoch 84/100\n",
            "380/380 - 1s - loss: 3.1770 - mse: 3.1816 - val_loss: 3.5079 - val_mse: 3.5093\n",
            "Epoch 85/100\n",
            "380/380 - 1s - loss: 3.1748 - mse: 3.1698 - val_loss: 3.4959 - val_mse: 3.4973\n",
            "Epoch 86/100\n",
            "380/380 - 1s - loss: 3.1532 - mse: 3.1580 - val_loss: 3.4838 - val_mse: 3.4852\n",
            "Epoch 87/100\n",
            "380/380 - 1s - loss: 3.1441 - mse: 3.1463 - val_loss: 3.4717 - val_mse: 3.4731\n",
            "Epoch 88/100\n",
            "380/380 - 1s - loss: 3.1401 - mse: 3.1345 - val_loss: 3.4596 - val_mse: 3.4611\n",
            "Epoch 89/100\n",
            "380/380 - 1s - loss: 3.1243 - mse: 3.1228 - val_loss: 3.4477 - val_mse: 3.4491\n",
            "Epoch 90/100\n",
            "380/380 - 1s - loss: 3.1144 - mse: 3.1112 - val_loss: 3.4357 - val_mse: 3.4371\n",
            "Epoch 91/100\n",
            "380/380 - 1s - loss: 3.0946 - mse: 3.0995 - val_loss: 3.4238 - val_mse: 3.4252\n",
            "Epoch 92/100\n",
            "380/380 - 1s - loss: 3.0887 - mse: 3.0880 - val_loss: 3.4119 - val_mse: 3.4133\n",
            "Epoch 93/100\n",
            "380/380 - 1s - loss: 3.0730 - mse: 3.0765 - val_loss: 3.4001 - val_mse: 3.4015\n",
            "Epoch 94/100\n",
            "380/380 - 1s - loss: 3.0598 - mse: 3.0651 - val_loss: 3.3884 - val_mse: 3.3899\n",
            "Epoch 95/100\n",
            "380/380 - 1s - loss: 3.0492 - mse: 3.0537 - val_loss: 3.3768 - val_mse: 3.3782\n",
            "Epoch 96/100\n",
            "380/380 - 1s - loss: 3.0387 - mse: 3.0424 - val_loss: 3.3651 - val_mse: 3.3666\n",
            "Epoch 97/100\n",
            "380/380 - 1s - loss: 3.0385 - mse: 3.0311 - val_loss: 3.3536 - val_mse: 3.3551\n",
            "Epoch 98/100\n",
            "380/380 - 1s - loss: 3.0221 - mse: 3.0199 - val_loss: 3.3420 - val_mse: 3.3434\n",
            "Epoch 99/100\n",
            "380/380 - 1s - loss: 3.0085 - mse: 3.0087 - val_loss: 3.3305 - val_mse: 3.3320\n",
            "Epoch 100/100\n",
            "380/380 - 1s - loss: 2.9939 - mse: 2.9976 - val_loss: 3.3191 - val_mse: 3.3206\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 29%|██▊       | 6/21 [06:55<17:06, 68.41s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 32)                288       \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 32)                1056      \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 32)                1056      \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "Output (Dense)               (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 2,433\n",
            "Trainable params: 2,433\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/100\n",
            "380/380 - 1s - loss: 5.7534 - mse: 5.7513 - val_loss: 5.9825 - val_mse: 5.9822\n",
            "Epoch 2/100\n",
            "380/380 - 1s - loss: 5.7003 - mse: 5.7043 - val_loss: 5.9309 - val_mse: 5.9306\n",
            "Epoch 3/100\n",
            "380/380 - 1s - loss: 5.6534 - mse: 5.6574 - val_loss: 5.8798 - val_mse: 5.8797\n",
            "Epoch 4/100\n",
            "380/380 - 1s - loss: 5.6033 - mse: 5.6111 - val_loss: 5.8293 - val_mse: 5.8291\n",
            "Epoch 5/100\n",
            "380/380 - 1s - loss: 5.5638 - mse: 5.5653 - val_loss: 5.7795 - val_mse: 5.7795\n",
            "Epoch 6/100\n",
            "380/380 - 1s - loss: 5.5184 - mse: 5.5200 - val_loss: 5.7303 - val_mse: 5.7303\n",
            "Epoch 7/100\n",
            "380/380 - 1s - loss: 5.4689 - mse: 5.4751 - val_loss: 5.6814 - val_mse: 5.6815\n",
            "Epoch 8/100\n",
            "380/380 - 1s - loss: 5.4281 - mse: 5.4305 - val_loss: 5.6328 - val_mse: 5.6329\n",
            "Epoch 9/100\n",
            "380/380 - 1s - loss: 5.3796 - mse: 5.3861 - val_loss: 5.5843 - val_mse: 5.5845\n",
            "Epoch 10/100\n",
            "380/380 - 1s - loss: 5.3436 - mse: 5.3421 - val_loss: 5.5366 - val_mse: 5.5368\n",
            "Epoch 11/100\n",
            "380/380 - 1s - loss: 5.3052 - mse: 5.2983 - val_loss: 5.4890 - val_mse: 5.4893\n",
            "Epoch 12/100\n",
            "380/380 - 1s - loss: 5.2567 - mse: 5.2548 - val_loss: 5.4416 - val_mse: 5.4420\n",
            "Epoch 13/100\n",
            "380/380 - 1s - loss: 5.2074 - mse: 5.2113 - val_loss: 5.3947 - val_mse: 5.3951\n",
            "Epoch 14/100\n",
            "380/380 - 1s - loss: 5.1772 - mse: 5.1682 - val_loss: 5.3477 - val_mse: 5.3481\n",
            "Epoch 15/100\n",
            "380/380 - 1s - loss: 5.1181 - mse: 5.1250 - val_loss: 5.3009 - val_mse: 5.3014\n",
            "Epoch 16/100\n",
            "380/380 - 1s - loss: 5.0788 - mse: 5.0823 - val_loss: 5.2544 - val_mse: 5.2550\n",
            "Epoch 17/100\n",
            "380/380 - 1s - loss: 5.0366 - mse: 5.0396 - val_loss: 5.2081 - val_mse: 5.2087\n",
            "Epoch 18/100\n",
            "380/380 - 1s - loss: 4.9958 - mse: 4.9970 - val_loss: 5.1619 - val_mse: 5.1626\n",
            "Epoch 19/100\n",
            "380/380 - 1s - loss: 4.9525 - mse: 4.9545 - val_loss: 5.1159 - val_mse: 5.1166\n",
            "Epoch 20/100\n",
            "380/380 - 1s - loss: 4.9059 - mse: 4.9122 - val_loss: 5.0702 - val_mse: 5.0709\n",
            "Epoch 21/100\n",
            "380/380 - 1s - loss: 4.8646 - mse: 4.8700 - val_loss: 5.0245 - val_mse: 5.0253\n",
            "Epoch 22/100\n",
            "380/380 - 1s - loss: 4.8248 - mse: 4.8279 - val_loss: 4.9790 - val_mse: 4.9798\n",
            "Epoch 23/100\n",
            "380/380 - 1s - loss: 4.7889 - mse: 4.7857 - val_loss: 4.9334 - val_mse: 4.9343\n",
            "Epoch 24/100\n",
            "380/380 - 1s - loss: 4.7460 - mse: 4.7436 - val_loss: 4.8878 - val_mse: 4.8887\n",
            "Epoch 25/100\n",
            "380/380 - 1s - loss: 4.7032 - mse: 4.7016 - val_loss: 4.8423 - val_mse: 4.8433\n",
            "Epoch 26/100\n",
            "380/380 - 1s - loss: 4.6566 - mse: 4.6596 - val_loss: 4.7971 - val_mse: 4.7981\n",
            "Epoch 27/100\n",
            "380/380 - 1s - loss: 4.6269 - mse: 4.6178 - val_loss: 4.7520 - val_mse: 4.7530\n",
            "Epoch 28/100\n",
            "380/380 - 1s - loss: 4.5769 - mse: 4.5761 - val_loss: 4.7072 - val_mse: 4.7083\n",
            "Epoch 29/100\n",
            "380/380 - 1s - loss: 4.5290 - mse: 4.5347 - val_loss: 4.6627 - val_mse: 4.6638\n",
            "Epoch 30/100\n",
            "380/380 - 1s - loss: 4.4994 - mse: 4.4934 - val_loss: 4.6181 - val_mse: 4.6193\n",
            "Epoch 31/100\n",
            "380/380 - 1s - loss: 4.4516 - mse: 4.4519 - val_loss: 4.5735 - val_mse: 4.5747\n",
            "Epoch 32/100\n",
            "380/380 - 1s - loss: 4.4152 - mse: 4.4105 - val_loss: 4.5290 - val_mse: 4.5303\n",
            "Epoch 33/100\n",
            "380/380 - 1s - loss: 4.3645 - mse: 4.3691 - val_loss: 4.4846 - val_mse: 4.4859\n",
            "Epoch 34/100\n",
            "380/380 - 1s - loss: 4.3271 - mse: 4.3277 - val_loss: 4.4403 - val_mse: 4.4416\n",
            "Epoch 35/100\n",
            "380/380 - 1s - loss: 4.2849 - mse: 4.2865 - val_loss: 4.3960 - val_mse: 4.3973\n",
            "Epoch 36/100\n",
            "380/380 - 1s - loss: 4.2424 - mse: 4.2453 - val_loss: 4.3517 - val_mse: 4.3531\n",
            "Epoch 37/100\n",
            "380/380 - 1s - loss: 4.2044 - mse: 4.2041 - val_loss: 4.3075 - val_mse: 4.3088\n",
            "Epoch 38/100\n",
            "380/380 - 1s - loss: 4.1691 - mse: 4.1629 - val_loss: 4.2633 - val_mse: 4.2647\n",
            "Epoch 39/100\n",
            "380/380 - 1s - loss: 4.1260 - mse: 4.1217 - val_loss: 4.2192 - val_mse: 4.2207\n",
            "Epoch 40/100\n",
            "380/380 - 1s - loss: 4.0741 - mse: 4.0806 - val_loss: 4.1753 - val_mse: 4.1768\n",
            "Epoch 41/100\n",
            "380/380 - 1s - loss: 4.0354 - mse: 4.0397 - val_loss: 4.1314 - val_mse: 4.1329\n",
            "Epoch 42/100\n",
            "380/380 - 1s - loss: 4.0038 - mse: 3.9987 - val_loss: 4.0877 - val_mse: 4.0893\n",
            "Epoch 43/100\n",
            "380/380 - 1s - loss: 3.9602 - mse: 3.9577 - val_loss: 4.0439 - val_mse: 4.0455\n",
            "Epoch 44/100\n",
            "380/380 - 1s - loss: 3.9140 - mse: 3.9167 - val_loss: 4.0000 - val_mse: 4.0016\n",
            "Epoch 45/100\n",
            "380/380 - 1s - loss: 3.8808 - mse: 3.8758 - val_loss: 3.9564 - val_mse: 3.9580\n",
            "Epoch 46/100\n",
            "380/380 - 1s - loss: 3.8324 - mse: 3.8348 - val_loss: 3.9127 - val_mse: 3.9143\n",
            "Epoch 47/100\n",
            "380/380 - 1s - loss: 3.7893 - mse: 3.7939 - val_loss: 3.8690 - val_mse: 3.8707\n",
            "Epoch 48/100\n",
            "380/380 - 1s - loss: 3.7545 - mse: 3.7531 - val_loss: 3.8256 - val_mse: 3.8273\n",
            "Epoch 49/100\n",
            "380/380 - 1s - loss: 3.7167 - mse: 3.7123 - val_loss: 3.7821 - val_mse: 3.7839\n",
            "Epoch 50/100\n",
            "380/380 - 1s - loss: 3.6740 - mse: 3.6715 - val_loss: 3.7388 - val_mse: 3.7405\n",
            "Epoch 51/100\n",
            "380/380 - 1s - loss: 3.6300 - mse: 3.6307 - val_loss: 3.6955 - val_mse: 3.6972\n",
            "Epoch 52/100\n",
            "380/380 - 1s - loss: 3.5875 - mse: 3.5901 - val_loss: 3.6522 - val_mse: 3.6539\n",
            "Epoch 53/100\n",
            "380/380 - 1s - loss: 3.5446 - mse: 3.5495 - val_loss: 3.6092 - val_mse: 3.6110\n",
            "Epoch 54/100\n",
            "380/380 - 1s - loss: 3.5094 - mse: 3.5090 - val_loss: 3.5661 - val_mse: 3.5679\n",
            "Epoch 55/100\n",
            "380/380 - 1s - loss: 3.4685 - mse: 3.4686 - val_loss: 3.5232 - val_mse: 3.5251\n",
            "Epoch 56/100\n",
            "380/380 - 1s - loss: 3.4274 - mse: 3.4283 - val_loss: 3.4807 - val_mse: 3.4825\n",
            "Epoch 57/100\n",
            "380/380 - 1s - loss: 3.3851 - mse: 3.3880 - val_loss: 3.4381 - val_mse: 3.4400\n",
            "Epoch 58/100\n",
            "380/380 - 1s - loss: 3.3456 - mse: 3.3479 - val_loss: 3.3957 - val_mse: 3.3975\n",
            "Epoch 59/100\n",
            "380/380 - 1s - loss: 3.3092 - mse: 3.3079 - val_loss: 3.3533 - val_mse: 3.3552\n",
            "Epoch 60/100\n",
            "380/380 - 1s - loss: 3.2643 - mse: 3.2678 - val_loss: 3.3109 - val_mse: 3.3128\n",
            "Epoch 61/100\n",
            "380/380 - 1s - loss: 3.2235 - mse: 3.2280 - val_loss: 3.2688 - val_mse: 3.2707\n",
            "Epoch 62/100\n",
            "380/380 - 1s - loss: 3.1900 - mse: 3.1882 - val_loss: 3.2269 - val_mse: 3.2289\n",
            "Epoch 63/100\n",
            "380/380 - 1s - loss: 3.1467 - mse: 3.1483 - val_loss: 3.1847 - val_mse: 3.1866\n",
            "Epoch 64/100\n",
            "380/380 - 1s - loss: 3.1131 - mse: 3.1086 - val_loss: 3.1430 - val_mse: 3.1449\n",
            "Epoch 65/100\n",
            "380/380 - 1s - loss: 3.0691 - mse: 3.0689 - val_loss: 3.1010 - val_mse: 3.1030\n",
            "Epoch 66/100\n",
            "380/380 - 1s - loss: 3.0259 - mse: 3.0293 - val_loss: 3.0593 - val_mse: 3.0613\n",
            "Epoch 67/100\n",
            "380/380 - 1s - loss: 2.9928 - mse: 2.9899 - val_loss: 3.0178 - val_mse: 3.0198\n",
            "Epoch 68/100\n",
            "380/380 - 1s - loss: 2.9475 - mse: 2.9504 - val_loss: 2.9764 - val_mse: 2.9784\n",
            "Epoch 69/100\n",
            "380/380 - 1s - loss: 2.9144 - mse: 2.9112 - val_loss: 2.9351 - val_mse: 2.9372\n",
            "Epoch 70/100\n",
            "380/380 - 1s - loss: 2.8734 - mse: 2.8719 - val_loss: 2.8940 - val_mse: 2.8960\n",
            "Epoch 71/100\n",
            "380/380 - 1s - loss: 2.8404 - mse: 2.8328 - val_loss: 2.8529 - val_mse: 2.8550\n",
            "Epoch 72/100\n",
            "380/380 - 1s - loss: 2.7954 - mse: 2.7939 - val_loss: 2.8123 - val_mse: 2.8144\n",
            "Epoch 73/100\n",
            "380/380 - 1s - loss: 2.7541 - mse: 2.7553 - val_loss: 2.7717 - val_mse: 2.7737\n",
            "Epoch 74/100\n",
            "380/380 - 1s - loss: 2.7139 - mse: 2.7168 - val_loss: 2.7314 - val_mse: 2.7335\n",
            "Epoch 75/100\n",
            "380/380 - 1s - loss: 2.6826 - mse: 2.6783 - val_loss: 2.6912 - val_mse: 2.6933\n",
            "Epoch 76/100\n",
            "380/380 - 1s - loss: 2.6390 - mse: 2.6399 - val_loss: 2.6510 - val_mse: 2.6531\n",
            "Epoch 77/100\n",
            "380/380 - 1s - loss: 2.6021 - mse: 2.6018 - val_loss: 2.6112 - val_mse: 2.6133\n",
            "Epoch 78/100\n",
            "380/380 - 1s - loss: 2.5692 - mse: 2.5638 - val_loss: 2.5715 - val_mse: 2.5736\n",
            "Epoch 79/100\n",
            "380/380 - 1s - loss: 2.5230 - mse: 2.5257 - val_loss: 2.5318 - val_mse: 2.5339\n",
            "Epoch 80/100\n",
            "380/380 - 1s - loss: 2.4854 - mse: 2.4879 - val_loss: 2.4923 - val_mse: 2.4944\n",
            "Epoch 81/100\n",
            "380/380 - 1s - loss: 2.4530 - mse: 2.4502 - val_loss: 2.4531 - val_mse: 2.4552\n",
            "Epoch 82/100\n",
            "380/380 - 1s - loss: 2.4115 - mse: 2.4126 - val_loss: 2.4139 - val_mse: 2.4160\n",
            "Epoch 83/100\n",
            "380/380 - 1s - loss: 2.3723 - mse: 2.3753 - val_loss: 2.3750 - val_mse: 2.3771\n",
            "Epoch 84/100\n",
            "380/380 - 1s - loss: 2.3358 - mse: 2.3383 - val_loss: 2.3366 - val_mse: 2.3387\n",
            "Epoch 85/100\n",
            "380/380 - 1s - loss: 2.3075 - mse: 2.3016 - val_loss: 2.2986 - val_mse: 2.3007\n",
            "Epoch 86/100\n",
            "380/380 - 1s - loss: 2.2620 - mse: 2.2651 - val_loss: 2.2606 - val_mse: 2.2627\n",
            "Epoch 87/100\n",
            "380/380 - 1s - loss: 2.2279 - mse: 2.2288 - val_loss: 2.2230 - val_mse: 2.2251\n",
            "Epoch 88/100\n",
            "380/380 - 1s - loss: 2.1950 - mse: 2.1926 - val_loss: 2.1856 - val_mse: 2.1877\n",
            "Epoch 89/100\n",
            "380/380 - 1s - loss: 2.1576 - mse: 2.1567 - val_loss: 2.1484 - val_mse: 2.1505\n",
            "Epoch 90/100\n",
            "380/380 - 1s - loss: 2.1203 - mse: 2.1211 - val_loss: 2.1115 - val_mse: 2.1136\n",
            "Epoch 91/100\n",
            "380/380 - 1s - loss: 2.0828 - mse: 2.0857 - val_loss: 2.0749 - val_mse: 2.0770\n",
            "Epoch 92/100\n",
            "380/380 - 1s - loss: 2.0486 - mse: 2.0506 - val_loss: 2.0386 - val_mse: 2.0407\n",
            "Epoch 93/100\n",
            "380/380 - 1s - loss: 2.0127 - mse: 2.0157 - val_loss: 2.0026 - val_mse: 2.0047\n",
            "Epoch 94/100\n",
            "380/380 - 1s - loss: 1.9780 - mse: 1.9811 - val_loss: 1.9670 - val_mse: 1.9691\n",
            "Epoch 95/100\n",
            "380/380 - 1s - loss: 1.9448 - mse: 1.9468 - val_loss: 1.9317 - val_mse: 1.9338\n",
            "Epoch 96/100\n",
            "380/380 - 1s - loss: 1.9100 - mse: 1.9129 - val_loss: 1.8968 - val_mse: 1.8988\n",
            "Epoch 97/100\n",
            "380/380 - 1s - loss: 1.8830 - mse: 1.8793 - val_loss: 1.8622 - val_mse: 1.8643\n",
            "Epoch 98/100\n",
            "380/380 - 1s - loss: 1.8486 - mse: 1.8459 - val_loss: 1.8278 - val_mse: 1.8299\n",
            "Epoch 99/100\n",
            "380/380 - 1s - loss: 1.8114 - mse: 1.8129 - val_loss: 1.7940 - val_mse: 1.7960\n",
            "Epoch 100/100\n",
            "380/380 - 1s - loss: 1.7783 - mse: 1.7803 - val_loss: 1.7606 - val_mse: 1.7626\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 33%|███▎      | 7/21 [08:09<16:24, 70.32s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 32)                288       \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 32)                1056      \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "Output (Dense)               (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 1,377\n",
            "Trainable params: 1,377\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/100\n",
            "380/380 - 1s - loss: 6.2614 - mse: 6.2583 - val_loss: 6.3718 - val_mse: 6.3708\n",
            "Epoch 2/100\n",
            "380/380 - 1s - loss: 5.9068 - mse: 5.9134 - val_loss: 6.0247 - val_mse: 6.0238\n",
            "Epoch 3/100\n",
            "380/380 - 1s - loss: 5.5876 - mse: 5.5928 - val_loss: 5.7008 - val_mse: 5.7000\n",
            "Epoch 4/100\n",
            "380/380 - 1s - loss: 5.2851 - mse: 5.2934 - val_loss: 5.3981 - val_mse: 5.3974\n",
            "Epoch 5/100\n",
            "380/380 - 1s - loss: 5.0097 - mse: 5.0133 - val_loss: 5.1142 - val_mse: 5.1136\n",
            "Epoch 6/100\n",
            "380/380 - 1s - loss: 4.7469 - mse: 4.7505 - val_loss: 4.8477 - val_mse: 4.8472\n",
            "Epoch 7/100\n",
            "380/380 - 1s - loss: 4.4976 - mse: 4.5037 - val_loss: 4.5970 - val_mse: 4.5966\n",
            "Epoch 8/100\n",
            "380/380 - 1s - loss: 4.2678 - mse: 4.2715 - val_loss: 4.3609 - val_mse: 4.3606\n",
            "Epoch 9/100\n",
            "380/380 - 1s - loss: 4.0457 - mse: 4.0526 - val_loss: 4.1385 - val_mse: 4.1383\n",
            "Epoch 10/100\n",
            "380/380 - 1s - loss: 3.8469 - mse: 3.8464 - val_loss: 3.9281 - val_mse: 3.9281\n",
            "Epoch 11/100\n",
            "380/380 - 1s - loss: 3.6564 - mse: 3.6514 - val_loss: 3.7295 - val_mse: 3.7295\n",
            "Epoch 12/100\n",
            "380/380 - 1s - loss: 3.4664 - mse: 3.4672 - val_loss: 3.5420 - val_mse: 3.5421\n",
            "Epoch 13/100\n",
            "380/380 - 1s - loss: 3.2908 - mse: 3.2933 - val_loss: 3.3648 - val_mse: 3.3650\n",
            "Epoch 14/100\n",
            "380/380 - 1s - loss: 3.1359 - mse: 3.1290 - val_loss: 3.1970 - val_mse: 3.1972\n",
            "Epoch 15/100\n",
            "380/380 - 1s - loss: 2.9685 - mse: 2.9734 - val_loss: 3.0389 - val_mse: 3.0392\n",
            "Epoch 16/100\n",
            "380/380 - 1s - loss: 2.8239 - mse: 2.8269 - val_loss: 2.8894 - val_mse: 2.8898\n",
            "Epoch 17/100\n",
            "380/380 - 1s - loss: 2.6860 - mse: 2.6883 - val_loss: 2.7481 - val_mse: 2.7486\n",
            "Epoch 18/100\n",
            "380/380 - 1s - loss: 2.5568 - mse: 2.5573 - val_loss: 2.6145 - val_mse: 2.6151\n",
            "Epoch 19/100\n",
            "380/380 - 1s - loss: 2.4322 - mse: 2.4336 - val_loss: 2.4886 - val_mse: 2.4892\n",
            "Epoch 20/100\n",
            "380/380 - 1s - loss: 2.3132 - mse: 2.3169 - val_loss: 2.3699 - val_mse: 2.3706\n",
            "Epoch 21/100\n",
            "380/380 - 1s - loss: 2.2037 - mse: 2.2069 - val_loss: 2.2579 - val_mse: 2.2587\n",
            "Epoch 22/100\n",
            "380/380 - 1s - loss: 2.1013 - mse: 2.1032 - val_loss: 2.1523 - val_mse: 2.1532\n",
            "Epoch 23/100\n",
            "380/380 - 1s - loss: 2.0064 - mse: 2.0054 - val_loss: 2.0529 - val_mse: 2.0538\n",
            "Epoch 24/100\n",
            "380/380 - 1s - loss: 1.9149 - mse: 1.9133 - val_loss: 1.9594 - val_mse: 1.9604\n",
            "Epoch 25/100\n",
            "380/380 - 1s - loss: 1.8251 - mse: 1.8268 - val_loss: 1.8715 - val_mse: 1.8726\n",
            "Epoch 26/100\n",
            "380/380 - 1s - loss: 1.7439 - mse: 1.7454 - val_loss: 1.7892 - val_mse: 1.7903\n",
            "Epoch 27/100\n",
            "380/380 - 1s - loss: 1.6738 - mse: 1.6692 - val_loss: 1.7117 - val_mse: 1.7128\n",
            "Epoch 28/100\n",
            "380/380 - 1s - loss: 1.5998 - mse: 1.5975 - val_loss: 1.6394 - val_mse: 1.6406\n",
            "Epoch 29/100\n",
            "380/380 - 1s - loss: 1.5290 - mse: 1.5305 - val_loss: 1.5719 - val_mse: 1.5731\n",
            "Epoch 30/100\n",
            "380/380 - 1s - loss: 1.4714 - mse: 1.4680 - val_loss: 1.5085 - val_mse: 1.5098\n",
            "Epoch 31/100\n",
            "380/380 - 1s - loss: 1.4089 - mse: 1.4093 - val_loss: 1.4495 - val_mse: 1.4508\n",
            "Epoch 32/100\n",
            "380/380 - 1s - loss: 1.3575 - mse: 1.3546 - val_loss: 1.3944 - val_mse: 1.3958\n",
            "Epoch 33/100\n",
            "380/380 - 1s - loss: 1.3014 - mse: 1.3036 - val_loss: 1.3432 - val_mse: 1.3446\n",
            "Epoch 34/100\n",
            "380/380 - 1s - loss: 1.2565 - mse: 1.2561 - val_loss: 1.2955 - val_mse: 1.2969\n",
            "Epoch 35/100\n",
            "380/380 - 1s - loss: 1.2110 - mse: 1.2119 - val_loss: 1.2511 - val_mse: 1.2526\n",
            "Epoch 36/100\n",
            "380/380 - 1s - loss: 1.1689 - mse: 1.1707 - val_loss: 1.2099 - val_mse: 1.2114\n",
            "Epoch 37/100\n",
            "380/380 - 1s - loss: 1.1323 - mse: 1.1325 - val_loss: 1.1716 - val_mse: 1.1731\n",
            "Epoch 38/100\n",
            "380/380 - 1s - loss: 1.0984 - mse: 1.0969 - val_loss: 1.1361 - val_mse: 1.1376\n",
            "Epoch 39/100\n",
            "380/380 - 1s - loss: 1.0672 - mse: 1.0638 - val_loss: 1.1032 - val_mse: 1.1048\n",
            "Epoch 40/100\n",
            "380/380 - 1s - loss: 1.0316 - mse: 1.0333 - val_loss: 1.0729 - val_mse: 1.0744\n",
            "Epoch 41/100\n",
            "380/380 - 1s - loss: 1.0035 - mse: 1.0050 - val_loss: 1.0448 - val_mse: 1.0464\n",
            "Epoch 42/100\n",
            "380/380 - 1s - loss: 0.9812 - mse: 0.9789 - val_loss: 1.0188 - val_mse: 1.0204\n",
            "Epoch 43/100\n",
            "380/380 - 1s - loss: 0.9550 - mse: 0.9545 - val_loss: 0.9948 - val_mse: 0.9964\n",
            "Epoch 44/100\n",
            "380/380 - 1s - loss: 0.9309 - mse: 0.9321 - val_loss: 0.9727 - val_mse: 0.9743\n",
            "Epoch 45/100\n",
            "380/380 - 1s - loss: 0.9134 - mse: 0.9114 - val_loss: 0.9522 - val_mse: 0.9538\n",
            "Epoch 46/100\n",
            "380/380 - 1s - loss: 0.8908 - mse: 0.8923 - val_loss: 0.9334 - val_mse: 0.9350\n",
            "Epoch 47/100\n",
            "380/380 - 1s - loss: 0.8731 - mse: 0.8746 - val_loss: 0.9161 - val_mse: 0.9177\n",
            "Epoch 48/100\n",
            "380/380 - 1s - loss: 0.8582 - mse: 0.8584 - val_loss: 0.9001 - val_mse: 0.9017\n",
            "Epoch 49/100\n",
            "380/380 - 1s - loss: 0.8434 - mse: 0.8433 - val_loss: 0.8854 - val_mse: 0.8870\n",
            "Epoch 50/100\n",
            "380/380 - 1s - loss: 0.8318 - mse: 0.8294 - val_loss: 0.8718 - val_mse: 0.8735\n",
            "Epoch 51/100\n",
            "380/380 - 1s - loss: 0.8168 - mse: 0.8167 - val_loss: 0.8594 - val_mse: 0.8610\n",
            "Epoch 52/100\n",
            "380/380 - 1s - loss: 0.8036 - mse: 0.8049 - val_loss: 0.8479 - val_mse: 0.8496\n",
            "Epoch 53/100\n",
            "380/380 - 1s - loss: 0.7931 - mse: 0.7941 - val_loss: 0.8374 - val_mse: 0.8391\n",
            "Epoch 54/100\n",
            "380/380 - 1s - loss: 0.7837 - mse: 0.7841 - val_loss: 0.8277 - val_mse: 0.8294\n",
            "Epoch 55/100\n",
            "380/380 - 1s - loss: 0.7740 - mse: 0.7748 - val_loss: 0.8187 - val_mse: 0.8204\n",
            "Epoch 56/100\n",
            "380/380 - 1s - loss: 0.7650 - mse: 0.7663 - val_loss: 0.8105 - val_mse: 0.8122\n",
            "Epoch 57/100\n",
            "380/380 - 1s - loss: 0.7579 - mse: 0.7585 - val_loss: 0.8029 - val_mse: 0.8046\n",
            "Epoch 58/100\n",
            "380/380 - 1s - loss: 0.7505 - mse: 0.7512 - val_loss: 0.7959 - val_mse: 0.7976\n",
            "Epoch 59/100\n",
            "380/380 - 1s - loss: 0.7454 - mse: 0.7445 - val_loss: 0.7895 - val_mse: 0.7911\n",
            "Epoch 60/100\n",
            "380/380 - 1s - loss: 0.7374 - mse: 0.7384 - val_loss: 0.7835 - val_mse: 0.7852\n",
            "Epoch 61/100\n",
            "380/380 - 1s - loss: 0.7317 - mse: 0.7327 - val_loss: 0.7780 - val_mse: 0.7797\n",
            "Epoch 62/100\n",
            "380/380 - 1s - loss: 0.7270 - mse: 0.7274 - val_loss: 0.7729 - val_mse: 0.7745\n",
            "Epoch 63/100\n",
            "380/380 - 1s - loss: 0.7223 - mse: 0.7224 - val_loss: 0.7681 - val_mse: 0.7698\n",
            "Epoch 64/100\n",
            "380/380 - 1s - loss: 0.7177 - mse: 0.7179 - val_loss: 0.7637 - val_mse: 0.7654\n",
            "Epoch 65/100\n",
            "380/380 - 1s - loss: 0.7145 - mse: 0.7136 - val_loss: 0.7596 - val_mse: 0.7613\n",
            "Epoch 66/100\n",
            "380/380 - 1s - loss: 0.7087 - mse: 0.7097 - val_loss: 0.7558 - val_mse: 0.7575\n",
            "Epoch 67/100\n",
            "380/380 - 1s - loss: 0.7077 - mse: 0.7060 - val_loss: 0.7523 - val_mse: 0.7539\n",
            "Epoch 68/100\n",
            "380/380 - 1s - loss: 0.7014 - mse: 0.7026 - val_loss: 0.7489 - val_mse: 0.7506\n",
            "Epoch 69/100\n",
            "380/380 - 1s - loss: 0.6995 - mse: 0.6994 - val_loss: 0.7458 - val_mse: 0.7475\n",
            "Epoch 70/100\n",
            "380/380 - 1s - loss: 0.6954 - mse: 0.6964 - val_loss: 0.7429 - val_mse: 0.7445\n",
            "Epoch 71/100\n",
            "380/380 - 1s - loss: 0.6987 - mse: 0.6936 - val_loss: 0.7401 - val_mse: 0.7417\n",
            "Epoch 72/100\n",
            "380/380 - 1s - loss: 0.6899 - mse: 0.6909 - val_loss: 0.7375 - val_mse: 0.7391\n",
            "Epoch 73/100\n",
            "380/380 - 1s - loss: 0.6877 - mse: 0.6884 - val_loss: 0.7351 - val_mse: 0.7367\n",
            "Epoch 74/100\n",
            "380/380 - 1s - loss: 0.6855 - mse: 0.6861 - val_loss: 0.7328 - val_mse: 0.7344\n",
            "Epoch 75/100\n",
            "380/380 - 1s - loss: 0.6863 - mse: 0.6839 - val_loss: 0.7306 - val_mse: 0.7322\n",
            "Epoch 76/100\n",
            "380/380 - 1s - loss: 0.6807 - mse: 0.6819 - val_loss: 0.7285 - val_mse: 0.7301\n",
            "Epoch 77/100\n",
            "380/380 - 1s - loss: 0.6796 - mse: 0.6799 - val_loss: 0.7265 - val_mse: 0.7281\n",
            "Epoch 78/100\n",
            "380/380 - 1s - loss: 0.6785 - mse: 0.6780 - val_loss: 0.7246 - val_mse: 0.7262\n",
            "Epoch 79/100\n",
            "380/380 - 1s - loss: 0.6756 - mse: 0.6763 - val_loss: 0.7228 - val_mse: 0.7244\n",
            "Epoch 80/100\n",
            "380/380 - 1s - loss: 0.6734 - mse: 0.6746 - val_loss: 0.7211 - val_mse: 0.7227\n",
            "Epoch 81/100\n",
            "380/380 - 1s - loss: 0.6724 - mse: 0.6730 - val_loss: 0.7194 - val_mse: 0.7210\n",
            "Epoch 82/100\n",
            "380/380 - 1s - loss: 0.6707 - mse: 0.6714 - val_loss: 0.7178 - val_mse: 0.7194\n",
            "Epoch 83/100\n",
            "380/380 - 1s - loss: 0.6691 - mse: 0.6700 - val_loss: 0.7162 - val_mse: 0.7178\n",
            "Epoch 84/100\n",
            "380/380 - 1s - loss: 0.6675 - mse: 0.6685 - val_loss: 0.7147 - val_mse: 0.7163\n",
            "Epoch 85/100\n",
            "380/380 - 1s - loss: 0.6698 - mse: 0.6672 - val_loss: 0.7133 - val_mse: 0.7148\n",
            "Epoch 86/100\n",
            "380/380 - 1s - loss: 0.6650 - mse: 0.6659 - val_loss: 0.7118 - val_mse: 0.7134\n",
            "Epoch 87/100\n",
            "380/380 - 1s - loss: 0.6637 - mse: 0.6646 - val_loss: 0.7105 - val_mse: 0.7120\n",
            "Epoch 88/100\n",
            "380/380 - 1s - loss: 0.6646 - mse: 0.6634 - val_loss: 0.7091 - val_mse: 0.7107\n",
            "Epoch 89/100\n",
            "380/380 - 1s - loss: 0.6616 - mse: 0.6622 - val_loss: 0.7078 - val_mse: 0.7093\n",
            "Epoch 90/100\n",
            "380/380 - 1s - loss: 0.6599 - mse: 0.6610 - val_loss: 0.7065 - val_mse: 0.7080\n",
            "Epoch 91/100\n",
            "380/380 - 1s - loss: 0.6597 - mse: 0.6599 - val_loss: 0.7052 - val_mse: 0.7068\n",
            "Epoch 92/100\n",
            "380/380 - 1s - loss: 0.6583 - mse: 0.6588 - val_loss: 0.7040 - val_mse: 0.7055\n",
            "Epoch 93/100\n",
            "380/380 - 1s - loss: 0.6572 - mse: 0.6577 - val_loss: 0.7028 - val_mse: 0.7043\n",
            "Epoch 94/100\n",
            "380/380 - 1s - loss: 0.6558 - mse: 0.6567 - val_loss: 0.7016 - val_mse: 0.7031\n",
            "Epoch 95/100\n",
            "380/380 - 1s - loss: 0.6549 - mse: 0.6557 - val_loss: 0.7004 - val_mse: 0.7020\n",
            "Epoch 96/100\n",
            "380/380 - 1s - loss: 0.6548 - mse: 0.6547 - val_loss: 0.6993 - val_mse: 0.7008\n",
            "Epoch 97/100\n",
            "380/380 - 1s - loss: 0.6543 - mse: 0.6537 - val_loss: 0.6981 - val_mse: 0.6996\n",
            "Epoch 98/100\n",
            "380/380 - 1s - loss: 0.6561 - mse: 0.6528 - val_loss: 0.6970 - val_mse: 0.6985\n",
            "Epoch 99/100\n",
            "380/380 - 1s - loss: 0.6513 - mse: 0.6518 - val_loss: 0.6959 - val_mse: 0.6974\n",
            "Epoch 100/100\n",
            "380/380 - 1s - loss: 0.6501 - mse: 0.6509 - val_loss: 0.6948 - val_mse: 0.6963\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 38%|███▊      | 8/21 [09:15<14:55, 68.86s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 64)                576       \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "Output (Dense)               (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 4,801\n",
            "Trainable params: 4,801\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/100\n",
            "380/380 - 1s - loss: 5.1724 - mse: 5.1695 - val_loss: 5.3120 - val_mse: 5.3132\n",
            "Epoch 2/100\n",
            "380/380 - 1s - loss: 4.9989 - mse: 5.0022 - val_loss: 5.1439 - val_mse: 5.1452\n",
            "Epoch 3/100\n",
            "380/380 - 1s - loss: 4.8365 - mse: 4.8399 - val_loss: 4.9814 - val_mse: 4.9827\n",
            "Epoch 4/100\n",
            "380/380 - 1s - loss: 4.6762 - mse: 4.6835 - val_loss: 4.8254 - val_mse: 4.8268\n",
            "Epoch 5/100\n",
            "380/380 - 1s - loss: 4.5326 - mse: 4.5330 - val_loss: 4.6750 - val_mse: 4.6765\n",
            "Epoch 6/100\n",
            "380/380 - 1s - loss: 4.3867 - mse: 4.3874 - val_loss: 4.5297 - val_mse: 4.5313\n",
            "Epoch 7/100\n",
            "380/380 - 1s - loss: 4.2411 - mse: 4.2467 - val_loss: 4.3897 - val_mse: 4.3913\n",
            "Epoch 8/100\n",
            "380/380 - 1s - loss: 4.1079 - mse: 4.1113 - val_loss: 4.2550 - val_mse: 4.2567\n",
            "Epoch 9/100\n",
            "380/380 - 1s - loss: 3.9740 - mse: 3.9800 - val_loss: 4.1240 - val_mse: 4.1258\n",
            "Epoch 10/100\n",
            "380/380 - 1s - loss: 3.8535 - mse: 3.8533 - val_loss: 3.9982 - val_mse: 4.0001\n",
            "Epoch 11/100\n",
            "380/380 - 1s - loss: 3.7378 - mse: 3.7314 - val_loss: 3.8768 - val_mse: 3.8786\n",
            "Epoch 12/100\n",
            "380/380 - 1s - loss: 3.6141 - mse: 3.6131 - val_loss: 3.7581 - val_mse: 3.7600\n",
            "Epoch 13/100\n",
            "380/380 - 1s - loss: 3.4946 - mse: 3.4983 - val_loss: 3.6441 - val_mse: 3.6461\n",
            "Epoch 14/100\n",
            "380/380 - 1s - loss: 3.3958 - mse: 3.3881 - val_loss: 3.5338 - val_mse: 3.5358\n",
            "Epoch 15/100\n",
            "380/380 - 1s - loss: 3.2760 - mse: 3.2816 - val_loss: 3.4273 - val_mse: 3.4293\n",
            "Epoch 16/100\n",
            "380/380 - 1s - loss: 3.1763 - mse: 3.1786 - val_loss: 3.3241 - val_mse: 3.3261\n",
            "Epoch 17/100\n",
            "380/380 - 1s - loss: 3.0771 - mse: 3.0790 - val_loss: 3.2246 - val_mse: 3.2266\n",
            "Epoch 18/100\n",
            "380/380 - 1s - loss: 2.9820 - mse: 2.9837 - val_loss: 3.1295 - val_mse: 3.1316\n",
            "Epoch 19/100\n",
            "380/380 - 1s - loss: 2.8916 - mse: 2.8922 - val_loss: 3.0380 - val_mse: 3.0401\n",
            "Epoch 20/100\n",
            "380/380 - 1s - loss: 2.7991 - mse: 2.8037 - val_loss: 2.9495 - val_mse: 2.9516\n",
            "Epoch 21/100\n",
            "380/380 - 1s - loss: 2.7140 - mse: 2.7187 - val_loss: 2.8647 - val_mse: 2.8669\n",
            "Epoch 22/100\n",
            "380/380 - 1s - loss: 2.6338 - mse: 2.6367 - val_loss: 2.7828 - val_mse: 2.7850\n",
            "Epoch 23/100\n",
            "380/380 - 1s - loss: 2.5591 - mse: 2.5575 - val_loss: 2.7043 - val_mse: 2.7065\n",
            "Epoch 24/100\n",
            "380/380 - 1s - loss: 2.4847 - mse: 2.4818 - val_loss: 2.6288 - val_mse: 2.6310\n",
            "Epoch 25/100\n",
            "380/380 - 1s - loss: 2.4086 - mse: 2.4093 - val_loss: 2.5557 - val_mse: 2.5580\n",
            "Epoch 26/100\n",
            "380/380 - 1s - loss: 2.3366 - mse: 2.3398 - val_loss: 2.4871 - val_mse: 2.4894\n",
            "Epoch 27/100\n",
            "380/380 - 1s - loss: 2.2797 - mse: 2.2736 - val_loss: 2.4199 - val_mse: 2.4222\n",
            "Epoch 28/100\n",
            "380/380 - 1s - loss: 2.2108 - mse: 2.2099 - val_loss: 2.3568 - val_mse: 2.3591\n",
            "Epoch 29/100\n",
            "380/380 - 1s - loss: 2.1475 - mse: 2.1498 - val_loss: 2.2961 - val_mse: 2.2984\n",
            "Epoch 30/100\n",
            "380/380 - 1s - loss: 2.0946 - mse: 2.0923 - val_loss: 2.2381 - val_mse: 2.2405\n",
            "Epoch 31/100\n",
            "380/380 - 1s - loss: 2.0358 - mse: 2.0372 - val_loss: 2.1825 - val_mse: 2.1848\n",
            "Epoch 32/100\n",
            "380/380 - 1s - loss: 1.9856 - mse: 1.9845 - val_loss: 2.1291 - val_mse: 2.1314\n",
            "Epoch 33/100\n",
            "380/380 - 1s - loss: 1.9316 - mse: 1.9340 - val_loss: 2.0777 - val_mse: 2.0800\n",
            "Epoch 34/100\n",
            "380/380 - 1s - loss: 1.8846 - mse: 1.8859 - val_loss: 2.0290 - val_mse: 2.0313\n",
            "Epoch 35/100\n",
            "380/380 - 1s - loss: 1.8389 - mse: 1.8402 - val_loss: 1.9822 - val_mse: 1.9845\n",
            "Epoch 36/100\n",
            "380/380 - 1s - loss: 1.7943 - mse: 1.7966 - val_loss: 1.9373 - val_mse: 1.9396\n",
            "Epoch 37/100\n",
            "380/380 - 1s - loss: 1.7537 - mse: 1.7547 - val_loss: 1.8943 - val_mse: 1.8967\n",
            "Epoch 38/100\n",
            "380/380 - 1s - loss: 1.7176 - mse: 1.7146 - val_loss: 1.8531 - val_mse: 1.8554\n",
            "Epoch 39/100\n",
            "380/380 - 1s - loss: 1.6788 - mse: 1.6765 - val_loss: 1.8139 - val_mse: 1.8163\n",
            "Epoch 40/100\n",
            "380/380 - 1s - loss: 1.6377 - mse: 1.6403 - val_loss: 1.7767 - val_mse: 1.7790\n",
            "Epoch 41/100\n",
            "380/380 - 1s - loss: 1.6036 - mse: 1.6060 - val_loss: 1.7410 - val_mse: 1.7433\n",
            "Epoch 42/100\n",
            "380/380 - 1s - loss: 1.5763 - mse: 1.5730 - val_loss: 1.7068 - val_mse: 1.7091\n",
            "Epoch 43/100\n",
            "380/380 - 1s - loss: 1.5426 - mse: 1.5412 - val_loss: 1.6738 - val_mse: 1.6761\n",
            "Epoch 44/100\n",
            "380/380 - 1s - loss: 1.5084 - mse: 1.5108 - val_loss: 1.6422 - val_mse: 1.6444\n",
            "Epoch 45/100\n",
            "380/380 - 1s - loss: 1.4844 - mse: 1.4818 - val_loss: 1.6119 - val_mse: 1.6142\n",
            "Epoch 46/100\n",
            "380/380 - 1s - loss: 1.4526 - mse: 1.4539 - val_loss: 1.5826 - val_mse: 1.5849\n",
            "Epoch 47/100\n",
            "380/380 - 1s - loss: 1.4253 - mse: 1.4271 - val_loss: 1.5545 - val_mse: 1.5567\n",
            "Epoch 48/100\n",
            "380/380 - 1s - loss: 1.4009 - mse: 1.4015 - val_loss: 1.5276 - val_mse: 1.5298\n",
            "Epoch 49/100\n",
            "380/380 - 1s - loss: 1.3780 - mse: 1.3769 - val_loss: 1.5017 - val_mse: 1.5039\n",
            "Epoch 50/100\n",
            "380/380 - 1s - loss: 1.3549 - mse: 1.3532 - val_loss: 1.4767 - val_mse: 1.4789\n",
            "Epoch 51/100\n",
            "380/380 - 1s - loss: 1.3290 - mse: 1.3303 - val_loss: 1.4525 - val_mse: 1.4547\n",
            "Epoch 52/100\n",
            "380/380 - 1s - loss: 1.3063 - mse: 1.3084 - val_loss: 1.4293 - val_mse: 1.4315\n",
            "Epoch 53/100\n",
            "380/380 - 1s - loss: 1.2854 - mse: 1.2873 - val_loss: 1.4071 - val_mse: 1.4092\n",
            "Epoch 54/100\n",
            "380/380 - 1s - loss: 1.2671 - mse: 1.2669 - val_loss: 1.3854 - val_mse: 1.3876\n",
            "Epoch 55/100\n",
            "380/380 - 1s - loss: 1.2456 - mse: 1.2471 - val_loss: 1.3645 - val_mse: 1.3667\n",
            "Epoch 56/100\n",
            "380/380 - 1s - loss: 1.2260 - mse: 1.2280 - val_loss: 1.3444 - val_mse: 1.3465\n",
            "Epoch 57/100\n",
            "380/380 - 1s - loss: 1.2079 - mse: 1.2096 - val_loss: 1.3251 - val_mse: 1.3272\n",
            "Epoch 58/100\n",
            "380/380 - 1s - loss: 1.1905 - mse: 1.1920 - val_loss: 1.3064 - val_mse: 1.3084\n",
            "Epoch 59/100\n",
            "380/380 - 1s - loss: 1.1763 - mse: 1.1749 - val_loss: 1.2883 - val_mse: 1.2904\n",
            "Epoch 60/100\n",
            "380/380 - 1s - loss: 1.1568 - mse: 1.1584 - val_loss: 1.2707 - val_mse: 1.2728\n",
            "Epoch 61/100\n",
            "380/380 - 1s - loss: 1.1408 - mse: 1.1424 - val_loss: 1.2537 - val_mse: 1.2557\n",
            "Epoch 62/100\n",
            "380/380 - 1s - loss: 1.1265 - mse: 1.1270 - val_loss: 1.2374 - val_mse: 1.2395\n",
            "Epoch 63/100\n",
            "380/380 - 1s - loss: 1.1112 - mse: 1.1122 - val_loss: 1.2216 - val_mse: 1.2236\n",
            "Epoch 64/100\n",
            "380/380 - 1s - loss: 1.0970 - mse: 1.0978 - val_loss: 1.2066 - val_mse: 1.2086\n",
            "Epoch 65/100\n",
            "380/380 - 1s - loss: 1.0834 - mse: 1.0839 - val_loss: 1.1917 - val_mse: 1.1937\n",
            "Epoch 66/100\n",
            "380/380 - 1s - loss: 1.0701 - mse: 1.0705 - val_loss: 1.1777 - val_mse: 1.1797\n",
            "Epoch 67/100\n",
            "380/380 - 1s - loss: 1.0580 - mse: 1.0576 - val_loss: 1.1639 - val_mse: 1.1658\n",
            "Epoch 68/100\n",
            "380/380 - 1s - loss: 1.0434 - mse: 1.0451 - val_loss: 1.1506 - val_mse: 1.1526\n",
            "Epoch 69/100\n",
            "380/380 - 1s - loss: 1.0335 - mse: 1.0330 - val_loss: 1.1378 - val_mse: 1.1398\n",
            "Epoch 70/100\n",
            "380/380 - 1s - loss: 1.0201 - mse: 1.0212 - val_loss: 1.1256 - val_mse: 1.1275\n",
            "Epoch 71/100\n",
            "380/380 - 1s - loss: 1.0118 - mse: 1.0100 - val_loss: 1.1138 - val_mse: 1.1157\n",
            "Epoch 72/100\n",
            "380/380 - 1s - loss: 0.9982 - mse: 0.9991 - val_loss: 1.1021 - val_mse: 1.1040\n",
            "Epoch 73/100\n",
            "380/380 - 1s - loss: 0.9875 - mse: 0.9886 - val_loss: 1.0911 - val_mse: 1.0930\n",
            "Epoch 74/100\n",
            "380/380 - 1s - loss: 0.9771 - mse: 0.9785 - val_loss: 1.0806 - val_mse: 1.0824\n",
            "Epoch 75/100\n",
            "380/380 - 1s - loss: 0.9697 - mse: 0.9688 - val_loss: 1.0704 - val_mse: 1.0723\n",
            "Epoch 76/100\n",
            "380/380 - 1s - loss: 0.9593 - mse: 0.9595 - val_loss: 1.0606 - val_mse: 1.0625\n",
            "Epoch 77/100\n",
            "380/380 - 1s - loss: 0.9499 - mse: 0.9506 - val_loss: 1.0513 - val_mse: 1.0531\n",
            "Epoch 78/100\n",
            "380/380 - 1s - loss: 0.9445 - mse: 0.9418 - val_loss: 1.0420 - val_mse: 1.0439\n",
            "Epoch 79/100\n",
            "380/380 - 1s - loss: 0.9328 - mse: 0.9335 - val_loss: 1.0334 - val_mse: 1.0352\n",
            "Epoch 80/100\n",
            "380/380 - 1s - loss: 0.9241 - mse: 0.9255 - val_loss: 1.0250 - val_mse: 1.0268\n",
            "Epoch 81/100\n",
            "380/380 - 1s - loss: 0.9175 - mse: 0.9178 - val_loss: 1.0170 - val_mse: 1.0188\n",
            "Epoch 82/100\n",
            "380/380 - 1s - loss: 0.9091 - mse: 0.9103 - val_loss: 1.0090 - val_mse: 1.0108\n",
            "Epoch 83/100\n",
            "380/380 - 1s - loss: 0.9018 - mse: 0.9030 - val_loss: 1.0013 - val_mse: 1.0031\n",
            "Epoch 84/100\n",
            "380/380 - 1s - loss: 0.8947 - mse: 0.8960 - val_loss: 0.9939 - val_mse: 0.9957\n",
            "Epoch 85/100\n",
            "380/380 - 1s - loss: 0.8921 - mse: 0.8893 - val_loss: 0.9868 - val_mse: 0.9886\n",
            "Epoch 86/100\n",
            "380/380 - 1s - loss: 0.8815 - mse: 0.8828 - val_loss: 0.9800 - val_mse: 0.9818\n",
            "Epoch 87/100\n",
            "380/380 - 1s - loss: 0.8755 - mse: 0.8765 - val_loss: 0.9733 - val_mse: 0.9751\n",
            "Epoch 88/100\n",
            "380/380 - 1s - loss: 0.8724 - mse: 0.8704 - val_loss: 0.9668 - val_mse: 0.9686\n",
            "Epoch 89/100\n",
            "380/380 - 1s - loss: 0.8645 - mse: 0.8645 - val_loss: 0.9607 - val_mse: 0.9624\n",
            "Epoch 90/100\n",
            "380/380 - 1s - loss: 0.8578 - mse: 0.8589 - val_loss: 0.9546 - val_mse: 0.9564\n",
            "Epoch 91/100\n",
            "380/380 - 1s - loss: 0.8524 - mse: 0.8534 - val_loss: 0.9488 - val_mse: 0.9505\n",
            "Epoch 92/100\n",
            "380/380 - 1s - loss: 0.8481 - mse: 0.8481 - val_loss: 0.9434 - val_mse: 0.9451\n",
            "Epoch 93/100\n",
            "380/380 - 1s - loss: 0.8418 - mse: 0.8430 - val_loss: 0.9376 - val_mse: 0.9393\n",
            "Epoch 94/100\n",
            "380/380 - 1s - loss: 0.8373 - mse: 0.8381 - val_loss: 0.9324 - val_mse: 0.9341\n",
            "Epoch 95/100\n",
            "380/380 - 1s - loss: 0.8324 - mse: 0.8335 - val_loss: 0.9274 - val_mse: 0.9291\n",
            "Epoch 96/100\n",
            "380/380 - 1s - loss: 0.8284 - mse: 0.8289 - val_loss: 0.9224 - val_mse: 0.9241\n",
            "Epoch 97/100\n",
            "380/380 - 1s - loss: 0.8249 - mse: 0.8245 - val_loss: 0.9176 - val_mse: 0.9193\n",
            "Epoch 98/100\n",
            "380/380 - 1s - loss: 0.8237 - mse: 0.8202 - val_loss: 0.9128 - val_mse: 0.9145\n",
            "Epoch 99/100\n",
            "380/380 - 1s - loss: 0.8147 - mse: 0.8160 - val_loss: 0.9083 - val_mse: 0.9100\n",
            "Epoch 100/100\n",
            "380/380 - 1s - loss: 0.8108 - mse: 0.8119 - val_loss: 0.9039 - val_mse: 0.9055\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 43%|████▎     | 9/21 [10:29<14:05, 70.49s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 64)                576       \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "Output (Dense)               (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 4,801\n",
            "Trainable params: 4,801\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/100\n",
            "380/380 - 1s - loss: 2.1436 - mse: 6.0551 - val_loss: 2.1767 - val_mse: 6.2939\n",
            "Epoch 2/100\n",
            "380/380 - 1s - loss: 2.1289 - mse: 5.9968 - val_loss: 2.1638 - val_mse: 6.2344\n",
            "Epoch 3/100\n",
            "380/380 - 1s - loss: 2.1165 - mse: 5.9393 - val_loss: 2.1509 - val_mse: 6.1754\n",
            "Epoch 4/100\n",
            "380/380 - 1s - loss: 2.1029 - mse: 5.8824 - val_loss: 2.1380 - val_mse: 6.1169\n",
            "Epoch 5/100\n",
            "380/380 - 1s - loss: 2.0918 - mse: 5.8260 - val_loss: 2.1252 - val_mse: 6.0588\n",
            "Epoch 6/100\n",
            "380/380 - 1s - loss: 2.0788 - mse: 5.7701 - val_loss: 2.1123 - val_mse: 6.0013\n",
            "Epoch 7/100\n",
            "380/380 - 1s - loss: 2.0650 - mse: 5.7146 - val_loss: 2.0995 - val_mse: 5.9442\n",
            "Epoch 8/100\n",
            "380/380 - 1s - loss: 2.0530 - mse: 5.6597 - val_loss: 2.0867 - val_mse: 5.8875\n",
            "Epoch 9/100\n",
            "380/380 - 1s - loss: 2.0391 - mse: 5.6047 - val_loss: 2.0739 - val_mse: 5.8313\n",
            "Epoch 10/100\n",
            "380/380 - 1s - loss: 2.0285 - mse: 5.5509 - val_loss: 2.0611 - val_mse: 5.7756\n",
            "Epoch 11/100\n",
            "380/380 - 1s - loss: 2.0169 - mse: 5.4970 - val_loss: 2.0484 - val_mse: 5.7203\n",
            "Epoch 12/100\n",
            "380/380 - 1s - loss: 2.0034 - mse: 5.4436 - val_loss: 2.0356 - val_mse: 5.6654\n",
            "Epoch 13/100\n",
            "380/380 - 1s - loss: 1.9898 - mse: 5.3907 - val_loss: 2.0229 - val_mse: 5.6109\n",
            "Epoch 14/100\n",
            "380/380 - 1s - loss: 1.9790 - mse: 5.3381 - val_loss: 2.0102 - val_mse: 5.5568\n",
            "Epoch 15/100\n",
            "380/380 - 1s - loss: 1.9631 - mse: 5.2860 - val_loss: 1.9975 - val_mse: 5.5031\n",
            "Epoch 16/100\n",
            "380/380 - 1s - loss: 1.9518 - mse: 5.2343 - val_loss: 1.9848 - val_mse: 5.4498\n",
            "Epoch 17/100\n",
            "380/380 - 1s - loss: 1.9385 - mse: 5.1828 - val_loss: 1.9721 - val_mse: 5.3969\n",
            "Epoch 18/100\n",
            "380/380 - 1s - loss: 1.9267 - mse: 5.1318 - val_loss: 1.9594 - val_mse: 5.3444\n",
            "Epoch 19/100\n",
            "380/380 - 1s - loss: 1.9136 - mse: 5.0814 - val_loss: 1.9468 - val_mse: 5.2923\n",
            "Epoch 20/100\n",
            "380/380 - 1s - loss: 1.9002 - mse: 5.0309 - val_loss: 1.9341 - val_mse: 5.2405\n",
            "Epoch 21/100\n",
            "380/380 - 1s - loss: 1.8878 - mse: 4.9812 - val_loss: 1.9215 - val_mse: 5.1891\n",
            "Epoch 22/100\n",
            "380/380 - 1s - loss: 1.8761 - mse: 4.9316 - val_loss: 1.9088 - val_mse: 5.1381\n",
            "Epoch 23/100\n",
            "380/380 - 1s - loss: 1.8642 - mse: 4.8825 - val_loss: 1.8962 - val_mse: 5.0875\n",
            "Epoch 24/100\n",
            "380/380 - 1s - loss: 1.8513 - mse: 4.8338 - val_loss: 1.8836 - val_mse: 5.0373\n",
            "Epoch 25/100\n",
            "380/380 - 1s - loss: 1.8391 - mse: 4.7854 - val_loss: 1.8710 - val_mse: 4.9875\n",
            "Epoch 26/100\n",
            "380/380 - 1s - loss: 1.8256 - mse: 4.7374 - val_loss: 1.8584 - val_mse: 4.9380\n",
            "Epoch 27/100\n",
            "380/380 - 1s - loss: 1.8152 - mse: 4.6898 - val_loss: 1.8458 - val_mse: 4.8889\n",
            "Epoch 28/100\n",
            "380/380 - 1s - loss: 1.8007 - mse: 4.6423 - val_loss: 1.8333 - val_mse: 4.8402\n",
            "Epoch 29/100\n",
            "380/380 - 1s - loss: 1.7867 - mse: 4.5961 - val_loss: 1.8208 - val_mse: 4.7920\n",
            "Epoch 30/100\n",
            "380/380 - 1s - loss: 1.7777 - mse: 4.5495 - val_loss: 1.8084 - val_mse: 4.7441\n",
            "Epoch 31/100\n",
            "380/380 - 1s - loss: 1.7638 - mse: 4.5034 - val_loss: 1.7960 - val_mse: 4.6966\n",
            "Epoch 32/100\n",
            "380/380 - 1s - loss: 1.7522 - mse: 4.4577 - val_loss: 1.7837 - val_mse: 4.6495\n",
            "Epoch 33/100\n",
            "380/380 - 1s - loss: 1.7389 - mse: 4.4125 - val_loss: 1.7714 - val_mse: 4.6028\n",
            "Epoch 34/100\n",
            "380/380 - 1s - loss: 1.7270 - mse: 4.3678 - val_loss: 1.7592 - val_mse: 4.5565\n",
            "Epoch 35/100\n",
            "380/380 - 1s - loss: 1.7145 - mse: 4.3231 - val_loss: 1.7471 - val_mse: 4.5106\n",
            "Epoch 36/100\n",
            "380/380 - 1s - loss: 1.7029 - mse: 4.2794 - val_loss: 1.7350 - val_mse: 4.4653\n",
            "Epoch 37/100\n",
            "380/380 - 1s - loss: 1.6906 - mse: 4.2359 - val_loss: 1.7230 - val_mse: 4.4203\n",
            "Epoch 38/100\n",
            "380/380 - 1s - loss: 1.6807 - mse: 4.1926 - val_loss: 1.7110 - val_mse: 4.3757\n",
            "Epoch 39/100\n",
            "380/380 - 1s - loss: 1.6675 - mse: 4.1502 - val_loss: 1.6992 - val_mse: 4.3318\n",
            "Epoch 40/100\n",
            "380/380 - 1s - loss: 1.6535 - mse: 4.1081 - val_loss: 1.6874 - val_mse: 4.2882\n",
            "Epoch 41/100\n",
            "380/380 - 1s - loss: 1.6430 - mse: 4.0664 - val_loss: 1.6757 - val_mse: 4.2451\n",
            "Epoch 42/100\n",
            "380/380 - 1s - loss: 1.6337 - mse: 4.0250 - val_loss: 1.6641 - val_mse: 4.2024\n",
            "Epoch 43/100\n",
            "380/380 - 1s - loss: 1.6217 - mse: 3.9841 - val_loss: 1.6526 - val_mse: 4.1600\n",
            "Epoch 44/100\n",
            "380/380 - 1s - loss: 1.6084 - mse: 3.9435 - val_loss: 1.6412 - val_mse: 4.1181\n",
            "Epoch 45/100\n",
            "380/380 - 1s - loss: 1.5989 - mse: 3.9034 - val_loss: 1.6299 - val_mse: 4.0766\n",
            "Epoch 46/100\n",
            "380/380 - 1s - loss: 1.5864 - mse: 3.8638 - val_loss: 1.6187 - val_mse: 4.0355\n",
            "Epoch 47/100\n",
            "380/380 - 1s - loss: 1.5742 - mse: 3.8242 - val_loss: 1.6076 - val_mse: 3.9949\n",
            "Epoch 48/100\n",
            "380/380 - 1s - loss: 1.5653 - mse: 3.7855 - val_loss: 1.5966 - val_mse: 3.9546\n",
            "Epoch 49/100\n",
            "380/380 - 1s - loss: 1.5546 - mse: 3.7472 - val_loss: 1.5856 - val_mse: 3.9148\n",
            "Epoch 50/100\n",
            "380/380 - 1s - loss: 1.5430 - mse: 3.7090 - val_loss: 1.5747 - val_mse: 3.8754\n",
            "Epoch 51/100\n",
            "380/380 - 1s - loss: 1.5322 - mse: 3.6714 - val_loss: 1.5639 - val_mse: 3.8364\n",
            "Epoch 52/100\n",
            "380/380 - 1s - loss: 1.5209 - mse: 3.6341 - val_loss: 1.5532 - val_mse: 3.7978\n",
            "Epoch 53/100\n",
            "380/380 - 1s - loss: 1.5092 - mse: 3.5973 - val_loss: 1.5426 - val_mse: 3.7597\n",
            "Epoch 54/100\n",
            "380/380 - 1s - loss: 1.5007 - mse: 3.5608 - val_loss: 1.5320 - val_mse: 3.7219\n",
            "Epoch 55/100\n",
            "380/380 - 1s - loss: 1.4902 - mse: 3.5247 - val_loss: 1.5216 - val_mse: 3.6846\n",
            "Epoch 56/100\n",
            "380/380 - 1s - loss: 1.4798 - mse: 3.4891 - val_loss: 1.5113 - val_mse: 3.6477\n",
            "Epoch 57/100\n",
            "380/380 - 1s - loss: 1.4680 - mse: 3.4541 - val_loss: 1.5011 - val_mse: 3.6112\n",
            "Epoch 58/100\n",
            "380/380 - 1s - loss: 1.4590 - mse: 3.4193 - val_loss: 1.4911 - val_mse: 3.5755\n",
            "Epoch 59/100\n",
            "380/380 - 1s - loss: 1.4498 - mse: 3.3851 - val_loss: 1.4811 - val_mse: 3.5401\n",
            "Epoch 60/100\n",
            "380/380 - 1s - loss: 1.4385 - mse: 3.3516 - val_loss: 1.4713 - val_mse: 3.5051\n",
            "Epoch 61/100\n",
            "380/380 - 1s - loss: 1.4281 - mse: 3.3182 - val_loss: 1.4616 - val_mse: 3.4706\n",
            "Epoch 62/100\n",
            "380/380 - 1s - loss: 1.4208 - mse: 3.2852 - val_loss: 1.4520 - val_mse: 3.4364\n",
            "Epoch 63/100\n",
            "380/380 - 1s - loss: 1.4100 - mse: 3.2528 - val_loss: 1.4425 - val_mse: 3.4027\n",
            "Epoch 64/100\n",
            "380/380 - 1s - loss: 1.4036 - mse: 3.2208 - val_loss: 1.4331 - val_mse: 3.3694\n",
            "Epoch 65/100\n",
            "380/380 - 1s - loss: 1.3930 - mse: 3.1890 - val_loss: 1.4238 - val_mse: 3.3364\n",
            "Epoch 66/100\n",
            "380/380 - 1s - loss: 1.3819 - mse: 3.1575 - val_loss: 1.4147 - val_mse: 3.3038\n",
            "Epoch 67/100\n",
            "380/380 - 1s - loss: 1.3756 - mse: 3.1265 - val_loss: 1.4055 - val_mse: 3.2715\n",
            "Epoch 68/100\n",
            "380/380 - 1s - loss: 1.3645 - mse: 3.0959 - val_loss: 1.3965 - val_mse: 3.2396\n",
            "Epoch 69/100\n",
            "380/380 - 1s - loss: 1.3577 - mse: 3.0654 - val_loss: 1.3875 - val_mse: 3.2081\n",
            "Epoch 70/100\n",
            "380/380 - 1s - loss: 1.3481 - mse: 3.0355 - val_loss: 1.3787 - val_mse: 3.1768\n",
            "Epoch 71/100\n",
            "380/380 - 1s - loss: 1.3410 - mse: 3.0058 - val_loss: 1.3699 - val_mse: 3.1458\n",
            "Epoch 72/100\n",
            "380/380 - 1s - loss: 1.3314 - mse: 2.9764 - val_loss: 1.3612 - val_mse: 3.1153\n",
            "Epoch 73/100\n",
            "380/380 - 1s - loss: 1.3203 - mse: 2.9473 - val_loss: 1.3527 - val_mse: 3.0852\n",
            "Epoch 74/100\n",
            "380/380 - 1s - loss: 1.3122 - mse: 2.9188 - val_loss: 1.3443 - val_mse: 3.0555\n",
            "Epoch 75/100\n",
            "380/380 - 1s - loss: 1.3058 - mse: 2.8907 - val_loss: 1.3359 - val_mse: 3.0262\n",
            "Epoch 76/100\n",
            "380/380 - 1s - loss: 1.2962 - mse: 2.8629 - val_loss: 1.3276 - val_mse: 2.9973\n",
            "Epoch 77/100\n",
            "380/380 - 1s - loss: 1.2888 - mse: 2.8356 - val_loss: 1.3195 - val_mse: 2.9688\n",
            "Epoch 78/100\n",
            "380/380 - 1s - loss: 1.2825 - mse: 2.8086 - val_loss: 1.3115 - val_mse: 2.9407\n",
            "Epoch 79/100\n",
            "380/380 - 1s - loss: 1.2714 - mse: 2.7821 - val_loss: 1.3036 - val_mse: 2.9130\n",
            "Epoch 80/100\n",
            "380/380 - 1s - loss: 1.2646 - mse: 2.7558 - val_loss: 1.2958 - val_mse: 2.8857\n",
            "Epoch 81/100\n",
            "380/380 - 1s - loss: 1.2590 - mse: 2.7299 - val_loss: 1.2880 - val_mse: 2.8588\n",
            "Epoch 82/100\n",
            "380/380 - 1s - loss: 1.2504 - mse: 2.7047 - val_loss: 1.2804 - val_mse: 2.8324\n",
            "Epoch 83/100\n",
            "380/380 - 1s - loss: 1.2418 - mse: 2.6797 - val_loss: 1.2729 - val_mse: 2.8064\n",
            "Epoch 84/100\n",
            "380/380 - 1s - loss: 1.2343 - mse: 2.6549 - val_loss: 1.2655 - val_mse: 2.7807\n",
            "Epoch 85/100\n",
            "380/380 - 1s - loss: 1.2297 - mse: 2.6308 - val_loss: 1.2582 - val_mse: 2.7555\n",
            "Epoch 86/100\n",
            "380/380 - 1s - loss: 1.2201 - mse: 2.6070 - val_loss: 1.2510 - val_mse: 2.7305\n",
            "Epoch 87/100\n",
            "380/380 - 1s - loss: 1.2142 - mse: 2.5837 - val_loss: 1.2440 - val_mse: 2.7060\n",
            "Epoch 88/100\n",
            "380/380 - 1s - loss: 1.2091 - mse: 2.5602 - val_loss: 1.2370 - val_mse: 2.6818\n",
            "Epoch 89/100\n",
            "380/380 - 1s - loss: 1.2014 - mse: 2.5375 - val_loss: 1.2301 - val_mse: 2.6578\n",
            "Epoch 90/100\n",
            "380/380 - 1s - loss: 1.1955 - mse: 2.5149 - val_loss: 1.2234 - val_mse: 2.6342\n",
            "Epoch 91/100\n",
            "380/380 - 1s - loss: 1.1868 - mse: 2.4927 - val_loss: 1.2167 - val_mse: 2.6110\n",
            "Epoch 92/100\n",
            "380/380 - 1s - loss: 1.1816 - mse: 2.4706 - val_loss: 1.2102 - val_mse: 2.5882\n",
            "Epoch 93/100\n",
            "380/380 - 1s - loss: 1.1750 - mse: 2.4494 - val_loss: 1.2038 - val_mse: 2.5658\n",
            "Epoch 94/100\n",
            "380/380 - 1s - loss: 1.1684 - mse: 2.4282 - val_loss: 1.1975 - val_mse: 2.5437\n",
            "Epoch 95/100\n",
            "380/380 - 1s - loss: 1.1628 - mse: 2.4077 - val_loss: 1.1914 - val_mse: 2.5218\n",
            "Epoch 96/100\n",
            "380/380 - 1s - loss: 1.1567 - mse: 2.3871 - val_loss: 1.1852 - val_mse: 2.5002\n",
            "Epoch 97/100\n",
            "380/380 - 1s - loss: 1.1536 - mse: 2.3667 - val_loss: 1.1792 - val_mse: 2.4790\n",
            "Epoch 98/100\n",
            "380/380 - 1s - loss: 1.1462 - mse: 2.3467 - val_loss: 1.1733 - val_mse: 2.4580\n",
            "Epoch 99/100\n",
            "380/380 - 1s - loss: 1.1411 - mse: 2.3271 - val_loss: 1.1675 - val_mse: 2.4374\n",
            "Epoch 100/100\n",
            "380/380 - 1s - loss: 1.1338 - mse: 2.3078 - val_loss: 1.1618 - val_mse: 2.4171\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 48%|████▊     | 10/21 [11:36<12:44, 69.50s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 32)                288       \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "Output (Dense)               (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 321\n",
            "Trainable params: 321\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/100\n",
            "380/380 - 1s - loss: 2.1440 - mse: 6.1770 - val_loss: 2.1805 - val_mse: 6.4586\n",
            "Epoch 2/100\n",
            "380/380 - 1s - loss: 2.1385 - mse: 6.1545 - val_loss: 2.1750 - val_mse: 6.4362\n",
            "Epoch 3/100\n",
            "380/380 - 1s - loss: 2.1327 - mse: 6.1322 - val_loss: 2.1695 - val_mse: 6.4138\n",
            "Epoch 4/100\n",
            "380/380 - 1s - loss: 2.1259 - mse: 6.1102 - val_loss: 2.1640 - val_mse: 6.3916\n",
            "Epoch 5/100\n",
            "380/380 - 1s - loss: 2.1226 - mse: 6.0880 - val_loss: 2.1585 - val_mse: 6.3694\n",
            "Epoch 6/100\n",
            "380/380 - 1s - loss: 2.1171 - mse: 6.0661 - val_loss: 2.1531 - val_mse: 6.3474\n",
            "Epoch 7/100\n",
            "380/380 - 1s - loss: 2.1104 - mse: 6.0442 - val_loss: 2.1476 - val_mse: 6.3254\n",
            "Epoch 8/100\n",
            "380/380 - 1s - loss: 2.1061 - mse: 6.0224 - val_loss: 2.1422 - val_mse: 6.3036\n",
            "Epoch 9/100\n",
            "380/380 - 1s - loss: 2.1001 - mse: 6.0007 - val_loss: 2.1367 - val_mse: 6.2818\n",
            "Epoch 10/100\n",
            "380/380 - 1s - loss: 2.0955 - mse: 5.9791 - val_loss: 2.1313 - val_mse: 6.2601\n",
            "Epoch 11/100\n",
            "380/380 - 1s - loss: 2.0910 - mse: 5.9575 - val_loss: 2.1259 - val_mse: 6.2386\n",
            "Epoch 12/100\n",
            "380/380 - 1s - loss: 2.0855 - mse: 5.9362 - val_loss: 2.1205 - val_mse: 6.2171\n",
            "Epoch 13/100\n",
            "380/380 - 1s - loss: 2.0782 - mse: 5.9148 - val_loss: 2.1151 - val_mse: 6.1957\n",
            "Epoch 14/100\n",
            "380/380 - 1s - loss: 2.0754 - mse: 5.8935 - val_loss: 2.1098 - val_mse: 6.1743\n",
            "Epoch 15/100\n",
            "380/380 - 1s - loss: 2.0662 - mse: 5.8722 - val_loss: 2.1044 - val_mse: 6.1530\n",
            "Epoch 16/100\n",
            "380/380 - 1s - loss: 2.0624 - mse: 5.8512 - val_loss: 2.0991 - val_mse: 6.1318\n",
            "Epoch 17/100\n",
            "380/380 - 1s - loss: 2.0568 - mse: 5.8301 - val_loss: 2.0937 - val_mse: 6.1107\n",
            "Epoch 18/100\n",
            "380/380 - 1s - loss: 2.0514 - mse: 5.8090 - val_loss: 2.0884 - val_mse: 6.0897\n",
            "Epoch 19/100\n",
            "380/380 - 1s - loss: 2.0467 - mse: 5.7881 - val_loss: 2.0831 - val_mse: 6.0687\n",
            "Epoch 20/100\n",
            "380/380 - 1s - loss: 2.0396 - mse: 5.7673 - val_loss: 2.0778 - val_mse: 6.0478\n",
            "Epoch 21/100\n",
            "380/380 - 1s - loss: 2.0343 - mse: 5.7466 - val_loss: 2.0724 - val_mse: 6.0269\n",
            "Epoch 22/100\n",
            "380/380 - 1s - loss: 2.0302 - mse: 5.7258 - val_loss: 2.0671 - val_mse: 6.0062\n",
            "Epoch 23/100\n",
            "380/380 - 1s - loss: 2.0256 - mse: 5.7053 - val_loss: 2.0619 - val_mse: 5.9855\n",
            "Epoch 24/100\n",
            "380/380 - 1s - loss: 2.0205 - mse: 5.6847 - val_loss: 2.0566 - val_mse: 5.9649\n",
            "Epoch 25/100\n",
            "380/380 - 1s - loss: 2.0149 - mse: 5.6643 - val_loss: 2.0513 - val_mse: 5.9444\n",
            "Epoch 26/100\n",
            "380/380 - 1s - loss: 2.0087 - mse: 5.6437 - val_loss: 2.0460 - val_mse: 5.9239\n",
            "Epoch 27/100\n",
            "380/380 - 1s - loss: 2.0065 - mse: 5.6235 - val_loss: 2.0408 - val_mse: 5.9035\n",
            "Epoch 28/100\n",
            "380/380 - 1s - loss: 1.9979 - mse: 5.6031 - val_loss: 2.0355 - val_mse: 5.8832\n",
            "Epoch 29/100\n",
            "380/380 - 1s - loss: 1.9922 - mse: 5.5831 - val_loss: 2.0303 - val_mse: 5.8630\n",
            "Epoch 30/100\n",
            "380/380 - 1s - loss: 1.9898 - mse: 5.5629 - val_loss: 2.0250 - val_mse: 5.8429\n",
            "Epoch 31/100\n",
            "380/380 - 1s - loss: 1.9836 - mse: 5.5429 - val_loss: 2.0198 - val_mse: 5.8228\n",
            "Epoch 32/100\n",
            "380/380 - 1s - loss: 1.9790 - mse: 5.5230 - val_loss: 2.0146 - val_mse: 5.8028\n",
            "Epoch 33/100\n",
            "380/380 - 1s - loss: 1.9717 - mse: 5.5031 - val_loss: 2.0094 - val_mse: 5.7829\n",
            "Epoch 34/100\n",
            "380/380 - 1s - loss: 1.9668 - mse: 5.4833 - val_loss: 2.0041 - val_mse: 5.7630\n",
            "Epoch 35/100\n",
            "380/380 - 1s - loss: 1.9627 - mse: 5.4635 - val_loss: 1.9989 - val_mse: 5.7433\n",
            "Epoch 36/100\n",
            "380/380 - 1s - loss: 1.9569 - mse: 5.4440 - val_loss: 1.9938 - val_mse: 5.7236\n",
            "Epoch 37/100\n",
            "380/380 - 1s - loss: 1.9527 - mse: 5.4244 - val_loss: 1.9886 - val_mse: 5.7040\n",
            "Epoch 38/100\n",
            "380/380 - 1s - loss: 1.9488 - mse: 5.4049 - val_loss: 1.9834 - val_mse: 5.6845\n",
            "Epoch 39/100\n",
            "380/380 - 1s - loss: 1.9430 - mse: 5.3855 - val_loss: 1.9782 - val_mse: 5.6650\n",
            "Epoch 40/100\n",
            "380/380 - 1s - loss: 1.9343 - mse: 5.3660 - val_loss: 1.9731 - val_mse: 5.6457\n",
            "Epoch 41/100\n",
            "380/380 - 1s - loss: 1.9300 - mse: 5.3469 - val_loss: 1.9680 - val_mse: 5.6264\n",
            "Epoch 42/100\n",
            "380/380 - 1s - loss: 1.9278 - mse: 5.3278 - val_loss: 1.9629 - val_mse: 5.6072\n",
            "Epoch 43/100\n",
            "380/380 - 1s - loss: 1.9218 - mse: 5.3086 - val_loss: 1.9578 - val_mse: 5.5882\n",
            "Epoch 44/100\n",
            "380/380 - 1s - loss: 1.9151 - mse: 5.2896 - val_loss: 1.9527 - val_mse: 5.5691\n",
            "Epoch 45/100\n",
            "380/380 - 1s - loss: 1.9120 - mse: 5.2706 - val_loss: 1.9476 - val_mse: 5.5501\n",
            "Epoch 46/100\n",
            "380/380 - 1s - loss: 1.9050 - mse: 5.2519 - val_loss: 1.9425 - val_mse: 5.5312\n",
            "Epoch 47/100\n",
            "380/380 - 1s - loss: 1.8998 - mse: 5.2328 - val_loss: 1.9374 - val_mse: 5.5123\n",
            "Epoch 48/100\n",
            "380/380 - 1s - loss: 1.8968 - mse: 5.2141 - val_loss: 1.9323 - val_mse: 5.4936\n",
            "Epoch 49/100\n",
            "380/380 - 1s - loss: 1.8926 - mse: 5.1955 - val_loss: 1.9273 - val_mse: 5.4748\n",
            "Epoch 50/100\n",
            "380/380 - 1s - loss: 1.8861 - mse: 5.1768 - val_loss: 1.9222 - val_mse: 5.4562\n",
            "Epoch 51/100\n",
            "380/380 - 1s - loss: 1.8814 - mse: 5.1583 - val_loss: 1.9172 - val_mse: 5.4376\n",
            "Epoch 52/100\n",
            "380/380 - 1s - loss: 1.8759 - mse: 5.1397 - val_loss: 1.9122 - val_mse: 5.4191\n",
            "Epoch 53/100\n",
            "380/380 - 1s - loss: 1.8691 - mse: 5.1213 - val_loss: 1.9073 - val_mse: 5.4007\n",
            "Epoch 54/100\n",
            "380/380 - 1s - loss: 1.8670 - mse: 5.1032 - val_loss: 1.9024 - val_mse: 5.3824\n",
            "Epoch 55/100\n",
            "380/380 - 1s - loss: 1.8618 - mse: 5.0849 - val_loss: 1.8975 - val_mse: 5.3642\n",
            "Epoch 56/100\n",
            "380/380 - 1s - loss: 1.8571 - mse: 5.0668 - val_loss: 1.8926 - val_mse: 5.3461\n",
            "Epoch 57/100\n",
            "380/380 - 1s - loss: 1.8504 - mse: 5.0487 - val_loss: 1.8877 - val_mse: 5.3281\n",
            "Epoch 58/100\n",
            "380/380 - 1s - loss: 1.8463 - mse: 5.0308 - val_loss: 1.8829 - val_mse: 5.3101\n",
            "Epoch 59/100\n",
            "380/380 - 1s - loss: 1.8426 - mse: 5.0130 - val_loss: 1.8780 - val_mse: 5.2922\n",
            "Epoch 60/100\n",
            "380/380 - 1s - loss: 1.8366 - mse: 4.9952 - val_loss: 1.8732 - val_mse: 5.2743\n",
            "Epoch 61/100\n",
            "380/380 - 1s - loss: 1.8301 - mse: 4.9774 - val_loss: 1.8684 - val_mse: 5.2566\n",
            "Epoch 62/100\n",
            "380/380 - 1s - loss: 1.8285 - mse: 4.9596 - val_loss: 1.8636 - val_mse: 5.2389\n",
            "Epoch 63/100\n",
            "380/380 - 1s - loss: 1.8217 - mse: 4.9421 - val_loss: 1.8588 - val_mse: 5.2213\n",
            "Epoch 64/100\n",
            "380/380 - 1s - loss: 1.8210 - mse: 4.9245 - val_loss: 1.8541 - val_mse: 5.2037\n",
            "Epoch 65/100\n",
            "380/380 - 1s - loss: 1.8134 - mse: 4.9070 - val_loss: 1.8493 - val_mse: 5.1862\n",
            "Epoch 66/100\n",
            "380/380 - 1s - loss: 1.8068 - mse: 4.8896 - val_loss: 1.8446 - val_mse: 5.1688\n",
            "Epoch 67/100\n",
            "380/380 - 1s - loss: 1.8046 - mse: 4.8724 - val_loss: 1.8399 - val_mse: 5.1515\n",
            "Epoch 68/100\n",
            "380/380 - 1s - loss: 1.7984 - mse: 4.8550 - val_loss: 1.8352 - val_mse: 5.1342\n",
            "Epoch 69/100\n",
            "380/380 - 1s - loss: 1.7952 - mse: 4.8379 - val_loss: 1.8305 - val_mse: 5.1170\n",
            "Epoch 70/100\n",
            "380/380 - 1s - loss: 1.7904 - mse: 4.8207 - val_loss: 1.8258 - val_mse: 5.0999\n",
            "Epoch 71/100\n",
            "380/380 - 1s - loss: 1.7876 - mse: 4.8036 - val_loss: 1.8212 - val_mse: 5.0828\n",
            "Epoch 72/100\n",
            "380/380 - 1s - loss: 1.7823 - mse: 4.7867 - val_loss: 1.8166 - val_mse: 5.0659\n",
            "Epoch 73/100\n",
            "380/380 - 1s - loss: 1.7745 - mse: 4.7699 - val_loss: 1.8120 - val_mse: 5.0490\n",
            "Epoch 74/100\n",
            "380/380 - 1s - loss: 1.7701 - mse: 4.7532 - val_loss: 1.8074 - val_mse: 5.0323\n",
            "Epoch 75/100\n",
            "380/380 - 1s - loss: 1.7682 - mse: 4.7364 - val_loss: 1.8029 - val_mse: 5.0156\n",
            "Epoch 76/100\n",
            "380/380 - 1s - loss: 1.7610 - mse: 4.7198 - val_loss: 1.7984 - val_mse: 4.9989\n",
            "Epoch 77/100\n",
            "380/380 - 1s - loss: 1.7580 - mse: 4.7032 - val_loss: 1.7939 - val_mse: 4.9824\n",
            "Epoch 78/100\n",
            "380/380 - 1s - loss: 1.7555 - mse: 4.6868 - val_loss: 1.7894 - val_mse: 4.9659\n",
            "Epoch 79/100\n",
            "380/380 - 1s - loss: 1.7463 - mse: 4.6703 - val_loss: 1.7849 - val_mse: 4.9495\n",
            "Epoch 80/100\n",
            "380/380 - 1s - loss: 1.7433 - mse: 4.6539 - val_loss: 1.7805 - val_mse: 4.9331\n",
            "Epoch 81/100\n",
            "380/380 - 1s - loss: 1.7409 - mse: 4.6375 - val_loss: 1.7760 - val_mse: 4.9169\n",
            "Epoch 82/100\n",
            "380/380 - 1s - loss: 1.7357 - mse: 4.6214 - val_loss: 1.7716 - val_mse: 4.9007\n",
            "Epoch 83/100\n",
            "380/380 - 1s - loss: 1.7296 - mse: 4.6053 - val_loss: 1.7672 - val_mse: 4.8845\n",
            "Epoch 84/100\n",
            "380/380 - 1s - loss: 1.7243 - mse: 4.5892 - val_loss: 1.7628 - val_mse: 4.8684\n",
            "Epoch 85/100\n",
            "380/380 - 1s - loss: 1.7233 - mse: 4.5731 - val_loss: 1.7585 - val_mse: 4.8524\n",
            "Epoch 86/100\n",
            "380/380 - 1s - loss: 1.7163 - mse: 4.5571 - val_loss: 1.7541 - val_mse: 4.8365\n",
            "Epoch 87/100\n",
            "380/380 - 1s - loss: 1.7125 - mse: 4.5414 - val_loss: 1.7498 - val_mse: 4.8206\n",
            "Epoch 88/100\n",
            "380/380 - 1s - loss: 1.7100 - mse: 4.5255 - val_loss: 1.7455 - val_mse: 4.8048\n",
            "Epoch 89/100\n",
            "380/380 - 1s - loss: 1.7044 - mse: 4.5097 - val_loss: 1.7412 - val_mse: 4.7891\n",
            "Epoch 90/100\n",
            "380/380 - 1s - loss: 1.7010 - mse: 4.4940 - val_loss: 1.7369 - val_mse: 4.7734\n",
            "Epoch 91/100\n",
            "380/380 - 1s - loss: 1.6942 - mse: 4.4784 - val_loss: 1.7327 - val_mse: 4.7578\n",
            "Epoch 92/100\n",
            "380/380 - 1s - loss: 1.6909 - mse: 4.4629 - val_loss: 1.7285 - val_mse: 4.7422\n",
            "Epoch 93/100\n",
            "380/380 - 1s - loss: 1.6864 - mse: 4.4473 - val_loss: 1.7243 - val_mse: 4.7267\n",
            "Epoch 94/100\n",
            "380/380 - 1s - loss: 1.6810 - mse: 4.4318 - val_loss: 1.7201 - val_mse: 4.7113\n",
            "Epoch 95/100\n",
            "380/380 - 1s - loss: 1.6773 - mse: 4.4164 - val_loss: 1.7158 - val_mse: 4.6959\n",
            "Epoch 96/100\n",
            "380/380 - 1s - loss: 1.6736 - mse: 4.4011 - val_loss: 1.7117 - val_mse: 4.6806\n",
            "Epoch 97/100\n",
            "380/380 - 1s - loss: 1.6731 - mse: 4.3857 - val_loss: 1.7075 - val_mse: 4.6654\n",
            "Epoch 98/100\n",
            "380/380 - 1s - loss: 1.6658 - mse: 4.3707 - val_loss: 1.7034 - val_mse: 4.6502\n",
            "Epoch 99/100\n",
            "380/380 - 1s - loss: 1.6625 - mse: 4.3554 - val_loss: 1.6993 - val_mse: 4.6351\n",
            "Epoch 100/100\n",
            "380/380 - 1s - loss: 1.6564 - mse: 4.3404 - val_loss: 1.6952 - val_mse: 4.6200\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 52%|█████▏    | 11/21 [12:41<11:19, 67.95s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 32)                288       \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "Output (Dense)               (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 321\n",
            "Trainable params: 321\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/100\n",
            "380/380 - 1s - loss: 6.5005 - mse: 6.4951 - val_loss: 6.4415 - val_mse: 6.4445\n",
            "Epoch 2/100\n",
            "380/380 - 1s - loss: 6.1927 - mse: 6.1972 - val_loss: 6.1494 - val_mse: 6.1523\n",
            "Epoch 3/100\n",
            "380/380 - 1s - loss: 5.9127 - mse: 5.9173 - val_loss: 5.8743 - val_mse: 5.8773\n",
            "Epoch 4/100\n",
            "380/380 - 1s - loss: 5.6474 - mse: 5.6539 - val_loss: 5.6154 - val_mse: 5.6184\n",
            "Epoch 5/100\n",
            "380/380 - 1s - loss: 5.4036 - mse: 5.4059 - val_loss: 5.3712 - val_mse: 5.3741\n",
            "Epoch 6/100\n",
            "380/380 - 1s - loss: 5.1697 - mse: 5.1721 - val_loss: 5.1408 - val_mse: 5.1437\n",
            "Epoch 7/100\n",
            "380/380 - 1s - loss: 4.9475 - mse: 4.9516 - val_loss: 4.9234 - val_mse: 4.9263\n",
            "Epoch 8/100\n",
            "380/380 - 1s - loss: 4.7423 - mse: 4.7435 - val_loss: 4.7180 - val_mse: 4.7209\n",
            "Epoch 9/100\n",
            "380/380 - 1s - loss: 4.5425 - mse: 4.5470 - val_loss: 4.5240 - val_mse: 4.5269\n",
            "Epoch 10/100\n",
            "380/380 - 1s - loss: 4.3634 - mse: 4.3614 - val_loss: 4.3404 - val_mse: 4.3432\n",
            "Epoch 11/100\n",
            "380/380 - 1s - loss: 4.1939 - mse: 4.1858 - val_loss: 4.1667 - val_mse: 4.1694\n",
            "Epoch 12/100\n",
            "380/380 - 1s - loss: 4.0216 - mse: 4.0197 - val_loss: 4.0023 - val_mse: 4.0050\n",
            "Epoch 13/100\n",
            "380/380 - 1s - loss: 3.8596 - mse: 3.8626 - val_loss: 3.8468 - val_mse: 3.8495\n",
            "Epoch 14/100\n",
            "380/380 - 1s - loss: 3.7204 - mse: 3.7140 - val_loss: 3.6994 - val_mse: 3.7020\n",
            "Epoch 15/100\n",
            "380/380 - 1s - loss: 3.5683 - mse: 3.5732 - val_loss: 3.5599 - val_mse: 3.5625\n",
            "Epoch 16/100\n",
            "380/380 - 1s - loss: 3.4373 - mse: 3.4400 - val_loss: 3.4277 - val_mse: 3.4303\n",
            "Epoch 17/100\n",
            "380/380 - 1s - loss: 3.3126 - mse: 3.3138 - val_loss: 3.3024 - val_mse: 3.3049\n",
            "Epoch 18/100\n",
            "380/380 - 1s - loss: 3.1914 - mse: 3.1940 - val_loss: 3.1834 - val_mse: 3.1859\n",
            "Epoch 19/100\n",
            "380/380 - 1s - loss: 3.0781 - mse: 3.0805 - val_loss: 3.0706 - val_mse: 3.0730\n",
            "Epoch 20/100\n",
            "380/380 - 1s - loss: 2.9690 - mse: 2.9727 - val_loss: 2.9635 - val_mse: 2.9659\n",
            "Epoch 21/100\n",
            "380/380 - 1s - loss: 2.8682 - mse: 2.8705 - val_loss: 2.8618 - val_mse: 2.8641\n",
            "Epoch 22/100\n",
            "380/380 - 1s - loss: 2.7705 - mse: 2.7734 - val_loss: 2.7652 - val_mse: 2.7675\n",
            "Epoch 23/100\n",
            "380/380 - 1s - loss: 2.6843 - mse: 2.6813 - val_loss: 2.6734 - val_mse: 2.6757\n",
            "Epoch 24/100\n",
            "380/380 - 1s - loss: 2.5957 - mse: 2.5936 - val_loss: 2.5862 - val_mse: 2.5884\n",
            "Epoch 25/100\n",
            "380/380 - 1s - loss: 2.5077 - mse: 2.5104 - val_loss: 2.5033 - val_mse: 2.5055\n",
            "Epoch 26/100\n",
            "380/380 - 1s - loss: 2.4294 - mse: 2.4313 - val_loss: 2.4244 - val_mse: 2.4266\n",
            "Epoch 27/100\n",
            "380/380 - 1s - loss: 2.3564 - mse: 2.3561 - val_loss: 2.3494 - val_mse: 2.3515\n",
            "Epoch 28/100\n",
            "380/380 - 1s - loss: 2.2876 - mse: 2.2845 - val_loss: 2.2780 - val_mse: 2.2801\n",
            "Epoch 29/100\n",
            "380/380 - 1s - loss: 2.2139 - mse: 2.2164 - val_loss: 2.2102 - val_mse: 2.2122\n",
            "Epoch 30/100\n",
            "380/380 - 1s - loss: 2.1549 - mse: 2.1516 - val_loss: 2.1455 - val_mse: 2.1475\n",
            "Epoch 31/100\n",
            "380/380 - 1s - loss: 2.0913 - mse: 2.0899 - val_loss: 2.0839 - val_mse: 2.0859\n",
            "Epoch 32/100\n",
            "380/380 - 1s - loss: 2.0352 - mse: 2.0311 - val_loss: 2.0253 - val_mse: 2.0272\n",
            "Epoch 33/100\n",
            "380/380 - 1s - loss: 1.9724 - mse: 1.9752 - val_loss: 1.9695 - val_mse: 1.9714\n",
            "Epoch 34/100\n",
            "380/380 - 1s - loss: 1.9222 - mse: 1.9219 - val_loss: 1.9164 - val_mse: 1.9183\n",
            "Epoch 35/100\n",
            "380/380 - 1s - loss: 1.8706 - mse: 1.8711 - val_loss: 1.8658 - val_mse: 1.8676\n",
            "Epoch 36/100\n",
            "380/380 - 1s - loss: 1.8210 - mse: 1.8227 - val_loss: 1.8175 - val_mse: 1.8193\n",
            "Epoch 37/100\n",
            "380/380 - 1s - loss: 1.7748 - mse: 1.7766 - val_loss: 1.7715 - val_mse: 1.7733\n",
            "Epoch 38/100\n",
            "380/380 - 1s - loss: 1.7367 - mse: 1.7327 - val_loss: 1.7276 - val_mse: 1.7293\n",
            "Epoch 39/100\n",
            "380/380 - 1s - loss: 1.6955 - mse: 1.6906 - val_loss: 1.6858 - val_mse: 1.6874\n",
            "Epoch 40/100\n",
            "380/380 - 1s - loss: 1.6477 - mse: 1.6506 - val_loss: 1.6459 - val_mse: 1.6476\n",
            "Epoch 41/100\n",
            "380/380 - 1s - loss: 1.6102 - mse: 1.6125 - val_loss: 1.6079 - val_mse: 1.6095\n",
            "Epoch 42/100\n",
            "380/380 - 1s - loss: 1.5796 - mse: 1.5761 - val_loss: 1.5716 - val_mse: 1.5731\n",
            "Epoch 43/100\n",
            "380/380 - 1s - loss: 1.5441 - mse: 1.5412 - val_loss: 1.5369 - val_mse: 1.5384\n",
            "Epoch 44/100\n",
            "380/380 - 1s - loss: 1.5059 - mse: 1.5079 - val_loss: 1.5038 - val_mse: 1.5053\n",
            "Epoch 45/100\n",
            "380/380 - 1s - loss: 1.4793 - mse: 1.4762 - val_loss: 1.4722 - val_mse: 1.4737\n",
            "Epoch 46/100\n",
            "380/380 - 1s - loss: 1.4447 - mse: 1.4459 - val_loss: 1.4421 - val_mse: 1.4435\n",
            "Epoch 47/100\n",
            "380/380 - 1s - loss: 1.4149 - mse: 1.4169 - val_loss: 1.4133 - val_mse: 1.4147\n",
            "Epoch 48/100\n",
            "380/380 - 1s - loss: 1.3885 - mse: 1.3892 - val_loss: 1.3858 - val_mse: 1.3872\n",
            "Epoch 49/100\n",
            "380/380 - 1s - loss: 1.3643 - mse: 1.3627 - val_loss: 1.3595 - val_mse: 1.3608\n",
            "Epoch 50/100\n",
            "380/380 - 1s - loss: 1.3386 - mse: 1.3373 - val_loss: 1.3344 - val_mse: 1.3357\n",
            "Epoch 51/100\n",
            "380/380 - 1s - loss: 1.3118 - mse: 1.3131 - val_loss: 1.3104 - val_mse: 1.3117\n",
            "Epoch 52/100\n",
            "380/380 - 1s - loss: 1.2880 - mse: 1.2900 - val_loss: 1.2875 - val_mse: 1.2888\n",
            "Epoch 53/100\n",
            "380/380 - 1s - loss: 1.2655 - mse: 1.2678 - val_loss: 1.2656 - val_mse: 1.2669\n",
            "Epoch 54/100\n",
            "380/380 - 1s - loss: 1.2462 - mse: 1.2467 - val_loss: 1.2446 - val_mse: 1.2459\n",
            "Epoch 55/100\n",
            "380/380 - 1s - loss: 1.2256 - mse: 1.2264 - val_loss: 1.2246 - val_mse: 1.2258\n",
            "Epoch 56/100\n",
            "380/380 - 1s - loss: 1.2054 - mse: 1.2069 - val_loss: 1.2054 - val_mse: 1.2066\n",
            "Epoch 57/100\n",
            "380/380 - 1s - loss: 1.1868 - mse: 1.1883 - val_loss: 1.1871 - val_mse: 1.1883\n",
            "Epoch 58/100\n",
            "380/380 - 1s - loss: 1.1693 - mse: 1.1705 - val_loss: 1.1695 - val_mse: 1.1707\n",
            "Epoch 59/100\n",
            "380/380 - 1s - loss: 1.1560 - mse: 1.1535 - val_loss: 1.1527 - val_mse: 1.1539\n",
            "Epoch 60/100\n",
            "380/380 - 1s - loss: 1.1353 - mse: 1.1371 - val_loss: 1.1367 - val_mse: 1.1378\n",
            "Epoch 61/100\n",
            "380/380 - 1s - loss: 1.1197 - mse: 1.1215 - val_loss: 1.1213 - val_mse: 1.1224\n",
            "Epoch 62/100\n",
            "380/380 - 1s - loss: 1.1060 - mse: 1.1065 - val_loss: 1.1066 - val_mse: 1.1076\n",
            "Epoch 63/100\n",
            "380/380 - 1s - loss: 1.0905 - mse: 1.0921 - val_loss: 1.0925 - val_mse: 1.0935\n",
            "Epoch 64/100\n",
            "380/380 - 1s - loss: 1.0778 - mse: 1.0784 - val_loss: 1.0789 - val_mse: 1.0800\n",
            "Epoch 65/100\n",
            "380/380 - 1s - loss: 1.0649 - mse: 1.0651 - val_loss: 1.0660 - val_mse: 1.0671\n",
            "Epoch 66/100\n",
            "380/380 - 1s - loss: 1.0514 - mse: 1.0525 - val_loss: 1.0536 - val_mse: 1.0547\n",
            "Epoch 67/100\n",
            "380/380 - 1s - loss: 1.0414 - mse: 1.0403 - val_loss: 1.0417 - val_mse: 1.0427\n",
            "Epoch 68/100\n",
            "380/380 - 1s - loss: 1.0271 - mse: 1.0286 - val_loss: 1.0303 - val_mse: 1.0313\n",
            "Epoch 69/100\n",
            "380/380 - 1s - loss: 1.0177 - mse: 1.0175 - val_loss: 1.0194 - val_mse: 1.0204\n",
            "Epoch 70/100\n",
            "380/380 - 1s - loss: 1.0054 - mse: 1.0067 - val_loss: 1.0089 - val_mse: 1.0099\n",
            "Epoch 71/100\n",
            "380/380 - 1s - loss: 0.9998 - mse: 0.9964 - val_loss: 0.9988 - val_mse: 0.9998\n",
            "Epoch 72/100\n",
            "380/380 - 1s - loss: 0.9862 - mse: 0.9865 - val_loss: 0.9892 - val_mse: 0.9901\n",
            "Epoch 73/100\n",
            "380/380 - 1s - loss: 0.9755 - mse: 0.9770 - val_loss: 0.9800 - val_mse: 0.9809\n",
            "Epoch 74/100\n",
            "380/380 - 1s - loss: 0.9667 - mse: 0.9679 - val_loss: 0.9711 - val_mse: 0.9721\n",
            "Epoch 75/100\n",
            "380/380 - 1s - loss: 0.9609 - mse: 0.9591 - val_loss: 0.9626 - val_mse: 0.9635\n",
            "Epoch 76/100\n",
            "380/380 - 1s - loss: 0.9502 - mse: 0.9507 - val_loss: 0.9545 - val_mse: 0.9554\n",
            "Epoch 77/100\n",
            "380/380 - 1s - loss: 0.9423 - mse: 0.9426 - val_loss: 0.9466 - val_mse: 0.9476\n",
            "Epoch 78/100\n",
            "380/380 - 1s - loss: 0.9374 - mse: 0.9348 - val_loss: 0.9391 - val_mse: 0.9400\n",
            "Epoch 79/100\n",
            "380/380 - 1s - loss: 0.9266 - mse: 0.9274 - val_loss: 0.9319 - val_mse: 0.9328\n",
            "Epoch 80/100\n",
            "380/380 - 1s - loss: 0.9187 - mse: 0.9202 - val_loss: 0.9250 - val_mse: 0.9259\n",
            "Epoch 81/100\n",
            "380/380 - 1s - loss: 0.9128 - mse: 0.9133 - val_loss: 0.9184 - val_mse: 0.9193\n",
            "Epoch 82/100\n",
            "380/380 - 1s - loss: 0.9058 - mse: 0.9067 - val_loss: 0.9120 - val_mse: 0.9129\n",
            "Epoch 83/100\n",
            "380/380 - 1s - loss: 0.8987 - mse: 0.9003 - val_loss: 0.9058 - val_mse: 0.9067\n",
            "Epoch 84/100\n",
            "380/380 - 1s - loss: 0.8928 - mse: 0.8942 - val_loss: 0.9000 - val_mse: 0.9008\n",
            "Epoch 85/100\n",
            "380/380 - 1s - loss: 0.8921 - mse: 0.8883 - val_loss: 0.8943 - val_mse: 0.8951\n",
            "Epoch 86/100\n",
            "380/380 - 1s - loss: 0.8815 - mse: 0.8826 - val_loss: 0.8888 - val_mse: 0.8897\n",
            "Epoch 87/100\n",
            "380/380 - 1s - loss: 0.8763 - mse: 0.8771 - val_loss: 0.8836 - val_mse: 0.8845\n",
            "Epoch 88/100\n",
            "380/380 - 1s - loss: 0.8739 - mse: 0.8719 - val_loss: 0.8785 - val_mse: 0.8794\n",
            "Epoch 89/100\n",
            "380/380 - 1s - loss: 0.8661 - mse: 0.8668 - val_loss: 0.8737 - val_mse: 0.8746\n",
            "Epoch 90/100\n",
            "380/380 - 1s - loss: 0.8607 - mse: 0.8619 - val_loss: 0.8690 - val_mse: 0.8699\n",
            "Epoch 91/100\n",
            "380/380 - 1s - loss: 0.8558 - mse: 0.8572 - val_loss: 0.8646 - val_mse: 0.8654\n",
            "Epoch 92/100\n",
            "380/380 - 1s - loss: 0.8524 - mse: 0.8527 - val_loss: 0.8603 - val_mse: 0.8611\n",
            "Epoch 93/100\n",
            "380/380 - 1s - loss: 0.8474 - mse: 0.8483 - val_loss: 0.8561 - val_mse: 0.8570\n",
            "Epoch 94/100\n",
            "380/380 - 1s - loss: 0.8427 - mse: 0.8441 - val_loss: 0.8521 - val_mse: 0.8530\n",
            "Epoch 95/100\n",
            "380/380 - 1s - loss: 0.8394 - mse: 0.8401 - val_loss: 0.8482 - val_mse: 0.8491\n",
            "Epoch 96/100\n",
            "380/380 - 1s - loss: 0.8353 - mse: 0.8362 - val_loss: 0.8445 - val_mse: 0.8454\n",
            "Epoch 97/100\n",
            "380/380 - 1s - loss: 0.8335 - mse: 0.8324 - val_loss: 0.8409 - val_mse: 0.8418\n",
            "Epoch 98/100\n",
            "380/380 - 1s - loss: 0.8319 - mse: 0.8287 - val_loss: 0.8374 - val_mse: 0.8383\n",
            "Epoch 99/100\n",
            "380/380 - 1s - loss: 0.8240 - mse: 0.8252 - val_loss: 0.8341 - val_mse: 0.8349\n",
            "Epoch 100/100\n",
            "380/380 - 1s - loss: 0.8208 - mse: 0.8218 - val_loss: 0.8309 - val_mse: 0.8317\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 57%|█████▋    | 12/21 [13:44<10:00, 66.70s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 32)                288       \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 32)                1056      \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "Output (Dense)               (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 1,377\n",
            "Trainable params: 1,377\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/100\n",
            "380/380 - 1s - loss: 4.8188 - mse: 4.8172 - val_loss: 4.9032 - val_mse: 4.9066\n",
            "Epoch 2/100\n",
            "380/380 - 1s - loss: 4.7406 - mse: 4.7427 - val_loss: 4.8278 - val_mse: 4.8311\n",
            "Epoch 3/100\n",
            "380/380 - 1s - loss: 4.6662 - mse: 4.6694 - val_loss: 4.7534 - val_mse: 4.7568\n",
            "Epoch 4/100\n",
            "380/380 - 1s - loss: 4.5908 - mse: 4.5972 - val_loss: 4.6801 - val_mse: 4.6835\n",
            "Epoch 5/100\n",
            "380/380 - 1s - loss: 4.5257 - mse: 4.5262 - val_loss: 4.6081 - val_mse: 4.6115\n",
            "Epoch 6/100\n",
            "380/380 - 1s - loss: 4.4570 - mse: 4.4563 - val_loss: 4.5371 - val_mse: 4.5405\n",
            "Epoch 7/100\n",
            "380/380 - 1s - loss: 4.3823 - mse: 4.3875 - val_loss: 4.4672 - val_mse: 4.4706\n",
            "Epoch 8/100\n",
            "380/380 - 1s - loss: 4.3180 - mse: 4.3196 - val_loss: 4.3983 - val_mse: 4.4017\n",
            "Epoch 9/100\n",
            "380/380 - 1s - loss: 4.2487 - mse: 4.2524 - val_loss: 4.3296 - val_mse: 4.3330\n",
            "Epoch 10/100\n",
            "380/380 - 1s - loss: 4.1855 - mse: 4.1859 - val_loss: 4.2622 - val_mse: 4.2656\n",
            "Epoch 11/100\n",
            "380/380 - 1s - loss: 4.1286 - mse: 4.1204 - val_loss: 4.1956 - val_mse: 4.1990\n",
            "Epoch 12/100\n",
            "380/380 - 1s - loss: 4.0586 - mse: 4.0555 - val_loss: 4.1296 - val_mse: 4.1330\n",
            "Epoch 13/100\n",
            "380/380 - 1s - loss: 3.9866 - mse: 3.9914 - val_loss: 4.0648 - val_mse: 4.0682\n",
            "Epoch 14/100\n",
            "380/380 - 1s - loss: 3.9336 - mse: 3.9282 - val_loss: 4.0006 - val_mse: 4.0040\n",
            "Epoch 15/100\n",
            "380/380 - 1s - loss: 3.8596 - mse: 3.8658 - val_loss: 3.9374 - val_mse: 3.9408\n",
            "Epoch 16/100\n",
            "380/380 - 1s - loss: 3.8012 - mse: 3.8042 - val_loss: 3.8750 - val_mse: 3.8783\n",
            "Epoch 17/100\n",
            "380/380 - 1s - loss: 3.7430 - mse: 3.7434 - val_loss: 3.8135 - val_mse: 3.8168\n",
            "Epoch 18/100\n",
            "380/380 - 1s - loss: 3.6800 - mse: 3.6836 - val_loss: 3.7531 - val_mse: 3.7564\n",
            "Epoch 19/100\n",
            "380/380 - 1s - loss: 3.6246 - mse: 3.6248 - val_loss: 3.6937 - val_mse: 3.6970\n",
            "Epoch 20/100\n",
            "380/380 - 1s - loss: 3.5612 - mse: 3.5667 - val_loss: 3.6350 - val_mse: 3.6383\n",
            "Epoch 21/100\n",
            "380/380 - 1s - loss: 3.5043 - mse: 3.5095 - val_loss: 3.5773 - val_mse: 3.5805\n",
            "Epoch 22/100\n",
            "380/380 - 1s - loss: 3.4483 - mse: 3.4530 - val_loss: 3.5203 - val_mse: 3.5235\n",
            "Epoch 23/100\n",
            "380/380 - 1s - loss: 3.4010 - mse: 3.3973 - val_loss: 3.4642 - val_mse: 3.4674\n",
            "Epoch 24/100\n",
            "380/380 - 1s - loss: 3.3443 - mse: 3.3426 - val_loss: 3.4092 - val_mse: 3.4124\n",
            "Epoch 25/100\n",
            "380/380 - 1s - loss: 3.2882 - mse: 3.2887 - val_loss: 3.3547 - val_mse: 3.3579\n",
            "Epoch 26/100\n",
            "380/380 - 1s - loss: 3.2328 - mse: 3.2357 - val_loss: 3.3019 - val_mse: 3.3050\n",
            "Epoch 27/100\n",
            "380/380 - 1s - loss: 3.1932 - mse: 3.1838 - val_loss: 3.2497 - val_mse: 3.2528\n",
            "Epoch 28/100\n",
            "380/380 - 1s - loss: 3.1339 - mse: 3.1327 - val_loss: 3.1987 - val_mse: 3.2018\n",
            "Epoch 29/100\n",
            "380/380 - 1s - loss: 3.0786 - mse: 3.0829 - val_loss: 3.1487 - val_mse: 3.1518\n",
            "Epoch 30/100\n",
            "380/380 - 1s - loss: 3.0387 - mse: 3.0339 - val_loss: 3.0997 - val_mse: 3.1027\n",
            "Epoch 31/100\n",
            "380/380 - 1s - loss: 2.9872 - mse: 2.9856 - val_loss: 3.0514 - val_mse: 3.0545\n",
            "Epoch 32/100\n",
            "380/380 - 1s - loss: 2.9439 - mse: 2.9381 - val_loss: 3.0040 - val_mse: 3.0070\n",
            "Epoch 33/100\n",
            "380/380 - 1s - loss: 2.8869 - mse: 2.8913 - val_loss: 2.9573 - val_mse: 2.9603\n",
            "Epoch 34/100\n",
            "380/380 - 1s - loss: 2.8441 - mse: 2.8456 - val_loss: 2.9121 - val_mse: 2.9151\n",
            "Epoch 35/100\n",
            "380/380 - 1s - loss: 2.7994 - mse: 2.8009 - val_loss: 2.8676 - val_mse: 2.8705\n",
            "Epoch 36/100\n",
            "380/380 - 1s - loss: 2.7538 - mse: 2.7571 - val_loss: 2.8239 - val_mse: 2.8268\n",
            "Epoch 37/100\n",
            "380/380 - 1s - loss: 2.7159 - mse: 2.7141 - val_loss: 2.7811 - val_mse: 2.7839\n",
            "Epoch 38/100\n",
            "380/380 - 1s - loss: 2.6769 - mse: 2.6718 - val_loss: 2.7392 - val_mse: 2.7420\n",
            "Epoch 39/100\n",
            "380/380 - 1s - loss: 2.6371 - mse: 2.6306 - val_loss: 2.6983 - val_mse: 2.7011\n",
            "Epoch 40/100\n",
            "380/380 - 1s - loss: 2.5858 - mse: 2.5905 - val_loss: 2.6586 - val_mse: 2.6613\n",
            "Epoch 41/100\n",
            "380/380 - 1s - loss: 2.5481 - mse: 2.5514 - val_loss: 2.6197 - val_mse: 2.6225\n",
            "Epoch 42/100\n",
            "380/380 - 1s - loss: 2.5143 - mse: 2.5129 - val_loss: 2.5815 - val_mse: 2.5842\n",
            "Epoch 43/100\n",
            "380/380 - 1s - loss: 2.4781 - mse: 2.4750 - val_loss: 2.5442 - val_mse: 2.5469\n",
            "Epoch 44/100\n",
            "380/380 - 1s - loss: 2.4357 - mse: 2.4382 - val_loss: 2.5076 - val_mse: 2.5102\n",
            "Epoch 45/100\n",
            "380/380 - 1s - loss: 2.4045 - mse: 2.4022 - val_loss: 2.4720 - val_mse: 2.4745\n",
            "Epoch 46/100\n",
            "380/380 - 1s - loss: 2.3645 - mse: 2.3668 - val_loss: 2.4369 - val_mse: 2.4394\n",
            "Epoch 47/100\n",
            "380/380 - 1s - loss: 2.3288 - mse: 2.3322 - val_loss: 2.4028 - val_mse: 2.4053\n",
            "Epoch 48/100\n",
            "380/380 - 1s - loss: 2.2979 - mse: 2.2984 - val_loss: 2.3694 - val_mse: 2.3719\n",
            "Epoch 49/100\n",
            "380/380 - 1s - loss: 2.2701 - mse: 2.2653 - val_loss: 2.3367 - val_mse: 2.3391\n",
            "Epoch 50/100\n",
            "380/380 - 1s - loss: 2.2360 - mse: 2.2327 - val_loss: 2.3046 - val_mse: 2.3070\n",
            "Epoch 51/100\n",
            "380/380 - 1s - loss: 2.2001 - mse: 2.2007 - val_loss: 2.2730 - val_mse: 2.2754\n",
            "Epoch 52/100\n",
            "380/380 - 1s - loss: 2.1673 - mse: 2.1694 - val_loss: 2.2421 - val_mse: 2.2445\n",
            "Epoch 53/100\n",
            "380/380 - 1s - loss: 2.1350 - mse: 2.1389 - val_loss: 2.2121 - val_mse: 2.2144\n",
            "Epoch 54/100\n",
            "380/380 - 1s - loss: 2.1086 - mse: 2.1091 - val_loss: 2.1826 - val_mse: 2.1849\n",
            "Epoch 55/100\n",
            "380/380 - 1s - loss: 2.0790 - mse: 2.0797 - val_loss: 2.1537 - val_mse: 2.1559\n",
            "Epoch 56/100\n",
            "380/380 - 1s - loss: 2.0490 - mse: 2.0509 - val_loss: 2.1253 - val_mse: 2.1275\n",
            "Epoch 57/100\n",
            "380/380 - 1s - loss: 2.0201 - mse: 2.0227 - val_loss: 2.0978 - val_mse: 2.0999\n",
            "Epoch 58/100\n",
            "380/380 - 1s - loss: 1.9932 - mse: 1.9952 - val_loss: 2.0706 - val_mse: 2.0727\n",
            "Epoch 59/100\n",
            "380/380 - 1s - loss: 1.9693 - mse: 1.9681 - val_loss: 2.0439 - val_mse: 2.0460\n",
            "Epoch 60/100\n",
            "380/380 - 1s - loss: 1.9386 - mse: 1.9416 - val_loss: 2.0182 - val_mse: 2.0203\n",
            "Epoch 61/100\n",
            "380/380 - 1s - loss: 1.9126 - mse: 1.9158 - val_loss: 1.9931 - val_mse: 1.9951\n",
            "Epoch 62/100\n",
            "380/380 - 1s - loss: 1.8921 - mse: 1.8905 - val_loss: 1.9687 - val_mse: 1.9707\n",
            "Epoch 63/100\n",
            "380/380 - 1s - loss: 1.8637 - mse: 1.8657 - val_loss: 1.9445 - val_mse: 1.9464\n",
            "Epoch 64/100\n",
            "380/380 - 1s - loss: 1.8419 - mse: 1.8415 - val_loss: 1.9210 - val_mse: 1.9229\n",
            "Epoch 65/100\n",
            "380/380 - 1s - loss: 1.8160 - mse: 1.8175 - val_loss: 1.8976 - val_mse: 1.8995\n",
            "Epoch 66/100\n",
            "380/380 - 1s - loss: 1.7913 - mse: 1.7941 - val_loss: 1.8749 - val_mse: 1.8768\n",
            "Epoch 67/100\n",
            "380/380 - 1s - loss: 1.7720 - mse: 1.7711 - val_loss: 1.8525 - val_mse: 1.8543\n",
            "Epoch 68/100\n",
            "380/380 - 1s - loss: 1.7453 - mse: 1.7483 - val_loss: 1.8306 - val_mse: 1.8324\n",
            "Epoch 69/100\n",
            "380/380 - 1s - loss: 1.7271 - mse: 1.7261 - val_loss: 1.8090 - val_mse: 1.8108\n",
            "Epoch 70/100\n",
            "380/380 - 1s - loss: 1.7023 - mse: 1.7042 - val_loss: 1.7880 - val_mse: 1.7898\n",
            "Epoch 71/100\n",
            "380/380 - 1s - loss: 1.6886 - mse: 1.6829 - val_loss: 1.7674 - val_mse: 1.7691\n",
            "Epoch 72/100\n",
            "380/380 - 1s - loss: 1.6621 - mse: 1.6620 - val_loss: 1.7472 - val_mse: 1.7489\n",
            "Epoch 73/100\n",
            "380/380 - 1s - loss: 1.6396 - mse: 1.6414 - val_loss: 1.7273 - val_mse: 1.7290\n",
            "Epoch 74/100\n",
            "380/380 - 1s - loss: 1.6191 - mse: 1.6213 - val_loss: 1.7079 - val_mse: 1.7095\n",
            "Epoch 75/100\n",
            "380/380 - 1s - loss: 1.6063 - mse: 1.6015 - val_loss: 1.6889 - val_mse: 1.6905\n",
            "Epoch 76/100\n",
            "380/380 - 1s - loss: 1.5807 - mse: 1.5821 - val_loss: 1.6701 - val_mse: 1.6717\n",
            "Epoch 77/100\n",
            "380/380 - 1s - loss: 1.5633 - mse: 1.5631 - val_loss: 1.6519 - val_mse: 1.6534\n",
            "Epoch 78/100\n",
            "380/380 - 1s - loss: 1.5486 - mse: 1.5445 - val_loss: 1.6338 - val_mse: 1.6354\n",
            "Epoch 79/100\n",
            "380/380 - 1s - loss: 1.5244 - mse: 1.5261 - val_loss: 1.6163 - val_mse: 1.6178\n",
            "Epoch 80/100\n",
            "380/380 - 1s - loss: 1.5073 - mse: 1.5082 - val_loss: 1.5991 - val_mse: 1.6006\n",
            "Epoch 81/100\n",
            "380/380 - 1s - loss: 1.4908 - mse: 1.4907 - val_loss: 1.5822 - val_mse: 1.5837\n",
            "Epoch 82/100\n",
            "380/380 - 1s - loss: 1.4715 - mse: 1.4733 - val_loss: 1.5655 - val_mse: 1.5669\n",
            "Epoch 83/100\n",
            "380/380 - 1s - loss: 1.4539 - mse: 1.4563 - val_loss: 1.5491 - val_mse: 1.5505\n",
            "Epoch 84/100\n",
            "380/380 - 1s - loss: 1.4372 - mse: 1.4396 - val_loss: 1.5332 - val_mse: 1.5345\n",
            "Epoch 85/100\n",
            "380/380 - 1s - loss: 1.4281 - mse: 1.4234 - val_loss: 1.5176 - val_mse: 1.5190\n",
            "Epoch 86/100\n",
            "380/380 - 1s - loss: 1.4051 - mse: 1.4075 - val_loss: 1.5024 - val_mse: 1.5037\n",
            "Epoch 87/100\n",
            "380/380 - 1s - loss: 1.3901 - mse: 1.3919 - val_loss: 1.4873 - val_mse: 1.4887\n",
            "Epoch 88/100\n",
            "380/380 - 1s - loss: 1.3779 - mse: 1.3766 - val_loss: 1.4726 - val_mse: 1.4739\n",
            "Epoch 89/100\n",
            "380/380 - 1s - loss: 1.3610 - mse: 1.3616 - val_loss: 1.4583 - val_mse: 1.4595\n",
            "Epoch 90/100\n",
            "380/380 - 1s - loss: 1.3453 - mse: 1.3470 - val_loss: 1.4441 - val_mse: 1.4454\n",
            "Epoch 91/100\n",
            "380/380 - 1s - loss: 1.3301 - mse: 1.3327 - val_loss: 1.4305 - val_mse: 1.4317\n",
            "Epoch 92/100\n",
            "380/380 - 1s - loss: 1.3178 - mse: 1.3188 - val_loss: 1.4171 - val_mse: 1.4183\n",
            "Epoch 93/100\n",
            "380/380 - 1s - loss: 1.3027 - mse: 1.3052 - val_loss: 1.4039 - val_mse: 1.4051\n",
            "Epoch 94/100\n",
            "380/380 - 1s - loss: 1.2899 - mse: 1.2919 - val_loss: 1.3912 - val_mse: 1.3924\n",
            "Epoch 95/100\n",
            "380/380 - 1s - loss: 1.2773 - mse: 1.2790 - val_loss: 1.3788 - val_mse: 1.3800\n",
            "Epoch 96/100\n",
            "380/380 - 1s - loss: 1.2643 - mse: 1.2664 - val_loss: 1.3666 - val_mse: 1.3678\n",
            "Epoch 97/100\n",
            "380/380 - 1s - loss: 1.2557 - mse: 1.2540 - val_loss: 1.3547 - val_mse: 1.3559\n",
            "Epoch 98/100\n",
            "380/380 - 1s - loss: 1.2449 - mse: 1.2418 - val_loss: 1.3429 - val_mse: 1.3440\n",
            "Epoch 99/100\n",
            "380/380 - 1s - loss: 1.2287 - mse: 1.2299 - val_loss: 1.3315 - val_mse: 1.3326\n",
            "Epoch 100/100\n",
            "380/380 - 1s - loss: 1.2164 - mse: 1.2184 - val_loss: 1.3205 - val_mse: 1.3216\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 62%|██████▏   | 13/21 [14:57<09:06, 68.33s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 32)                288       \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "Output (Dense)               (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 321\n",
            "Trainable params: 321\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/100\n",
            "380/380 - 1s - loss: 1.9463 - mse: 4.9432 - val_loss: 1.9446 - val_mse: 5.0721\n",
            "Epoch 2/100\n",
            "380/380 - 1s - loss: 1.9371 - mse: 4.9164 - val_loss: 1.9380 - val_mse: 5.0460\n",
            "Epoch 3/100\n",
            "380/380 - 1s - loss: 1.9309 - mse: 4.8896 - val_loss: 1.9313 - val_mse: 5.0200\n",
            "Epoch 4/100\n",
            "380/380 - 1s - loss: 1.9236 - mse: 4.8628 - val_loss: 1.9246 - val_mse: 4.9940\n",
            "Epoch 5/100\n",
            "380/380 - 1s - loss: 1.9174 - mse: 4.8359 - val_loss: 1.9180 - val_mse: 4.9681\n",
            "Epoch 6/100\n",
            "380/380 - 1s - loss: 1.9102 - mse: 4.8095 - val_loss: 1.9113 - val_mse: 4.9424\n",
            "Epoch 7/100\n",
            "380/380 - 1s - loss: 1.9029 - mse: 4.7830 - val_loss: 1.9046 - val_mse: 4.9168\n",
            "Epoch 8/100\n",
            "380/380 - 1s - loss: 1.8966 - mse: 4.7565 - val_loss: 1.8979 - val_mse: 4.8912\n",
            "Epoch 9/100\n",
            "380/380 - 1s - loss: 1.8890 - mse: 4.7301 - val_loss: 1.8912 - val_mse: 4.8656\n",
            "Epoch 10/100\n",
            "380/380 - 1s - loss: 1.8833 - mse: 4.7038 - val_loss: 1.8845 - val_mse: 4.8402\n",
            "Epoch 11/100\n",
            "380/380 - 1s - loss: 1.8781 - mse: 4.6775 - val_loss: 1.8778 - val_mse: 4.8149\n",
            "Epoch 12/100\n",
            "380/380 - 1s - loss: 1.8697 - mse: 4.6517 - val_loss: 1.8711 - val_mse: 4.7895\n",
            "Epoch 13/100\n",
            "380/380 - 1s - loss: 1.8622 - mse: 4.6258 - val_loss: 1.8644 - val_mse: 4.7645\n",
            "Epoch 14/100\n",
            "380/380 - 1s - loss: 1.8567 - mse: 4.5998 - val_loss: 1.8577 - val_mse: 4.7393\n",
            "Epoch 15/100\n",
            "380/380 - 1s - loss: 1.8471 - mse: 4.5740 - val_loss: 1.8510 - val_mse: 4.7143\n",
            "Epoch 16/100\n",
            "380/380 - 1s - loss: 1.8414 - mse: 4.5484 - val_loss: 1.8443 - val_mse: 4.6892\n",
            "Epoch 17/100\n",
            "380/380 - 1s - loss: 1.8343 - mse: 4.5226 - val_loss: 1.8376 - val_mse: 4.6644\n",
            "Epoch 18/100\n",
            "380/380 - 1s - loss: 1.8268 - mse: 4.4971 - val_loss: 1.8309 - val_mse: 4.6395\n",
            "Epoch 19/100\n",
            "380/380 - 1s - loss: 1.8203 - mse: 4.4713 - val_loss: 1.8241 - val_mse: 4.6147\n",
            "Epoch 20/100\n",
            "380/380 - 1s - loss: 1.8128 - mse: 4.4462 - val_loss: 1.8174 - val_mse: 4.5901\n",
            "Epoch 21/100\n",
            "380/380 - 1s - loss: 1.8058 - mse: 4.4206 - val_loss: 1.8107 - val_mse: 4.5654\n",
            "Epoch 22/100\n",
            "380/380 - 1s - loss: 1.7991 - mse: 4.3953 - val_loss: 1.8040 - val_mse: 4.5408\n",
            "Epoch 23/100\n",
            "380/380 - 1s - loss: 1.7929 - mse: 4.3702 - val_loss: 1.7972 - val_mse: 4.5162\n",
            "Epoch 24/100\n",
            "380/380 - 1s - loss: 1.7862 - mse: 4.3447 - val_loss: 1.7904 - val_mse: 4.4914\n",
            "Epoch 25/100\n",
            "380/380 - 1s - loss: 1.7786 - mse: 4.3195 - val_loss: 1.7837 - val_mse: 4.4669\n",
            "Epoch 26/100\n",
            "380/380 - 1s - loss: 1.7716 - mse: 4.2940 - val_loss: 1.7769 - val_mse: 4.4423\n",
            "Epoch 27/100\n",
            "380/380 - 1s - loss: 1.7660 - mse: 4.2689 - val_loss: 1.7701 - val_mse: 4.4177\n",
            "Epoch 28/100\n",
            "380/380 - 1s - loss: 1.7584 - mse: 4.2439 - val_loss: 1.7633 - val_mse: 4.3935\n",
            "Epoch 29/100\n",
            "380/380 - 1s - loss: 1.7501 - mse: 4.2190 - val_loss: 1.7565 - val_mse: 4.3690\n",
            "Epoch 30/100\n",
            "380/380 - 1s - loss: 1.7453 - mse: 4.1938 - val_loss: 1.7498 - val_mse: 4.3447\n",
            "Epoch 31/100\n",
            "380/380 - 1s - loss: 1.7374 - mse: 4.1689 - val_loss: 1.7430 - val_mse: 4.3204\n",
            "Epoch 32/100\n",
            "380/380 - 1s - loss: 1.7310 - mse: 4.1442 - val_loss: 1.7361 - val_mse: 4.2962\n",
            "Epoch 33/100\n",
            "380/380 - 1s - loss: 1.7229 - mse: 4.1193 - val_loss: 1.7293 - val_mse: 4.2719\n",
            "Epoch 34/100\n",
            "380/380 - 1s - loss: 1.7162 - mse: 4.0944 - val_loss: 1.7224 - val_mse: 4.2476\n",
            "Epoch 35/100\n",
            "380/380 - 1s - loss: 1.7088 - mse: 4.0697 - val_loss: 1.7156 - val_mse: 4.2235\n",
            "Epoch 36/100\n",
            "380/380 - 1s - loss: 1.7016 - mse: 4.0449 - val_loss: 1.7088 - val_mse: 4.1994\n",
            "Epoch 37/100\n",
            "380/380 - 1s - loss: 1.6946 - mse: 4.0200 - val_loss: 1.7019 - val_mse: 4.1752\n",
            "Epoch 38/100\n",
            "380/380 - 1s - loss: 1.6900 - mse: 3.9955 - val_loss: 1.6950 - val_mse: 4.1512\n",
            "Epoch 39/100\n",
            "380/380 - 1s - loss: 1.6817 - mse: 3.9709 - val_loss: 1.6882 - val_mse: 4.1271\n",
            "Epoch 40/100\n",
            "380/380 - 1s - loss: 1.6724 - mse: 3.9465 - val_loss: 1.6813 - val_mse: 4.1032\n",
            "Epoch 41/100\n",
            "380/380 - 1s - loss: 1.6662 - mse: 3.9220 - val_loss: 1.6744 - val_mse: 4.0793\n",
            "Epoch 42/100\n",
            "380/380 - 1s - loss: 1.6608 - mse: 3.8977 - val_loss: 1.6675 - val_mse: 4.0554\n",
            "Epoch 43/100\n",
            "380/380 - 1s - loss: 1.6539 - mse: 3.8733 - val_loss: 1.6606 - val_mse: 4.0316\n",
            "Epoch 44/100\n",
            "380/380 - 1s - loss: 1.6453 - mse: 3.8488 - val_loss: 1.6537 - val_mse: 4.0077\n",
            "Epoch 45/100\n",
            "380/380 - 1s - loss: 1.6397 - mse: 3.8247 - val_loss: 1.6468 - val_mse: 3.9840\n",
            "Epoch 46/100\n",
            "380/380 - 1s - loss: 1.6317 - mse: 3.8003 - val_loss: 1.6398 - val_mse: 3.9600\n",
            "Epoch 47/100\n",
            "380/380 - 1s - loss: 1.6237 - mse: 3.7759 - val_loss: 1.6329 - val_mse: 3.9362\n",
            "Epoch 48/100\n",
            "380/380 - 1s - loss: 1.6175 - mse: 3.7515 - val_loss: 1.6259 - val_mse: 3.9124\n",
            "Epoch 49/100\n",
            "380/380 - 1s - loss: 1.6117 - mse: 3.7274 - val_loss: 1.6190 - val_mse: 3.8885\n",
            "Epoch 50/100\n",
            "380/380 - 1s - loss: 1.6035 - mse: 3.7030 - val_loss: 1.6121 - val_mse: 3.8647\n",
            "Epoch 51/100\n",
            "380/380 - 1s - loss: 1.5960 - mse: 3.6788 - val_loss: 1.6051 - val_mse: 3.8408\n",
            "Epoch 52/100\n",
            "380/380 - 1s - loss: 1.5885 - mse: 3.6548 - val_loss: 1.5981 - val_mse: 3.8170\n",
            "Epoch 53/100\n",
            "380/380 - 1s - loss: 1.5808 - mse: 3.6304 - val_loss: 1.5912 - val_mse: 3.7932\n",
            "Epoch 54/100\n",
            "380/380 - 1s - loss: 1.5755 - mse: 3.6063 - val_loss: 1.5842 - val_mse: 3.7694\n",
            "Epoch 55/100\n",
            "380/380 - 1s - loss: 1.5683 - mse: 3.5822 - val_loss: 1.5772 - val_mse: 3.7455\n",
            "Epoch 56/100\n",
            "380/380 - 1s - loss: 1.5609 - mse: 3.5580 - val_loss: 1.5702 - val_mse: 3.7219\n",
            "Epoch 57/100\n",
            "380/380 - 1s - loss: 1.5528 - mse: 3.5340 - val_loss: 1.5632 - val_mse: 3.6981\n",
            "Epoch 58/100\n",
            "380/380 - 1s - loss: 1.5467 - mse: 3.5101 - val_loss: 1.5562 - val_mse: 3.6745\n",
            "Epoch 59/100\n",
            "380/380 - 1s - loss: 1.5395 - mse: 3.4859 - val_loss: 1.5492 - val_mse: 3.6506\n",
            "Epoch 60/100\n",
            "380/380 - 1s - loss: 1.5316 - mse: 3.4619 - val_loss: 1.5421 - val_mse: 3.6268\n",
            "Epoch 61/100\n",
            "380/380 - 1s - loss: 1.5239 - mse: 3.4378 - val_loss: 1.5351 - val_mse: 3.6031\n",
            "Epoch 62/100\n",
            "380/380 - 1s - loss: 1.5187 - mse: 3.4139 - val_loss: 1.5280 - val_mse: 3.5794\n",
            "Epoch 63/100\n",
            "380/380 - 1s - loss: 1.5103 - mse: 3.3897 - val_loss: 1.5209 - val_mse: 3.5554\n",
            "Epoch 64/100\n",
            "380/380 - 1s - loss: 1.5050 - mse: 3.3657 - val_loss: 1.5139 - val_mse: 3.5318\n",
            "Epoch 65/100\n",
            "380/380 - 1s - loss: 1.4967 - mse: 3.3418 - val_loss: 1.5067 - val_mse: 3.5079\n",
            "Epoch 66/100\n",
            "380/380 - 1s - loss: 1.4883 - mse: 3.3177 - val_loss: 1.4996 - val_mse: 3.4841\n",
            "Epoch 67/100\n",
            "380/380 - 1s - loss: 1.4841 - mse: 3.2938 - val_loss: 1.4925 - val_mse: 3.4602\n",
            "Epoch 68/100\n",
            "380/380 - 1s - loss: 1.4738 - mse: 3.2697 - val_loss: 1.4853 - val_mse: 3.4363\n",
            "Epoch 69/100\n",
            "380/380 - 1s - loss: 1.4688 - mse: 3.2458 - val_loss: 1.4782 - val_mse: 3.4125\n",
            "Epoch 70/100\n",
            "380/380 - 1s - loss: 1.4609 - mse: 3.2216 - val_loss: 1.4711 - val_mse: 3.3887\n",
            "Epoch 71/100\n",
            "380/380 - 1s - loss: 1.4561 - mse: 3.1976 - val_loss: 1.4639 - val_mse: 3.3648\n",
            "Epoch 72/100\n",
            "380/380 - 1s - loss: 1.4469 - mse: 3.1739 - val_loss: 1.4568 - val_mse: 3.3410\n",
            "Epoch 73/100\n",
            "380/380 - 1s - loss: 1.4380 - mse: 3.1500 - val_loss: 1.4496 - val_mse: 3.3173\n",
            "Epoch 74/100\n",
            "380/380 - 1s - loss: 1.4307 - mse: 3.1263 - val_loss: 1.4425 - val_mse: 3.2936\n",
            "Epoch 75/100\n",
            "380/380 - 1s - loss: 1.4252 - mse: 3.1024 - val_loss: 1.4353 - val_mse: 3.2700\n",
            "Epoch 76/100\n",
            "380/380 - 1s - loss: 1.4172 - mse: 3.0788 - val_loss: 1.4281 - val_mse: 3.2462\n",
            "Epoch 77/100\n",
            "380/380 - 1s - loss: 1.4094 - mse: 3.0550 - val_loss: 1.4209 - val_mse: 3.2226\n",
            "Epoch 78/100\n",
            "380/380 - 1s - loss: 1.4039 - mse: 3.0314 - val_loss: 1.4137 - val_mse: 3.1990\n",
            "Epoch 79/100\n",
            "380/380 - 1s - loss: 1.3942 - mse: 3.0077 - val_loss: 1.4065 - val_mse: 3.1752\n",
            "Epoch 80/100\n",
            "380/380 - 1s - loss: 1.3875 - mse: 2.9840 - val_loss: 1.3993 - val_mse: 3.1516\n",
            "Epoch 81/100\n",
            "380/380 - 1s - loss: 1.3817 - mse: 2.9604 - val_loss: 1.3920 - val_mse: 3.1281\n",
            "Epoch 82/100\n",
            "380/380 - 1s - loss: 1.3728 - mse: 2.9368 - val_loss: 1.3848 - val_mse: 3.1043\n",
            "Epoch 83/100\n",
            "380/380 - 1s - loss: 1.3647 - mse: 2.9132 - val_loss: 1.3775 - val_mse: 3.0807\n",
            "Epoch 84/100\n",
            "380/380 - 1s - loss: 1.3574 - mse: 2.8899 - val_loss: 1.3703 - val_mse: 3.0572\n",
            "Epoch 85/100\n",
            "380/380 - 1s - loss: 1.3523 - mse: 2.8665 - val_loss: 1.3630 - val_mse: 3.0338\n",
            "Epoch 86/100\n",
            "380/380 - 1s - loss: 1.3423 - mse: 2.8430 - val_loss: 1.3558 - val_mse: 3.0104\n",
            "Epoch 87/100\n",
            "380/380 - 1s - loss: 1.3357 - mse: 2.8198 - val_loss: 1.3485 - val_mse: 2.9870\n",
            "Epoch 88/100\n",
            "380/380 - 1s - loss: 1.3296 - mse: 2.7967 - val_loss: 1.3412 - val_mse: 2.9637\n",
            "Epoch 89/100\n",
            "380/380 - 1s - loss: 1.3216 - mse: 2.7733 - val_loss: 1.3339 - val_mse: 2.9402\n",
            "Epoch 90/100\n",
            "380/380 - 1s - loss: 1.3142 - mse: 2.7501 - val_loss: 1.3266 - val_mse: 2.9167\n",
            "Epoch 91/100\n",
            "380/380 - 1s - loss: 1.3055 - mse: 2.7268 - val_loss: 1.3193 - val_mse: 2.8933\n",
            "Epoch 92/100\n",
            "380/380 - 1s - loss: 1.2980 - mse: 2.7038 - val_loss: 1.3120 - val_mse: 2.8700\n",
            "Epoch 93/100\n",
            "380/380 - 1s - loss: 1.2909 - mse: 2.6806 - val_loss: 1.3046 - val_mse: 2.8466\n",
            "Epoch 94/100\n",
            "380/380 - 1s - loss: 1.2835 - mse: 2.6575 - val_loss: 1.2973 - val_mse: 2.8234\n",
            "Epoch 95/100\n",
            "380/380 - 1s - loss: 1.2764 - mse: 2.6346 - val_loss: 1.2899 - val_mse: 2.8002\n",
            "Epoch 96/100\n",
            "380/380 - 1s - loss: 1.2685 - mse: 2.6116 - val_loss: 1.2826 - val_mse: 2.7770\n",
            "Epoch 97/100\n",
            "380/380 - 1s - loss: 1.2642 - mse: 2.5886 - val_loss: 1.2753 - val_mse: 2.7537\n",
            "Epoch 98/100\n",
            "380/380 - 1s - loss: 1.2553 - mse: 2.5659 - val_loss: 1.2679 - val_mse: 2.7306\n",
            "Epoch 99/100\n",
            "380/380 - 1s - loss: 1.2481 - mse: 2.5431 - val_loss: 1.2606 - val_mse: 2.7076\n",
            "Epoch 100/100\n",
            "380/380 - 1s - loss: 1.2394 - mse: 2.5204 - val_loss: 1.2533 - val_mse: 2.6845\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 67%|██████▋   | 14/21 [16:05<07:58, 68.32s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 64)                576       \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "Output (Dense)               (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 641\n",
            "Trainable params: 641\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/100\n",
            "380/380 - 1s - loss: 6.0191 - mse: 6.0166 - val_loss: 6.1563 - val_mse: 6.1587\n",
            "Epoch 2/100\n",
            "380/380 - 1s - loss: 5.9868 - mse: 5.9907 - val_loss: 6.1295 - val_mse: 6.1319\n",
            "Epoch 3/100\n",
            "380/380 - 1s - loss: 5.9611 - mse: 5.9648 - val_loss: 6.1028 - val_mse: 6.1052\n",
            "Epoch 4/100\n",
            "380/380 - 1s - loss: 5.9311 - mse: 5.9391 - val_loss: 6.0761 - val_mse: 6.0785\n",
            "Epoch 5/100\n",
            "380/380 - 1s - loss: 5.9123 - mse: 5.9135 - val_loss: 6.0497 - val_mse: 6.0521\n",
            "Epoch 6/100\n",
            "380/380 - 1s - loss: 5.8867 - mse: 5.8882 - val_loss: 6.0236 - val_mse: 6.0259\n",
            "Epoch 7/100\n",
            "380/380 - 1s - loss: 5.8570 - mse: 5.8630 - val_loss: 5.9975 - val_mse: 5.9999\n",
            "Epoch 8/100\n",
            "380/380 - 1s - loss: 5.8354 - mse: 5.8378 - val_loss: 5.9715 - val_mse: 5.9739\n",
            "Epoch 9/100\n",
            "380/380 - 1s - loss: 5.8078 - mse: 5.8128 - val_loss: 5.9455 - val_mse: 5.9479\n",
            "Epoch 10/100\n",
            "380/380 - 1s - loss: 5.7882 - mse: 5.7879 - val_loss: 5.9199 - val_mse: 5.9222\n",
            "Epoch 11/100\n",
            "380/380 - 1s - loss: 5.7717 - mse: 5.7631 - val_loss: 5.8944 - val_mse: 5.8967\n",
            "Epoch 12/100\n",
            "380/380 - 1s - loss: 5.7398 - mse: 5.7385 - val_loss: 5.8688 - val_mse: 5.8712\n",
            "Epoch 13/100\n",
            "380/380 - 1s - loss: 5.7095 - mse: 5.7139 - val_loss: 5.8436 - val_mse: 5.8459\n",
            "Epoch 14/100\n",
            "380/380 - 1s - loss: 5.6986 - mse: 5.6894 - val_loss: 5.8182 - val_mse: 5.8206\n",
            "Epoch 15/100\n",
            "380/380 - 1s - loss: 5.6571 - mse: 5.6650 - val_loss: 5.7930 - val_mse: 5.7953\n",
            "Epoch 16/100\n",
            "380/380 - 1s - loss: 5.6368 - mse: 5.6407 - val_loss: 5.7678 - val_mse: 5.7701\n",
            "Epoch 17/100\n",
            "380/380 - 1s - loss: 5.6145 - mse: 5.6165 - val_loss: 5.7427 - val_mse: 5.7451\n",
            "Epoch 18/100\n",
            "380/380 - 1s - loss: 5.5880 - mse: 5.5923 - val_loss: 5.7178 - val_mse: 5.7201\n",
            "Epoch 19/100\n",
            "380/380 - 1s - loss: 5.5667 - mse: 5.5683 - val_loss: 5.6930 - val_mse: 5.6953\n",
            "Epoch 20/100\n",
            "380/380 - 1s - loss: 5.5368 - mse: 5.5443 - val_loss: 5.6682 - val_mse: 5.6705\n",
            "Epoch 21/100\n",
            "380/380 - 1s - loss: 5.5135 - mse: 5.5205 - val_loss: 5.6435 - val_mse: 5.6458\n",
            "Epoch 22/100\n",
            "380/380 - 1s - loss: 5.4908 - mse: 5.4967 - val_loss: 5.6190 - val_mse: 5.6213\n",
            "Epoch 23/100\n",
            "380/380 - 1s - loss: 5.4759 - mse: 5.4729 - val_loss: 5.5944 - val_mse: 5.5967\n",
            "Epoch 24/100\n",
            "380/380 - 1s - loss: 5.4511 - mse: 5.4492 - val_loss: 5.5698 - val_mse: 5.5721\n",
            "Epoch 25/100\n",
            "380/380 - 1s - loss: 5.4278 - mse: 5.4255 - val_loss: 5.5453 - val_mse: 5.5476\n",
            "Epoch 26/100\n",
            "380/380 - 1s - loss: 5.3981 - mse: 5.4019 - val_loss: 5.5210 - val_mse: 5.5233\n",
            "Epoch 27/100\n",
            "380/380 - 1s - loss: 5.3897 - mse: 5.3784 - val_loss: 5.4968 - val_mse: 5.4990\n",
            "Epoch 28/100\n",
            "380/380 - 1s - loss: 5.3559 - mse: 5.3550 - val_loss: 5.4726 - val_mse: 5.4749\n",
            "Epoch 29/100\n",
            "380/380 - 1s - loss: 5.3258 - mse: 5.3319 - val_loss: 5.4487 - val_mse: 5.4510\n",
            "Epoch 30/100\n",
            "380/380 - 1s - loss: 5.3142 - mse: 5.3088 - val_loss: 5.4249 - val_mse: 5.4271\n",
            "Epoch 31/100\n",
            "380/380 - 1s - loss: 5.2867 - mse: 5.2858 - val_loss: 5.4011 - val_mse: 5.4034\n",
            "Epoch 32/100\n",
            "380/380 - 1s - loss: 5.2696 - mse: 5.2628 - val_loss: 5.3774 - val_mse: 5.3796\n",
            "Epoch 33/100\n",
            "380/380 - 1s - loss: 5.2350 - mse: 5.2399 - val_loss: 5.3537 - val_mse: 5.3559\n",
            "Epoch 34/100\n",
            "380/380 - 1s - loss: 5.2145 - mse: 5.2170 - val_loss: 5.3301 - val_mse: 5.3323\n",
            "Epoch 35/100\n",
            "380/380 - 1s - loss: 5.1930 - mse: 5.1943 - val_loss: 5.3066 - val_mse: 5.3088\n",
            "Epoch 36/100\n",
            "380/380 - 1s - loss: 5.1676 - mse: 5.1715 - val_loss: 5.2831 - val_mse: 5.2853\n",
            "Epoch 37/100\n",
            "380/380 - 1s - loss: 5.1506 - mse: 5.1489 - val_loss: 5.2597 - val_mse: 5.2619\n",
            "Epoch 38/100\n",
            "380/380 - 1s - loss: 5.1345 - mse: 5.1263 - val_loss: 5.2363 - val_mse: 5.2385\n",
            "Epoch 39/100\n",
            "380/380 - 1s - loss: 5.1089 - mse: 5.1037 - val_loss: 5.2130 - val_mse: 5.2152\n",
            "Epoch 40/100\n",
            "380/380 - 1s - loss: 5.0734 - mse: 5.0813 - val_loss: 5.1898 - val_mse: 5.1920\n",
            "Epoch 41/100\n",
            "380/380 - 1s - loss: 5.0537 - mse: 5.0590 - val_loss: 5.1668 - val_mse: 5.1690\n",
            "Epoch 42/100\n",
            "380/380 - 1s - loss: 5.0425 - mse: 5.0368 - val_loss: 5.1438 - val_mse: 5.1460\n",
            "Epoch 43/100\n",
            "380/380 - 1s - loss: 5.0175 - mse: 5.0146 - val_loss: 5.1209 - val_mse: 5.1231\n",
            "Epoch 44/100\n",
            "380/380 - 1s - loss: 4.9887 - mse: 4.9924 - val_loss: 5.0979 - val_mse: 5.1001\n",
            "Epoch 45/100\n",
            "380/380 - 1s - loss: 4.9759 - mse: 4.9703 - val_loss: 5.0752 - val_mse: 5.0773\n",
            "Epoch 46/100\n",
            "380/380 - 1s - loss: 4.9448 - mse: 4.9483 - val_loss: 5.0524 - val_mse: 5.0545\n",
            "Epoch 47/100\n",
            "380/380 - 1s - loss: 4.9210 - mse: 4.9263 - val_loss: 5.0297 - val_mse: 5.0318\n",
            "Epoch 48/100\n",
            "380/380 - 1s - loss: 4.9052 - mse: 4.9044 - val_loss: 5.0070 - val_mse: 5.0092\n",
            "Epoch 49/100\n",
            "380/380 - 1s - loss: 4.8894 - mse: 4.8826 - val_loss: 4.9845 - val_mse: 4.9866\n",
            "Epoch 50/100\n",
            "380/380 - 1s - loss: 4.8635 - mse: 4.8608 - val_loss: 4.9621 - val_mse: 4.9642\n",
            "Epoch 51/100\n",
            "380/380 - 1s - loss: 4.8377 - mse: 4.8392 - val_loss: 4.9397 - val_mse: 4.9418\n",
            "Epoch 52/100\n",
            "380/380 - 1s - loss: 4.8142 - mse: 4.8176 - val_loss: 4.9173 - val_mse: 4.9195\n",
            "Epoch 53/100\n",
            "380/380 - 1s - loss: 4.7893 - mse: 4.7961 - val_loss: 4.8952 - val_mse: 4.8973\n",
            "Epoch 54/100\n",
            "380/380 - 1s - loss: 4.7757 - mse: 4.7747 - val_loss: 4.8730 - val_mse: 4.8751\n",
            "Epoch 55/100\n",
            "380/380 - 1s - loss: 4.7528 - mse: 4.7533 - val_loss: 4.8509 - val_mse: 4.8530\n",
            "Epoch 56/100\n",
            "380/380 - 1s - loss: 4.7318 - mse: 4.7320 - val_loss: 4.8290 - val_mse: 4.8311\n",
            "Epoch 57/100\n",
            "380/380 - 1s - loss: 4.7063 - mse: 4.7108 - val_loss: 4.8070 - val_mse: 4.8091\n",
            "Epoch 58/100\n",
            "380/380 - 1s - loss: 4.6865 - mse: 4.6896 - val_loss: 4.7852 - val_mse: 4.7873\n",
            "Epoch 59/100\n",
            "380/380 - 1s - loss: 4.6700 - mse: 4.6686 - val_loss: 4.7634 - val_mse: 4.7655\n",
            "Epoch 60/100\n",
            "380/380 - 1s - loss: 4.6431 - mse: 4.6475 - val_loss: 4.7417 - val_mse: 4.7438\n",
            "Epoch 61/100\n",
            "380/380 - 1s - loss: 4.6196 - mse: 4.6266 - val_loss: 4.7200 - val_mse: 4.7221\n",
            "Epoch 62/100\n",
            "380/380 - 1s - loss: 4.6086 - mse: 4.6057 - val_loss: 4.6986 - val_mse: 4.7006\n",
            "Epoch 63/100\n",
            "380/380 - 1s - loss: 4.5816 - mse: 4.5848 - val_loss: 4.6768 - val_mse: 4.6789\n",
            "Epoch 64/100\n",
            "380/380 - 1s - loss: 4.5731 - mse: 4.5640 - val_loss: 4.6554 - val_mse: 4.6575\n",
            "Epoch 65/100\n",
            "380/380 - 1s - loss: 4.5420 - mse: 4.5432 - val_loss: 4.6339 - val_mse: 4.6360\n",
            "Epoch 66/100\n",
            "380/380 - 1s - loss: 4.5172 - mse: 4.5225 - val_loss: 4.6126 - val_mse: 4.6146\n",
            "Epoch 67/100\n",
            "380/380 - 1s - loss: 4.5080 - mse: 4.5019 - val_loss: 4.5913 - val_mse: 4.5933\n",
            "Epoch 68/100\n",
            "380/380 - 1s - loss: 4.4769 - mse: 4.4813 - val_loss: 4.5700 - val_mse: 4.5721\n",
            "Epoch 69/100\n",
            "380/380 - 1s - loss: 4.4660 - mse: 4.4608 - val_loss: 4.5489 - val_mse: 4.5509\n",
            "Epoch 70/100\n",
            "380/380 - 1s - loss: 4.4436 - mse: 4.4403 - val_loss: 4.5278 - val_mse: 4.5298\n",
            "Epoch 71/100\n",
            "380/380 - 1s - loss: 4.4508 - mse: 4.4199 - val_loss: 4.5067 - val_mse: 4.5088\n",
            "Epoch 72/100\n",
            "380/380 - 1s - loss: 4.4031 - mse: 4.3996 - val_loss: 4.4859 - val_mse: 4.4879\n",
            "Epoch 73/100\n",
            "380/380 - 1s - loss: 4.3768 - mse: 4.3796 - val_loss: 4.4651 - val_mse: 4.4671\n",
            "Epoch 74/100\n",
            "380/380 - 1s - loss: 4.3552 - mse: 4.3596 - val_loss: 4.4445 - val_mse: 4.4465\n",
            "Epoch 75/100\n",
            "380/380 - 1s - loss: 4.3468 - mse: 4.3396 - val_loss: 4.4238 - val_mse: 4.4258\n",
            "Epoch 76/100\n",
            "380/380 - 1s - loss: 4.3173 - mse: 4.3196 - val_loss: 4.4031 - val_mse: 4.4051\n",
            "Epoch 77/100\n",
            "380/380 - 1s - loss: 4.2998 - mse: 4.2997 - val_loss: 4.3826 - val_mse: 4.3846\n",
            "Epoch 78/100\n",
            "380/380 - 1s - loss: 4.2894 - mse: 4.2798 - val_loss: 4.3621 - val_mse: 4.3641\n",
            "Epoch 79/100\n",
            "380/380 - 1s - loss: 4.2546 - mse: 4.2599 - val_loss: 4.3414 - val_mse: 4.3434\n",
            "Epoch 80/100\n",
            "380/380 - 1s - loss: 4.2371 - mse: 4.2401 - val_loss: 4.3210 - val_mse: 4.3230\n",
            "Epoch 81/100\n",
            "380/380 - 1s - loss: 4.2265 - mse: 4.2202 - val_loss: 4.3005 - val_mse: 4.3025\n",
            "Epoch 82/100\n",
            "380/380 - 1s - loss: 4.1994 - mse: 4.2004 - val_loss: 4.2800 - val_mse: 4.2820\n",
            "Epoch 83/100\n",
            "380/380 - 1s - loss: 4.1760 - mse: 4.1807 - val_loss: 4.2596 - val_mse: 4.2616\n",
            "Epoch 84/100\n",
            "380/380 - 1s - loss: 4.1558 - mse: 4.1611 - val_loss: 4.2394 - val_mse: 4.2413\n",
            "Epoch 85/100\n",
            "380/380 - 1s - loss: 4.1491 - mse: 4.1415 - val_loss: 4.2193 - val_mse: 4.2213\n",
            "Epoch 86/100\n",
            "380/380 - 1s - loss: 4.1170 - mse: 4.1220 - val_loss: 4.1991 - val_mse: 4.2011\n",
            "Epoch 87/100\n",
            "380/380 - 1s - loss: 4.0996 - mse: 4.1026 - val_loss: 4.1791 - val_mse: 4.1811\n",
            "Epoch 88/100\n",
            "380/380 - 1s - loss: 4.0865 - mse: 4.0832 - val_loss: 4.1591 - val_mse: 4.1611\n",
            "Epoch 89/100\n",
            "380/380 - 1s - loss: 4.0653 - mse: 4.0638 - val_loss: 4.1391 - val_mse: 4.1410\n",
            "Epoch 90/100\n",
            "380/380 - 1s - loss: 4.0458 - mse: 4.0445 - val_loss: 4.1191 - val_mse: 4.1211\n",
            "Epoch 91/100\n",
            "380/380 - 1s - loss: 4.0202 - mse: 4.0252 - val_loss: 4.0992 - val_mse: 4.1012\n",
            "Epoch 92/100\n",
            "380/380 - 1s - loss: 4.0031 - mse: 4.0060 - val_loss: 4.0793 - val_mse: 4.0813\n",
            "Epoch 93/100\n",
            "380/380 - 1s - loss: 3.9817 - mse: 3.9868 - val_loss: 4.0596 - val_mse: 4.0615\n",
            "Epoch 94/100\n",
            "380/380 - 1s - loss: 3.9624 - mse: 3.9677 - val_loss: 4.0399 - val_mse: 4.0419\n",
            "Epoch 95/100\n",
            "380/380 - 1s - loss: 3.9450 - mse: 3.9486 - val_loss: 4.0202 - val_mse: 4.0222\n",
            "Epoch 96/100\n",
            "380/380 - 1s - loss: 3.9246 - mse: 3.9296 - val_loss: 4.0006 - val_mse: 4.0026\n",
            "Epoch 97/100\n",
            "380/380 - 1s - loss: 3.9194 - mse: 3.9107 - val_loss: 3.9811 - val_mse: 3.9830\n",
            "Epoch 98/100\n",
            "380/380 - 1s - loss: 3.8938 - mse: 3.8917 - val_loss: 3.9615 - val_mse: 3.9634\n",
            "Epoch 99/100\n",
            "380/380 - 1s - loss: 3.8721 - mse: 3.8729 - val_loss: 3.9420 - val_mse: 3.9439\n",
            "Epoch 100/100\n",
            "380/380 - 1s - loss: 3.8494 - mse: 3.8540 - val_loss: 3.9226 - val_mse: 3.9246\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 71%|███████▏  | 15/21 [17:13<06:49, 68.21s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 32)                288       \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 32)                1056      \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "Output (Dense)               (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 1,377\n",
            "Trainable params: 1,377\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/100\n",
            "380/380 - 1s - loss: 2.0635 - mse: 5.9093 - val_loss: 2.0960 - val_mse: 6.1402\n",
            "Epoch 2/100\n",
            "380/380 - 1s - loss: 2.0472 - mse: 5.8331 - val_loss: 2.0790 - val_mse: 6.0630\n",
            "Epoch 3/100\n",
            "380/380 - 1s - loss: 2.0294 - mse: 5.7578 - val_loss: 2.0620 - val_mse: 5.9866\n",
            "Epoch 4/100\n",
            "380/380 - 1s - loss: 2.0109 - mse: 5.6833 - val_loss: 2.0451 - val_mse: 5.9110\n",
            "Epoch 5/100\n",
            "380/380 - 1s - loss: 1.9964 - mse: 5.6095 - val_loss: 2.0283 - val_mse: 5.8360\n",
            "Epoch 6/100\n",
            "380/380 - 1s - loss: 1.9801 - mse: 5.5365 - val_loss: 2.0115 - val_mse: 5.7618\n",
            "Epoch 7/100\n",
            "380/380 - 1s - loss: 1.9616 - mse: 5.4643 - val_loss: 1.9948 - val_mse: 5.6884\n",
            "Epoch 8/100\n",
            "380/380 - 1s - loss: 1.9460 - mse: 5.3929 - val_loss: 1.9781 - val_mse: 5.6157\n",
            "Epoch 9/100\n",
            "380/380 - 1s - loss: 1.9293 - mse: 5.3217 - val_loss: 1.9615 - val_mse: 5.5436\n",
            "Epoch 10/100\n",
            "380/380 - 1s - loss: 1.9130 - mse: 5.2518 - val_loss: 1.9450 - val_mse: 5.4723\n",
            "Epoch 11/100\n",
            "380/380 - 1s - loss: 1.8973 - mse: 5.1823 - val_loss: 1.9285 - val_mse: 5.4017\n",
            "Epoch 12/100\n",
            "380/380 - 1s - loss: 1.8811 - mse: 5.1135 - val_loss: 1.9121 - val_mse: 5.3318\n",
            "Epoch 13/100\n",
            "380/380 - 1s - loss: 1.8619 - mse: 5.0455 - val_loss: 1.8957 - val_mse: 5.2626\n",
            "Epoch 14/100\n",
            "380/380 - 1s - loss: 1.8485 - mse: 4.9781 - val_loss: 1.8795 - val_mse: 5.1941\n",
            "Epoch 15/100\n",
            "380/380 - 1s - loss: 1.8291 - mse: 4.9115 - val_loss: 1.8633 - val_mse: 5.1263\n",
            "Epoch 16/100\n",
            "380/380 - 1s - loss: 1.8139 - mse: 4.8456 - val_loss: 1.8472 - val_mse: 5.0591\n",
            "Epoch 17/100\n",
            "380/380 - 1s - loss: 1.7981 - mse: 4.7801 - val_loss: 1.8312 - val_mse: 4.9926\n",
            "Epoch 18/100\n",
            "380/380 - 1s - loss: 1.7813 - mse: 4.7155 - val_loss: 1.8152 - val_mse: 4.9268\n",
            "Epoch 19/100\n",
            "380/380 - 1s - loss: 1.7669 - mse: 4.6515 - val_loss: 1.7993 - val_mse: 4.8615\n",
            "Epoch 20/100\n",
            "380/380 - 1s - loss: 1.7490 - mse: 4.5882 - val_loss: 1.7835 - val_mse: 4.7970\n",
            "Epoch 21/100\n",
            "380/380 - 1s - loss: 1.7339 - mse: 4.5255 - val_loss: 1.7678 - val_mse: 4.7331\n",
            "Epoch 22/100\n",
            "380/380 - 1s - loss: 1.7181 - mse: 4.4633 - val_loss: 1.7521 - val_mse: 4.6699\n",
            "Epoch 23/100\n",
            "380/380 - 1s - loss: 1.7041 - mse: 4.4020 - val_loss: 1.7365 - val_mse: 4.6073\n",
            "Epoch 24/100\n",
            "380/380 - 1s - loss: 1.6890 - mse: 4.3411 - val_loss: 1.7209 - val_mse: 4.5453\n",
            "Epoch 25/100\n",
            "380/380 - 1s - loss: 1.6733 - mse: 4.2811 - val_loss: 1.7055 - val_mse: 4.4840\n",
            "Epoch 26/100\n",
            "380/380 - 1s - loss: 1.6573 - mse: 4.2215 - val_loss: 1.6901 - val_mse: 4.4233\n",
            "Epoch 27/100\n",
            "380/380 - 1s - loss: 1.6445 - mse: 4.1625 - val_loss: 1.6748 - val_mse: 4.3631\n",
            "Epoch 28/100\n",
            "380/380 - 1s - loss: 1.6269 - mse: 4.1040 - val_loss: 1.6596 - val_mse: 4.3037\n",
            "Epoch 29/100\n",
            "380/380 - 1s - loss: 1.6114 - mse: 4.0471 - val_loss: 1.6445 - val_mse: 4.2449\n",
            "Epoch 30/100\n",
            "380/380 - 1s - loss: 1.5983 - mse: 3.9897 - val_loss: 1.6294 - val_mse: 4.1866\n",
            "Epoch 31/100\n",
            "380/380 - 1s - loss: 1.5831 - mse: 3.9331 - val_loss: 1.6145 - val_mse: 4.1289\n",
            "Epoch 32/100\n",
            "380/380 - 1s - loss: 1.5687 - mse: 3.8774 - val_loss: 1.5996 - val_mse: 4.0719\n",
            "Epoch 33/100\n",
            "380/380 - 1s - loss: 1.5515 - mse: 3.8222 - val_loss: 1.5849 - val_mse: 4.0155\n",
            "Epoch 34/100\n",
            "380/380 - 1s - loss: 1.5373 - mse: 3.7677 - val_loss: 1.5703 - val_mse: 3.9598\n",
            "Epoch 35/100\n",
            "380/380 - 1s - loss: 1.5240 - mse: 3.7135 - val_loss: 1.5558 - val_mse: 3.9046\n",
            "Epoch 36/100\n",
            "380/380 - 1s - loss: 1.5084 - mse: 3.6606 - val_loss: 1.5414 - val_mse: 3.8500\n",
            "Epoch 37/100\n",
            "380/380 - 1s - loss: 1.4960 - mse: 3.6075 - val_loss: 1.5271 - val_mse: 3.7960\n",
            "Epoch 38/100\n",
            "380/380 - 1s - loss: 1.4820 - mse: 3.5553 - val_loss: 1.5129 - val_mse: 3.7427\n",
            "Epoch 39/100\n",
            "380/380 - 1s - loss: 1.4678 - mse: 3.5039 - val_loss: 1.4989 - val_mse: 3.6900\n",
            "Epoch 40/100\n",
            "380/380 - 1s - loss: 1.4504 - mse: 3.4531 - val_loss: 1.4849 - val_mse: 3.6380\n",
            "Epoch 41/100\n",
            "380/380 - 1s - loss: 1.4372 - mse: 3.4031 - val_loss: 1.4711 - val_mse: 3.5867\n",
            "Epoch 42/100\n",
            "380/380 - 1s - loss: 1.4256 - mse: 3.3536 - val_loss: 1.4574 - val_mse: 3.5360\n",
            "Epoch 43/100\n",
            "380/380 - 1s - loss: 1.4111 - mse: 3.3048 - val_loss: 1.4438 - val_mse: 3.4859\n",
            "Epoch 44/100\n",
            "380/380 - 1s - loss: 1.3962 - mse: 3.2563 - val_loss: 1.4303 - val_mse: 3.4366\n",
            "Epoch 45/100\n",
            "380/380 - 1s - loss: 1.3843 - mse: 3.2090 - val_loss: 1.4170 - val_mse: 3.3878\n",
            "Epoch 46/100\n",
            "380/380 - 1s - loss: 1.3694 - mse: 3.1622 - val_loss: 1.4038 - val_mse: 3.3397\n",
            "Epoch 47/100\n",
            "380/380 - 1s - loss: 1.3562 - mse: 3.1157 - val_loss: 1.3907 - val_mse: 3.2923\n",
            "Epoch 48/100\n",
            "380/380 - 1s - loss: 1.3449 - mse: 3.0700 - val_loss: 1.3777 - val_mse: 3.2453\n",
            "Epoch 49/100\n",
            "380/380 - 1s - loss: 1.3325 - mse: 3.0251 - val_loss: 1.3648 - val_mse: 3.1989\n",
            "Epoch 50/100\n",
            "380/380 - 1s - loss: 1.3187 - mse: 2.9802 - val_loss: 1.3520 - val_mse: 3.1532\n",
            "Epoch 51/100\n",
            "380/380 - 1s - loss: 1.3062 - mse: 2.9364 - val_loss: 1.3393 - val_mse: 3.1080\n",
            "Epoch 52/100\n",
            "380/380 - 1s - loss: 1.2928 - mse: 2.8930 - val_loss: 1.3267 - val_mse: 3.0633\n",
            "Epoch 53/100\n",
            "380/380 - 1s - loss: 1.2787 - mse: 2.8504 - val_loss: 1.3143 - val_mse: 3.0194\n",
            "Epoch 54/100\n",
            "380/380 - 1s - loss: 1.2686 - mse: 2.8081 - val_loss: 1.3020 - val_mse: 2.9760\n",
            "Epoch 55/100\n",
            "380/380 - 1s - loss: 1.2561 - mse: 2.7664 - val_loss: 1.2899 - val_mse: 2.9332\n",
            "Epoch 56/100\n",
            "380/380 - 1s - loss: 1.2441 - mse: 2.7252 - val_loss: 1.2779 - val_mse: 2.8909\n",
            "Epoch 57/100\n",
            "380/380 - 1s - loss: 1.2303 - mse: 2.6850 - val_loss: 1.2660 - val_mse: 2.8493\n",
            "Epoch 58/100\n",
            "380/380 - 1s - loss: 1.2188 - mse: 2.6450 - val_loss: 1.2542 - val_mse: 2.8082\n",
            "Epoch 59/100\n",
            "380/380 - 1s - loss: 1.2079 - mse: 2.6057 - val_loss: 1.2425 - val_mse: 2.7677\n",
            "Epoch 60/100\n",
            "380/380 - 1s - loss: 1.1948 - mse: 2.5674 - val_loss: 1.2310 - val_mse: 2.7278\n",
            "Epoch 61/100\n",
            "380/380 - 1s - loss: 1.1824 - mse: 2.5292 - val_loss: 1.2197 - val_mse: 2.6887\n",
            "Epoch 62/100\n",
            "380/380 - 1s - loss: 1.1730 - mse: 2.4918 - val_loss: 1.2085 - val_mse: 2.6500\n",
            "Epoch 63/100\n",
            "380/380 - 1s - loss: 1.1609 - mse: 2.4550 - val_loss: 1.1976 - val_mse: 2.6121\n",
            "Epoch 64/100\n",
            "380/380 - 1s - loss: 1.1524 - mse: 2.4193 - val_loss: 1.1868 - val_mse: 2.5747\n",
            "Epoch 65/100\n",
            "380/380 - 1s - loss: 1.1391 - mse: 2.3834 - val_loss: 1.1761 - val_mse: 2.5380\n",
            "Epoch 66/100\n",
            "380/380 - 1s - loss: 1.1279 - mse: 2.3485 - val_loss: 1.1656 - val_mse: 2.5019\n",
            "Epoch 67/100\n",
            "380/380 - 1s - loss: 1.1186 - mse: 2.3143 - val_loss: 1.1552 - val_mse: 2.4664\n",
            "Epoch 68/100\n",
            "380/380 - 1s - loss: 1.1067 - mse: 2.2805 - val_loss: 1.1450 - val_mse: 2.4315\n",
            "Epoch 69/100\n",
            "380/380 - 1s - loss: 1.0985 - mse: 2.2473 - val_loss: 1.1350 - val_mse: 2.3972\n",
            "Epoch 70/100\n",
            "380/380 - 1s - loss: 1.0878 - mse: 2.2149 - val_loss: 1.1251 - val_mse: 2.3636\n",
            "Epoch 71/100\n",
            "380/380 - 1s - loss: 1.0792 - mse: 2.1832 - val_loss: 1.1154 - val_mse: 2.3306\n",
            "Epoch 72/100\n",
            "380/380 - 1s - loss: 1.0693 - mse: 2.1522 - val_loss: 1.1058 - val_mse: 2.2981\n",
            "Epoch 73/100\n",
            "380/380 - 1s - loss: 1.0580 - mse: 2.1213 - val_loss: 1.0965 - val_mse: 2.2662\n",
            "Epoch 74/100\n",
            "380/380 - 1s - loss: 1.0483 - mse: 2.0911 - val_loss: 1.0873 - val_mse: 2.2348\n",
            "Epoch 75/100\n",
            "380/380 - 1s - loss: 1.0415 - mse: 2.0614 - val_loss: 1.0782 - val_mse: 2.2039\n",
            "Epoch 76/100\n",
            "380/380 - 1s - loss: 1.0303 - mse: 2.0323 - val_loss: 1.0694 - val_mse: 2.1737\n",
            "Epoch 77/100\n",
            "380/380 - 1s - loss: 1.0227 - mse: 2.0039 - val_loss: 1.0607 - val_mse: 2.1439\n",
            "Epoch 78/100\n",
            "380/380 - 1s - loss: 1.0157 - mse: 1.9759 - val_loss: 1.0521 - val_mse: 2.1147\n",
            "Epoch 79/100\n",
            "380/380 - 1s - loss: 1.0045 - mse: 1.9485 - val_loss: 1.0437 - val_mse: 2.0862\n",
            "Epoch 80/100\n",
            "380/380 - 1s - loss: 0.9964 - mse: 1.9216 - val_loss: 1.0355 - val_mse: 2.0581\n",
            "Epoch 81/100\n",
            "380/380 - 1s - loss: 0.9901 - mse: 1.8953 - val_loss: 1.0273 - val_mse: 2.0306\n",
            "Epoch 82/100\n",
            "380/380 - 1s - loss: 0.9808 - mse: 1.8693 - val_loss: 1.0194 - val_mse: 2.0036\n",
            "Epoch 83/100\n",
            "380/380 - 1s - loss: 0.9721 - mse: 1.8441 - val_loss: 1.0117 - val_mse: 1.9771\n",
            "Epoch 84/100\n",
            "380/380 - 1s - loss: 0.9642 - mse: 1.8194 - val_loss: 1.0041 - val_mse: 1.9511\n",
            "Epoch 85/100\n",
            "380/380 - 1s - loss: 0.9594 - mse: 1.7950 - val_loss: 0.9966 - val_mse: 1.9256\n",
            "Epoch 86/100\n",
            "380/380 - 1s - loss: 0.9492 - mse: 1.7712 - val_loss: 0.9893 - val_mse: 1.9006\n",
            "Epoch 87/100\n",
            "380/380 - 1s - loss: 0.9426 - mse: 1.7479 - val_loss: 0.9821 - val_mse: 1.8760\n",
            "Epoch 88/100\n",
            "380/380 - 1s - loss: 0.9371 - mse: 1.7249 - val_loss: 0.9750 - val_mse: 1.8518\n",
            "Epoch 89/100\n",
            "380/380 - 1s - loss: 0.9293 - mse: 1.7022 - val_loss: 0.9680 - val_mse: 1.8282\n",
            "Epoch 90/100\n",
            "380/380 - 1s - loss: 0.9230 - mse: 1.6801 - val_loss: 0.9611 - val_mse: 1.8050\n",
            "Epoch 91/100\n",
            "380/380 - 1s - loss: 0.9145 - mse: 1.6587 - val_loss: 0.9543 - val_mse: 1.7824\n",
            "Epoch 92/100\n",
            "380/380 - 1s - loss: 0.9090 - mse: 1.6373 - val_loss: 0.9476 - val_mse: 1.7601\n",
            "Epoch 93/100\n",
            "380/380 - 1s - loss: 0.9013 - mse: 1.6166 - val_loss: 0.9409 - val_mse: 1.7381\n",
            "Epoch 94/100\n",
            "380/380 - 1s - loss: 0.8949 - mse: 1.5961 - val_loss: 0.9344 - val_mse: 1.7166\n",
            "Epoch 95/100\n",
            "380/380 - 1s - loss: 0.8887 - mse: 1.5761 - val_loss: 0.9280 - val_mse: 1.6954\n",
            "Epoch 96/100\n",
            "380/380 - 1s - loss: 0.8825 - mse: 1.5563 - val_loss: 0.9216 - val_mse: 1.6747\n",
            "Epoch 97/100\n",
            "380/380 - 1s - loss: 0.8797 - mse: 1.5369 - val_loss: 0.9153 - val_mse: 1.6542\n",
            "Epoch 98/100\n",
            "380/380 - 1s - loss: 0.8719 - mse: 1.5181 - val_loss: 0.9092 - val_mse: 1.6342\n",
            "Epoch 99/100\n",
            "380/380 - 1s - loss: 0.8656 - mse: 1.4996 - val_loss: 0.9031 - val_mse: 1.6146\n",
            "Epoch 100/100\n",
            "380/380 - 1s - loss: 0.8589 - mse: 1.4813 - val_loss: 0.8971 - val_mse: 1.5954\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 76%|███████▌  | 16/21 [18:20<05:38, 67.77s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 64)                576       \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "Output (Dense)               (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 8,961\n",
            "Trainable params: 8,961\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/100\n",
            "380/380 - 1s - loss: 1.9893 - mse: 5.2430 - val_loss: 2.0181 - val_mse: 5.4103\n",
            "Epoch 2/100\n",
            "380/380 - 1s - loss: 1.9676 - mse: 5.1683 - val_loss: 1.9990 - val_mse: 5.3360\n",
            "Epoch 3/100\n",
            "380/380 - 1s - loss: 1.9484 - mse: 5.0948 - val_loss: 1.9799 - val_mse: 5.2623\n",
            "Epoch 4/100\n",
            "380/380 - 1s - loss: 1.9280 - mse: 5.0208 - val_loss: 1.9608 - val_mse: 5.1884\n",
            "Epoch 5/100\n",
            "380/380 - 1s - loss: 1.9111 - mse: 4.9465 - val_loss: 1.9417 - val_mse: 5.1149\n",
            "Epoch 6/100\n",
            "380/380 - 1s - loss: 1.8907 - mse: 4.8728 - val_loss: 1.9226 - val_mse: 5.0420\n",
            "Epoch 7/100\n",
            "380/380 - 1s - loss: 1.8702 - mse: 4.8003 - val_loss: 1.9034 - val_mse: 4.9686\n",
            "Epoch 8/100\n",
            "380/380 - 1s - loss: 1.8513 - mse: 4.7273 - val_loss: 1.8842 - val_mse: 4.8948\n",
            "Epoch 9/100\n",
            "380/380 - 1s - loss: 1.8304 - mse: 4.6533 - val_loss: 1.8649 - val_mse: 4.8200\n",
            "Epoch 10/100\n",
            "380/380 - 1s - loss: 1.8135 - mse: 4.5795 - val_loss: 1.8456 - val_mse: 4.7456\n",
            "Epoch 11/100\n",
            "380/380 - 1s - loss: 1.7941 - mse: 4.5046 - val_loss: 1.8262 - val_mse: 4.6701\n",
            "Epoch 12/100\n",
            "380/380 - 1s - loss: 1.7737 - mse: 4.4310 - val_loss: 1.8068 - val_mse: 4.5950\n",
            "Epoch 13/100\n",
            "380/380 - 1s - loss: 1.7536 - mse: 4.3566 - val_loss: 1.7872 - val_mse: 4.5193\n",
            "Epoch 14/100\n",
            "380/380 - 1s - loss: 1.7357 - mse: 4.2810 - val_loss: 1.7676 - val_mse: 4.4426\n",
            "Epoch 15/100\n",
            "380/380 - 1s - loss: 1.7133 - mse: 4.2054 - val_loss: 1.7479 - val_mse: 4.3651\n",
            "Epoch 16/100\n",
            "380/380 - 1s - loss: 1.6941 - mse: 4.1296 - val_loss: 1.7281 - val_mse: 4.2869\n",
            "Epoch 17/100\n",
            "380/380 - 1s - loss: 1.6727 - mse: 4.0527 - val_loss: 1.7081 - val_mse: 4.2091\n",
            "Epoch 18/100\n",
            "380/380 - 1s - loss: 1.6543 - mse: 3.9771 - val_loss: 1.6881 - val_mse: 4.1317\n",
            "Epoch 19/100\n",
            "380/380 - 1s - loss: 1.6328 - mse: 3.9008 - val_loss: 1.6678 - val_mse: 4.0530\n",
            "Epoch 20/100\n",
            "380/380 - 1s - loss: 1.6108 - mse: 3.8250 - val_loss: 1.6474 - val_mse: 3.9744\n",
            "Epoch 21/100\n",
            "380/380 - 1s - loss: 1.5905 - mse: 3.7464 - val_loss: 1.6268 - val_mse: 3.8955\n",
            "Epoch 22/100\n",
            "380/380 - 1s - loss: 1.5708 - mse: 3.6699 - val_loss: 1.6059 - val_mse: 3.8165\n",
            "Epoch 23/100\n",
            "380/380 - 1s - loss: 1.5499 - mse: 3.5930 - val_loss: 1.5849 - val_mse: 3.7367\n",
            "Epoch 24/100\n",
            "380/380 - 1s - loss: 1.5285 - mse: 3.5156 - val_loss: 1.5636 - val_mse: 3.6563\n",
            "Epoch 25/100\n",
            "380/380 - 1s - loss: 1.5070 - mse: 3.4382 - val_loss: 1.5422 - val_mse: 3.5755\n",
            "Epoch 26/100\n",
            "380/380 - 1s - loss: 1.4857 - mse: 3.3592 - val_loss: 1.5205 - val_mse: 3.4943\n",
            "Epoch 27/100\n",
            "380/380 - 1s - loss: 1.4654 - mse: 3.2811 - val_loss: 1.4986 - val_mse: 3.4133\n",
            "Epoch 28/100\n",
            "380/380 - 1s - loss: 1.4417 - mse: 3.2026 - val_loss: 1.4765 - val_mse: 3.3325\n",
            "Epoch 29/100\n",
            "380/380 - 1s - loss: 1.4189 - mse: 3.1259 - val_loss: 1.4543 - val_mse: 3.2517\n",
            "Epoch 30/100\n",
            "380/380 - 1s - loss: 1.3996 - mse: 3.0478 - val_loss: 1.4319 - val_mse: 3.1710\n",
            "Epoch 31/100\n",
            "380/380 - 1s - loss: 1.3761 - mse: 2.9709 - val_loss: 1.4092 - val_mse: 3.0902\n",
            "Epoch 32/100\n",
            "380/380 - 1s - loss: 1.3547 - mse: 2.8937 - val_loss: 1.3863 - val_mse: 3.0100\n",
            "Epoch 33/100\n",
            "380/380 - 1s - loss: 1.3300 - mse: 2.8171 - val_loss: 1.3632 - val_mse: 2.9295\n",
            "Epoch 34/100\n",
            "380/380 - 1s - loss: 1.3093 - mse: 2.7402 - val_loss: 1.3400 - val_mse: 2.8497\n",
            "Epoch 35/100\n",
            "380/380 - 1s - loss: 1.2858 - mse: 2.6638 - val_loss: 1.3168 - val_mse: 2.7707\n",
            "Epoch 36/100\n",
            "380/380 - 1s - loss: 1.2637 - mse: 2.5894 - val_loss: 1.2935 - val_mse: 2.6922\n",
            "Epoch 37/100\n",
            "380/380 - 1s - loss: 1.2406 - mse: 2.5145 - val_loss: 1.2700 - val_mse: 2.6143\n",
            "Epoch 38/100\n",
            "380/380 - 1s - loss: 1.2188 - mse: 2.4411 - val_loss: 1.2466 - val_mse: 2.5375\n",
            "Epoch 39/100\n",
            "380/380 - 1s - loss: 1.1965 - mse: 2.3686 - val_loss: 1.2232 - val_mse: 2.4620\n",
            "Epoch 40/100\n",
            "380/380 - 1s - loss: 1.1711 - mse: 2.2967 - val_loss: 1.1999 - val_mse: 2.3875\n",
            "Epoch 41/100\n",
            "380/380 - 1s - loss: 1.1499 - mse: 2.2275 - val_loss: 1.1766 - val_mse: 2.3139\n",
            "Epoch 42/100\n",
            "380/380 - 1s - loss: 1.1298 - mse: 2.1580 - val_loss: 1.1535 - val_mse: 2.2417\n",
            "Epoch 43/100\n",
            "380/380 - 1s - loss: 1.1064 - mse: 2.0897 - val_loss: 1.1304 - val_mse: 2.1706\n",
            "Epoch 44/100\n",
            "380/380 - 1s - loss: 1.0836 - mse: 2.0228 - val_loss: 1.1072 - val_mse: 2.1008\n",
            "Epoch 45/100\n",
            "380/380 - 1s - loss: 1.0629 - mse: 1.9570 - val_loss: 1.0841 - val_mse: 2.0322\n",
            "Epoch 46/100\n",
            "380/380 - 1s - loss: 1.0389 - mse: 1.8934 - val_loss: 1.0612 - val_mse: 1.9651\n",
            "Epoch 47/100\n",
            "380/380 - 1s - loss: 1.0168 - mse: 1.8295 - val_loss: 1.0384 - val_mse: 1.8994\n",
            "Epoch 48/100\n",
            "380/380 - 1s - loss: 0.9974 - mse: 1.7683 - val_loss: 1.0159 - val_mse: 1.8357\n",
            "Epoch 49/100\n",
            "380/380 - 1s - loss: 0.9762 - mse: 1.7089 - val_loss: 0.9938 - val_mse: 1.7737\n",
            "Epoch 50/100\n",
            "380/380 - 1s - loss: 0.9558 - mse: 1.6506 - val_loss: 0.9722 - val_mse: 1.7139\n",
            "Epoch 51/100\n",
            "380/380 - 1s - loss: 0.9349 - mse: 1.5955 - val_loss: 0.9511 - val_mse: 1.6564\n",
            "Epoch 52/100\n",
            "380/380 - 1s - loss: 0.9149 - mse: 1.5411 - val_loss: 0.9304 - val_mse: 1.6007\n",
            "Epoch 53/100\n",
            "380/380 - 1s - loss: 0.8948 - mse: 1.4894 - val_loss: 0.9102 - val_mse: 1.5472\n",
            "Epoch 54/100\n",
            "380/380 - 1s - loss: 0.8773 - mse: 1.4402 - val_loss: 0.8908 - val_mse: 1.4960\n",
            "Epoch 55/100\n",
            "380/380 - 1s - loss: 0.8597 - mse: 1.3922 - val_loss: 0.8722 - val_mse: 1.4475\n",
            "Epoch 56/100\n",
            "380/380 - 1s - loss: 0.8419 - mse: 1.3470 - val_loss: 0.8545 - val_mse: 1.4013\n",
            "Epoch 57/100\n",
            "380/380 - 1s - loss: 0.8254 - mse: 1.3037 - val_loss: 0.8376 - val_mse: 1.3573\n",
            "Epoch 58/100\n",
            "380/380 - 1s - loss: 0.8091 - mse: 1.2630 - val_loss: 0.8216 - val_mse: 1.3161\n",
            "Epoch 59/100\n",
            "380/380 - 1s - loss: 0.7959 - mse: 1.2245 - val_loss: 0.8064 - val_mse: 1.2771\n",
            "Epoch 60/100\n",
            "380/380 - 1s - loss: 0.7801 - mse: 1.1886 - val_loss: 0.7915 - val_mse: 1.2399\n",
            "Epoch 61/100\n",
            "380/380 - 1s - loss: 0.7664 - mse: 1.1542 - val_loss: 0.7774 - val_mse: 1.2049\n",
            "Epoch 62/100\n",
            "380/380 - 1s - loss: 0.7546 - mse: 1.1209 - val_loss: 0.7641 - val_mse: 1.1723\n",
            "Epoch 63/100\n",
            "380/380 - 1s - loss: 0.7421 - mse: 1.0904 - val_loss: 0.7516 - val_mse: 1.1417\n",
            "Epoch 64/100\n",
            "380/380 - 1s - loss: 0.7314 - mse: 1.0624 - val_loss: 0.7402 - val_mse: 1.1140\n",
            "Epoch 65/100\n",
            "380/380 - 1s - loss: 0.7216 - mse: 1.0365 - val_loss: 0.7294 - val_mse: 1.0877\n",
            "Epoch 66/100\n",
            "380/380 - 1s - loss: 0.7108 - mse: 1.0120 - val_loss: 0.7194 - val_mse: 1.0635\n",
            "Epoch 67/100\n",
            "380/380 - 1s - loss: 0.7031 - mse: 0.9900 - val_loss: 0.7102 - val_mse: 1.0413\n",
            "Epoch 68/100\n",
            "380/380 - 1s - loss: 0.6934 - mse: 0.9693 - val_loss: 0.7017 - val_mse: 1.0209\n",
            "Epoch 69/100\n",
            "380/380 - 1s - loss: 0.6871 - mse: 0.9500 - val_loss: 0.6939 - val_mse: 1.0018\n",
            "Epoch 70/100\n",
            "380/380 - 1s - loss: 0.6791 - mse: 0.9320 - val_loss: 0.6867 - val_mse: 0.9840\n",
            "Epoch 71/100\n",
            "380/380 - 1s - loss: 0.6750 - mse: 0.9154 - val_loss: 0.6800 - val_mse: 0.9674\n",
            "Epoch 72/100\n",
            "380/380 - 1s - loss: 0.6668 - mse: 0.9000 - val_loss: 0.6738 - val_mse: 0.9519\n",
            "Epoch 73/100\n",
            "380/380 - 1s - loss: 0.6612 - mse: 0.8855 - val_loss: 0.6683 - val_mse: 0.9377\n",
            "Epoch 74/100\n",
            "380/380 - 1s - loss: 0.6564 - mse: 0.8726 - val_loss: 0.6633 - val_mse: 0.9249\n",
            "Epoch 75/100\n",
            "380/380 - 1s - loss: 0.6528 - mse: 0.8604 - val_loss: 0.6588 - val_mse: 0.9131\n",
            "Epoch 76/100\n",
            "380/380 - 1s - loss: 0.6475 - mse: 0.8494 - val_loss: 0.6545 - val_mse: 0.9021\n",
            "Epoch 77/100\n",
            "380/380 - 1s - loss: 0.6442 - mse: 0.8393 - val_loss: 0.6506 - val_mse: 0.8919\n",
            "Epoch 78/100\n",
            "380/380 - 1s - loss: 0.6414 - mse: 0.8300 - val_loss: 0.6470 - val_mse: 0.8823\n",
            "Epoch 79/100\n",
            "380/380 - 1s - loss: 0.6368 - mse: 0.8210 - val_loss: 0.6436 - val_mse: 0.8734\n",
            "Epoch 80/100\n",
            "380/380 - 1s - loss: 0.6331 - mse: 0.8133 - val_loss: 0.6404 - val_mse: 0.8651\n",
            "Epoch 81/100\n",
            "380/380 - 1s - loss: 0.6306 - mse: 0.8057 - val_loss: 0.6376 - val_mse: 0.8574\n",
            "Epoch 82/100\n",
            "380/380 - 1s - loss: 0.6281 - mse: 0.7986 - val_loss: 0.6349 - val_mse: 0.8502\n",
            "Epoch 83/100\n",
            "380/380 - 1s - loss: 0.6249 - mse: 0.7920 - val_loss: 0.6323 - val_mse: 0.8431\n",
            "Epoch 84/100\n",
            "380/380 - 1s - loss: 0.6226 - mse: 0.7859 - val_loss: 0.6299 - val_mse: 0.8365\n",
            "Epoch 85/100\n",
            "380/380 - 1s - loss: 0.6219 - mse: 0.7796 - val_loss: 0.6277 - val_mse: 0.8304\n",
            "Epoch 86/100\n",
            "380/380 - 1s - loss: 0.6181 - mse: 0.7741 - val_loss: 0.6255 - val_mse: 0.8244\n",
            "Epoch 87/100\n",
            "380/380 - 1s - loss: 0.6164 - mse: 0.7688 - val_loss: 0.6234 - val_mse: 0.8187\n",
            "Epoch 88/100\n",
            "380/380 - 1s - loss: 0.6146 - mse: 0.7637 - val_loss: 0.6214 - val_mse: 0.8133\n",
            "Epoch 89/100\n",
            "380/380 - 1s - loss: 0.6129 - mse: 0.7588 - val_loss: 0.6195 - val_mse: 0.8080\n",
            "Epoch 90/100\n",
            "380/380 - 1s - loss: 0.6099 - mse: 0.7542 - val_loss: 0.6176 - val_mse: 0.8030\n",
            "Epoch 91/100\n",
            "380/380 - 1s - loss: 0.6085 - mse: 0.7501 - val_loss: 0.6157 - val_mse: 0.7981\n",
            "Epoch 92/100\n",
            "380/380 - 1s - loss: 0.6071 - mse: 0.7456 - val_loss: 0.6140 - val_mse: 0.7933\n",
            "Epoch 93/100\n",
            "380/380 - 1s - loss: 0.6045 - mse: 0.7410 - val_loss: 0.6123 - val_mse: 0.7888\n",
            "Epoch 94/100\n",
            "380/380 - 1s - loss: 0.6029 - mse: 0.7372 - val_loss: 0.6106 - val_mse: 0.7845\n",
            "Epoch 95/100\n",
            "380/380 - 1s - loss: 0.6017 - mse: 0.7335 - val_loss: 0.6090 - val_mse: 0.7802\n",
            "Epoch 96/100\n",
            "380/380 - 1s - loss: 0.6000 - mse: 0.7299 - val_loss: 0.6074 - val_mse: 0.7760\n",
            "Epoch 97/100\n",
            "380/380 - 1s - loss: 0.5992 - mse: 0.7263 - val_loss: 0.6058 - val_mse: 0.7721\n",
            "Epoch 98/100\n",
            "380/380 - 1s - loss: 0.5983 - mse: 0.7224 - val_loss: 0.6042 - val_mse: 0.7679\n",
            "Epoch 99/100\n",
            "380/380 - 1s - loss: 0.5954 - mse: 0.7187 - val_loss: 0.6027 - val_mse: 0.7639\n",
            "Epoch 100/100\n",
            "380/380 - 1s - loss: 0.5941 - mse: 0.7154 - val_loss: 0.6012 - val_mse: 0.7601\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 81%|████████  | 17/21 [19:40<04:46, 71.54s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 64)                576       \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "Output (Dense)               (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 8,961\n",
            "Trainable params: 8,961\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/100\n",
            "380/380 - 1s - loss: 6.4474 - mse: 6.4454 - val_loss: 6.5201 - val_mse: 6.5201\n",
            "Epoch 2/100\n",
            "380/380 - 1s - loss: 6.1482 - mse: 6.1523 - val_loss: 6.2326 - val_mse: 6.2327\n",
            "Epoch 3/100\n",
            "380/380 - 1s - loss: 5.8707 - mse: 5.8753 - val_loss: 5.9570 - val_mse: 5.9573\n",
            "Epoch 4/100\n",
            "380/380 - 1s - loss: 5.6019 - mse: 5.6098 - val_loss: 5.6916 - val_mse: 5.6920\n",
            "Epoch 5/100\n",
            "380/380 - 1s - loss: 5.3529 - mse: 5.3549 - val_loss: 5.4367 - val_mse: 5.4372\n",
            "Epoch 6/100\n",
            "380/380 - 1s - loss: 5.1065 - mse: 5.1087 - val_loss: 5.1908 - val_mse: 5.1914\n",
            "Epoch 7/100\n",
            "380/380 - 1s - loss: 4.8644 - mse: 4.8702 - val_loss: 4.9524 - val_mse: 4.9532\n",
            "Epoch 8/100\n",
            "380/380 - 1s - loss: 4.6366 - mse: 4.6393 - val_loss: 4.7213 - val_mse: 4.7222\n",
            "Epoch 9/100\n",
            "380/380 - 1s - loss: 4.4089 - mse: 4.4151 - val_loss: 4.4957 - val_mse: 4.4967\n",
            "Epoch 10/100\n",
            "380/380 - 1s - loss: 4.1986 - mse: 4.1973 - val_loss: 4.2777 - val_mse: 4.2788\n",
            "Epoch 11/100\n",
            "380/380 - 1s - loss: 3.9926 - mse: 3.9865 - val_loss: 4.0672 - val_mse: 4.0684\n",
            "Epoch 12/100\n",
            "380/380 - 1s - loss: 3.7842 - mse: 3.7835 - val_loss: 3.8646 - val_mse: 3.8659\n",
            "Epoch 13/100\n",
            "380/380 - 1s - loss: 3.5847 - mse: 3.5882 - val_loss: 3.6710 - val_mse: 3.6723\n",
            "Epoch 14/100\n",
            "380/380 - 1s - loss: 3.4089 - mse: 3.4011 - val_loss: 3.4856 - val_mse: 3.4871\n",
            "Epoch 15/100\n",
            "380/380 - 1s - loss: 3.2170 - mse: 3.2223 - val_loss: 3.3089 - val_mse: 3.3105\n",
            "Epoch 16/100\n",
            "380/380 - 1s - loss: 3.0499 - mse: 3.0520 - val_loss: 3.1415 - val_mse: 3.1431\n",
            "Epoch 17/100\n",
            "380/380 - 1s - loss: 2.8895 - mse: 2.8910 - val_loss: 2.9840 - val_mse: 2.9857\n",
            "Epoch 18/100\n",
            "380/380 - 1s - loss: 2.7389 - mse: 2.7400 - val_loss: 2.8367 - val_mse: 2.8383\n",
            "Epoch 19/100\n",
            "380/380 - 1s - loss: 2.5972 - mse: 2.5990 - val_loss: 2.6994 - val_mse: 2.7010\n",
            "Epoch 20/100\n",
            "380/380 - 1s - loss: 2.4631 - mse: 2.4674 - val_loss: 2.5715 - val_mse: 2.5731\n",
            "Epoch 21/100\n",
            "380/380 - 1s - loss: 2.3415 - mse: 2.3456 - val_loss: 2.4534 - val_mse: 2.4551\n",
            "Epoch 22/100\n",
            "380/380 - 1s - loss: 2.2306 - mse: 2.2328 - val_loss: 2.3442 - val_mse: 2.3459\n",
            "Epoch 23/100\n",
            "380/380 - 1s - loss: 2.1321 - mse: 2.1293 - val_loss: 2.2443 - val_mse: 2.2459\n",
            "Epoch 24/100\n",
            "380/380 - 1s - loss: 2.0372 - mse: 2.0349 - val_loss: 2.1530 - val_mse: 2.1547\n",
            "Epoch 25/100\n",
            "380/380 - 1s - loss: 1.9474 - mse: 1.9487 - val_loss: 2.0688 - val_mse: 2.0704\n",
            "Epoch 26/100\n",
            "380/380 - 1s - loss: 1.8676 - mse: 1.8704 - val_loss: 1.9946 - val_mse: 1.9961\n",
            "Epoch 27/100\n",
            "380/380 - 1s - loss: 1.8026 - mse: 1.7996 - val_loss: 1.9251 - val_mse: 1.9266\n",
            "Epoch 28/100\n",
            "380/380 - 1s - loss: 1.7361 - mse: 1.7349 - val_loss: 1.8632 - val_mse: 1.8646\n",
            "Epoch 29/100\n",
            "380/380 - 1s - loss: 1.6753 - mse: 1.6766 - val_loss: 1.8059 - val_mse: 1.8073\n",
            "Epoch 30/100\n",
            "380/380 - 1s - loss: 1.6248 - mse: 1.6232 - val_loss: 1.7537 - val_mse: 1.7551\n",
            "Epoch 31/100\n",
            "380/380 - 1s - loss: 1.5732 - mse: 1.5740 - val_loss: 1.7055 - val_mse: 1.7068\n",
            "Epoch 32/100\n",
            "380/380 - 1s - loss: 1.5298 - mse: 1.5287 - val_loss: 1.6607 - val_mse: 1.6620\n",
            "Epoch 33/100\n",
            "380/380 - 1s - loss: 1.4846 - mse: 1.4865 - val_loss: 1.6186 - val_mse: 1.6199\n",
            "Epoch 34/100\n",
            "380/380 - 1s - loss: 1.4476 - mse: 1.4475 - val_loss: 1.5798 - val_mse: 1.5811\n",
            "Epoch 35/100\n",
            "380/380 - 1s - loss: 1.4109 - mse: 1.4115 - val_loss: 1.5433 - val_mse: 1.5446\n",
            "Epoch 36/100\n",
            "380/380 - 1s - loss: 1.3758 - mse: 1.3776 - val_loss: 1.5089 - val_mse: 1.5102\n",
            "Epoch 37/100\n",
            "380/380 - 1s - loss: 1.3444 - mse: 1.3457 - val_loss: 1.4765 - val_mse: 1.4777\n",
            "Epoch 38/100\n",
            "380/380 - 1s - loss: 1.3180 - mse: 1.3155 - val_loss: 1.4458 - val_mse: 1.4470\n",
            "Epoch 39/100\n",
            "380/380 - 1s - loss: 1.2897 - mse: 1.2870 - val_loss: 1.4169 - val_mse: 1.4181\n",
            "Epoch 40/100\n",
            "380/380 - 1s - loss: 1.2587 - mse: 1.2604 - val_loss: 1.3898 - val_mse: 1.3909\n",
            "Epoch 41/100\n",
            "380/380 - 1s - loss: 1.2334 - mse: 1.2353 - val_loss: 1.3640 - val_mse: 1.3651\n",
            "Epoch 42/100\n",
            "380/380 - 1s - loss: 1.2143 - mse: 1.2111 - val_loss: 1.3391 - val_mse: 1.3402\n",
            "Epoch 43/100\n",
            "380/380 - 1s - loss: 1.1889 - mse: 1.1878 - val_loss: 1.3152 - val_mse: 1.3164\n",
            "Epoch 44/100\n",
            "380/380 - 1s - loss: 1.1638 - mse: 1.1657 - val_loss: 1.2925 - val_mse: 1.2937\n",
            "Epoch 45/100\n",
            "380/380 - 1s - loss: 1.1473 - mse: 1.1446 - val_loss: 1.2708 - val_mse: 1.2720\n",
            "Epoch 46/100\n",
            "380/380 - 1s - loss: 1.1229 - mse: 1.1243 - val_loss: 1.2499 - val_mse: 1.2511\n",
            "Epoch 47/100\n",
            "380/380 - 1s - loss: 1.1034 - mse: 1.1048 - val_loss: 1.2299 - val_mse: 1.2311\n",
            "Epoch 48/100\n",
            "380/380 - 1s - loss: 1.0857 - mse: 1.0863 - val_loss: 1.2109 - val_mse: 1.2121\n",
            "Epoch 49/100\n",
            "380/380 - 1s - loss: 1.0693 - mse: 1.0685 - val_loss: 1.1928 - val_mse: 1.1940\n",
            "Epoch 50/100\n",
            "380/380 - 1s - loss: 1.0532 - mse: 1.0515 - val_loss: 1.1753 - val_mse: 1.1765\n",
            "Epoch 51/100\n",
            "380/380 - 1s - loss: 1.0340 - mse: 1.0352 - val_loss: 1.1586 - val_mse: 1.1598\n",
            "Epoch 52/100\n",
            "380/380 - 1s - loss: 1.0179 - mse: 1.0197 - val_loss: 1.1428 - val_mse: 1.1440\n",
            "Epoch 53/100\n",
            "380/380 - 1s - loss: 1.0036 - mse: 1.0049 - val_loss: 1.1278 - val_mse: 1.1290\n",
            "Epoch 54/100\n",
            "380/380 - 1s - loss: 0.9906 - mse: 0.9908 - val_loss: 1.1130 - val_mse: 1.1143\n",
            "Epoch 55/100\n",
            "380/380 - 1s - loss: 0.9760 - mse: 0.9772 - val_loss: 1.0992 - val_mse: 1.1005\n",
            "Epoch 56/100\n",
            "380/380 - 1s - loss: 0.9625 - mse: 0.9642 - val_loss: 1.0861 - val_mse: 1.0874\n",
            "Epoch 57/100\n",
            "380/380 - 1s - loss: 0.9505 - mse: 0.9519 - val_loss: 1.0737 - val_mse: 1.0750\n",
            "Epoch 58/100\n",
            "380/380 - 1s - loss: 0.9391 - mse: 0.9403 - val_loss: 1.0619 - val_mse: 1.0632\n",
            "Epoch 59/100\n",
            "380/380 - 1s - loss: 0.9302 - mse: 0.9292 - val_loss: 1.0509 - val_mse: 1.0522\n",
            "Epoch 60/100\n",
            "380/380 - 1s - loss: 0.9171 - mse: 0.9186 - val_loss: 1.0401 - val_mse: 1.0414\n",
            "Epoch 61/100\n",
            "380/380 - 1s - loss: 0.9072 - mse: 0.9085 - val_loss: 1.0296 - val_mse: 1.0310\n",
            "Epoch 62/100\n",
            "380/380 - 1s - loss: 0.8985 - mse: 0.8990 - val_loss: 1.0201 - val_mse: 1.0215\n",
            "Epoch 63/100\n",
            "380/380 - 1s - loss: 0.8895 - mse: 0.8900 - val_loss: 1.0110 - val_mse: 1.0124\n",
            "Epoch 64/100\n",
            "380/380 - 1s - loss: 0.8803 - mse: 0.8815 - val_loss: 1.0026 - val_mse: 1.0040\n",
            "Epoch 65/100\n",
            "380/380 - 1s - loss: 0.8739 - mse: 0.8734 - val_loss: 0.9943 - val_mse: 0.9957\n",
            "Epoch 66/100\n",
            "380/380 - 1s - loss: 0.8660 - mse: 0.8657 - val_loss: 0.9867 - val_mse: 0.9881\n",
            "Epoch 67/100\n",
            "380/380 - 1s - loss: 0.8587 - mse: 0.8584 - val_loss: 0.9792 - val_mse: 0.9806\n",
            "Epoch 68/100\n",
            "380/380 - 1s - loss: 0.8501 - mse: 0.8513 - val_loss: 0.9720 - val_mse: 0.9734\n",
            "Epoch 69/100\n",
            "380/380 - 1s - loss: 0.8452 - mse: 0.8447 - val_loss: 0.9652 - val_mse: 0.9666\n",
            "Epoch 70/100\n",
            "380/380 - 1s - loss: 0.8368 - mse: 0.8383 - val_loss: 0.9588 - val_mse: 0.9602\n",
            "Epoch 71/100\n",
            "380/380 - 1s - loss: 0.8340 - mse: 0.8323 - val_loss: 0.9525 - val_mse: 0.9539\n",
            "Epoch 72/100\n",
            "380/380 - 1s - loss: 0.8256 - mse: 0.8266 - val_loss: 0.9467 - val_mse: 0.9481\n",
            "Epoch 73/100\n",
            "380/380 - 1s - loss: 0.8202 - mse: 0.8211 - val_loss: 0.9409 - val_mse: 0.9423\n",
            "Epoch 74/100\n",
            "380/380 - 1s - loss: 0.8148 - mse: 0.8158 - val_loss: 0.9354 - val_mse: 0.9368\n",
            "Epoch 75/100\n",
            "380/380 - 1s - loss: 0.8114 - mse: 0.8107 - val_loss: 0.9301 - val_mse: 0.9315\n",
            "Epoch 76/100\n",
            "380/380 - 1s - loss: 0.8054 - mse: 0.8059 - val_loss: 0.9249 - val_mse: 0.9263\n",
            "Epoch 77/100\n",
            "380/380 - 1s - loss: 0.8007 - mse: 0.8012 - val_loss: 0.9198 - val_mse: 0.9213\n",
            "Epoch 78/100\n",
            "380/380 - 1s - loss: 0.7994 - mse: 0.7966 - val_loss: 0.9148 - val_mse: 0.9162\n",
            "Epoch 79/100\n",
            "380/380 - 1s - loss: 0.7916 - mse: 0.7921 - val_loss: 0.9101 - val_mse: 0.9115\n",
            "Epoch 80/100\n",
            "380/380 - 1s - loss: 0.7867 - mse: 0.7879 - val_loss: 0.9055 - val_mse: 0.9070\n",
            "Epoch 81/100\n",
            "380/380 - 1s - loss: 0.7841 - mse: 0.7837 - val_loss: 0.9011 - val_mse: 0.9025\n",
            "Epoch 82/100\n",
            "380/380 - 1s - loss: 0.7787 - mse: 0.7797 - val_loss: 0.8965 - val_mse: 0.8980\n",
            "Epoch 83/100\n",
            "380/380 - 1s - loss: 0.7747 - mse: 0.7757 - val_loss: 0.8921 - val_mse: 0.8935\n",
            "Epoch 84/100\n",
            "380/380 - 1s - loss: 0.7706 - mse: 0.7718 - val_loss: 0.8877 - val_mse: 0.8892\n",
            "Epoch 85/100\n",
            "380/380 - 1s - loss: 0.7708 - mse: 0.7680 - val_loss: 0.8835 - val_mse: 0.8850\n",
            "Epoch 86/100\n",
            "380/380 - 1s - loss: 0.7631 - mse: 0.7643 - val_loss: 0.8795 - val_mse: 0.8809\n",
            "Epoch 87/100\n",
            "380/380 - 1s - loss: 0.7599 - mse: 0.7607 - val_loss: 0.8752 - val_mse: 0.8767\n",
            "Epoch 88/100\n",
            "380/380 - 1s - loss: 0.7588 - mse: 0.7572 - val_loss: 0.8712 - val_mse: 0.8726\n",
            "Epoch 89/100\n",
            "380/380 - 1s - loss: 0.7539 - mse: 0.7537 - val_loss: 0.8673 - val_mse: 0.8688\n",
            "Epoch 90/100\n",
            "380/380 - 1s - loss: 0.7494 - mse: 0.7504 - val_loss: 0.8634 - val_mse: 0.8649\n",
            "Epoch 91/100\n",
            "380/380 - 1s - loss: 0.7461 - mse: 0.7471 - val_loss: 0.8595 - val_mse: 0.8609\n",
            "Epoch 92/100\n",
            "380/380 - 1s - loss: 0.7435 - mse: 0.7439 - val_loss: 0.8560 - val_mse: 0.8575\n",
            "Epoch 93/100\n",
            "380/380 - 1s - loss: 0.7394 - mse: 0.7407 - val_loss: 0.8517 - val_mse: 0.8532\n",
            "Epoch 94/100\n",
            "380/380 - 1s - loss: 0.7365 - mse: 0.7375 - val_loss: 0.8480 - val_mse: 0.8495\n",
            "Epoch 95/100\n",
            "380/380 - 1s - loss: 0.7334 - mse: 0.7345 - val_loss: 0.8443 - val_mse: 0.8458\n",
            "Epoch 96/100\n",
            "380/380 - 1s - loss: 0.7310 - mse: 0.7316 - val_loss: 0.8406 - val_mse: 0.8420\n",
            "Epoch 97/100\n",
            "380/380 - 1s - loss: 0.7291 - mse: 0.7286 - val_loss: 0.8370 - val_mse: 0.8384\n",
            "Epoch 98/100\n",
            "380/380 - 1s - loss: 0.7293 - mse: 0.7257 - val_loss: 0.8333 - val_mse: 0.8347\n",
            "Epoch 99/100\n",
            "380/380 - 1s - loss: 0.7217 - mse: 0.7228 - val_loss: 0.8297 - val_mse: 0.8311\n",
            "Epoch 100/100\n",
            "380/380 - 1s - loss: 0.7190 - mse: 0.7200 - val_loss: 0.8261 - val_mse: 0.8275\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 86%|████████▌ | 18/21 [20:59<03:41, 73.68s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 64)                576       \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "Output (Dense)               (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 4,801\n",
            "Trainable params: 4,801\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/100\n",
            "380/380 - 1s - loss: 4.5606 - mse: 4.5565 - val_loss: 4.6970 - val_mse: 4.6976\n",
            "Epoch 2/100\n",
            "380/380 - 1s - loss: 4.4684 - mse: 4.4718 - val_loss: 4.6102 - val_mse: 4.6108\n",
            "Epoch 3/100\n",
            "380/380 - 1s - loss: 4.3846 - mse: 4.3882 - val_loss: 4.5250 - val_mse: 4.5257\n",
            "Epoch 4/100\n",
            "380/380 - 1s - loss: 4.3000 - mse: 4.3062 - val_loss: 4.4414 - val_mse: 4.4421\n",
            "Epoch 5/100\n",
            "380/380 - 1s - loss: 4.2243 - mse: 4.2256 - val_loss: 4.3593 - val_mse: 4.3600\n",
            "Epoch 6/100\n",
            "380/380 - 1s - loss: 4.1444 - mse: 4.1464 - val_loss: 4.2788 - val_mse: 4.2796\n",
            "Epoch 7/100\n",
            "380/380 - 1s - loss: 4.0637 - mse: 4.0686 - val_loss: 4.1996 - val_mse: 4.2003\n",
            "Epoch 8/100\n",
            "380/380 - 1s - loss: 3.9895 - mse: 3.9921 - val_loss: 4.1215 - val_mse: 4.1223\n",
            "Epoch 9/100\n",
            "380/380 - 1s - loss: 3.9108 - mse: 3.9167 - val_loss: 4.0444 - val_mse: 4.0452\n",
            "Epoch 10/100\n",
            "380/380 - 1s - loss: 3.8437 - mse: 3.8426 - val_loss: 3.9689 - val_mse: 3.9697\n",
            "Epoch 11/100\n",
            "380/380 - 1s - loss: 3.7752 - mse: 3.7696 - val_loss: 3.8945 - val_mse: 3.8953\n",
            "Epoch 12/100\n",
            "380/380 - 1s - loss: 3.6977 - mse: 3.6978 - val_loss: 3.8210 - val_mse: 3.8219\n",
            "Epoch 13/100\n",
            "380/380 - 1s - loss: 3.6244 - mse: 3.6271 - val_loss: 3.7491 - val_mse: 3.7500\n",
            "Epoch 14/100\n",
            "380/380 - 1s - loss: 3.5670 - mse: 3.5576 - val_loss: 3.6780 - val_mse: 3.6789\n",
            "Epoch 15/100\n",
            "380/380 - 1s - loss: 3.4841 - mse: 3.4891 - val_loss: 3.6080 - val_mse: 3.6090\n",
            "Epoch 16/100\n",
            "380/380 - 1s - loss: 3.4199 - mse: 3.4218 - val_loss: 3.5393 - val_mse: 3.5403\n",
            "Epoch 17/100\n",
            "380/380 - 1s - loss: 3.3530 - mse: 3.3556 - val_loss: 3.4716 - val_mse: 3.4726\n",
            "Epoch 18/100\n",
            "380/380 - 1s - loss: 3.2895 - mse: 3.2906 - val_loss: 3.4052 - val_mse: 3.4063\n",
            "Epoch 19/100\n",
            "380/380 - 1s - loss: 3.2258 - mse: 3.2267 - val_loss: 3.3401 - val_mse: 3.3412\n",
            "Epoch 20/100\n",
            "380/380 - 1s - loss: 3.1592 - mse: 3.1639 - val_loss: 3.2759 - val_mse: 3.2770\n",
            "Epoch 21/100\n",
            "380/380 - 1s - loss: 3.0972 - mse: 3.1020 - val_loss: 3.2129 - val_mse: 3.2140\n",
            "Epoch 22/100\n",
            "380/380 - 1s - loss: 3.0380 - mse: 3.0411 - val_loss: 3.1509 - val_mse: 3.1520\n",
            "Epoch 23/100\n",
            "380/380 - 1s - loss: 2.9816 - mse: 2.9811 - val_loss: 3.0898 - val_mse: 3.0910\n",
            "Epoch 24/100\n",
            "380/380 - 1s - loss: 2.9253 - mse: 2.9222 - val_loss: 3.0299 - val_mse: 3.0311\n",
            "Epoch 25/100\n",
            "380/380 - 1s - loss: 2.8632 - mse: 2.8645 - val_loss: 2.9707 - val_mse: 2.9720\n",
            "Epoch 26/100\n",
            "380/380 - 1s - loss: 2.8053 - mse: 2.8078 - val_loss: 2.9135 - val_mse: 2.9148\n",
            "Epoch 27/100\n",
            "380/380 - 1s - loss: 2.7586 - mse: 2.7524 - val_loss: 2.8564 - val_mse: 2.8577\n",
            "Epoch 28/100\n",
            "380/380 - 1s - loss: 2.6989 - mse: 2.6979 - val_loss: 2.8015 - val_mse: 2.8029\n",
            "Epoch 29/100\n",
            "380/380 - 1s - loss: 2.6414 - mse: 2.6448 - val_loss: 2.7472 - val_mse: 2.7486\n",
            "Epoch 30/100\n",
            "380/380 - 1s - loss: 2.5965 - mse: 2.5927 - val_loss: 2.6940 - val_mse: 2.6954\n",
            "Epoch 31/100\n",
            "380/380 - 1s - loss: 2.5406 - mse: 2.5414 - val_loss: 2.6418 - val_mse: 2.6433\n",
            "Epoch 32/100\n",
            "380/380 - 1s - loss: 2.4941 - mse: 2.4912 - val_loss: 2.5907 - val_mse: 2.5921\n",
            "Epoch 33/100\n",
            "380/380 - 1s - loss: 2.4389 - mse: 2.4419 - val_loss: 2.5402 - val_mse: 2.5417\n",
            "Epoch 34/100\n",
            "380/380 - 1s - loss: 2.3932 - mse: 2.3936 - val_loss: 2.4910 - val_mse: 2.4925\n",
            "Epoch 35/100\n",
            "380/380 - 1s - loss: 2.3448 - mse: 2.3464 - val_loss: 2.4428 - val_mse: 2.4443\n",
            "Epoch 36/100\n",
            "380/380 - 1s - loss: 2.2980 - mse: 2.3002 - val_loss: 2.3954 - val_mse: 2.3969\n",
            "Epoch 37/100\n",
            "380/380 - 1s - loss: 2.2542 - mse: 2.2548 - val_loss: 2.3490 - val_mse: 2.3505\n",
            "Epoch 38/100\n",
            "380/380 - 1s - loss: 2.2137 - mse: 2.2103 - val_loss: 2.3036 - val_mse: 2.3052\n",
            "Epoch 39/100\n",
            "380/380 - 1s - loss: 2.1698 - mse: 2.1669 - val_loss: 2.2595 - val_mse: 2.2611\n",
            "Epoch 40/100\n",
            "380/380 - 1s - loss: 2.1210 - mse: 2.1249 - val_loss: 2.2165 - val_mse: 2.2181\n",
            "Epoch 41/100\n",
            "380/380 - 1s - loss: 2.0812 - mse: 2.0840 - val_loss: 2.1745 - val_mse: 2.1761\n",
            "Epoch 42/100\n",
            "380/380 - 1s - loss: 2.0486 - mse: 2.0438 - val_loss: 2.1333 - val_mse: 2.1349\n",
            "Epoch 43/100\n",
            "380/380 - 1s - loss: 2.0056 - mse: 2.0044 - val_loss: 2.0931 - val_mse: 2.0947\n",
            "Epoch 44/100\n",
            "380/380 - 1s - loss: 1.9644 - mse: 1.9661 - val_loss: 2.0538 - val_mse: 2.0554\n",
            "Epoch 45/100\n",
            "380/380 - 1s - loss: 1.9334 - mse: 1.9288 - val_loss: 2.0155 - val_mse: 2.0171\n",
            "Epoch 46/100\n",
            "380/380 - 1s - loss: 1.8904 - mse: 1.8923 - val_loss: 1.9781 - val_mse: 1.9797\n",
            "Epoch 47/100\n",
            "380/380 - 1s - loss: 1.8538 - mse: 1.8567 - val_loss: 1.9416 - val_mse: 1.9433\n",
            "Epoch 48/100\n",
            "380/380 - 1s - loss: 1.8230 - mse: 1.8222 - val_loss: 1.9062 - val_mse: 1.9079\n",
            "Epoch 49/100\n",
            "380/380 - 1s - loss: 1.7897 - mse: 1.7885 - val_loss: 1.8717 - val_mse: 1.8734\n",
            "Epoch 50/100\n",
            "380/380 - 1s - loss: 1.7584 - mse: 1.7556 - val_loss: 1.8379 - val_mse: 1.8396\n",
            "Epoch 51/100\n",
            "380/380 - 1s - loss: 1.7233 - mse: 1.7236 - val_loss: 1.8051 - val_mse: 1.8068\n",
            "Epoch 52/100\n",
            "380/380 - 1s - loss: 1.6909 - mse: 1.6925 - val_loss: 1.7733 - val_mse: 1.7750\n",
            "Epoch 53/100\n",
            "380/380 - 1s - loss: 1.6595 - mse: 1.6624 - val_loss: 1.7423 - val_mse: 1.7441\n",
            "Epoch 54/100\n",
            "380/380 - 1s - loss: 1.6343 - mse: 1.6330 - val_loss: 1.7122 - val_mse: 1.7139\n",
            "Epoch 55/100\n",
            "380/380 - 1s - loss: 1.6037 - mse: 1.6043 - val_loss: 1.6828 - val_mse: 1.6846\n",
            "Epoch 56/100\n",
            "380/380 - 1s - loss: 1.5754 - mse: 1.5765 - val_loss: 1.6544 - val_mse: 1.6562\n",
            "Epoch 57/100\n",
            "380/380 - 1s - loss: 1.5475 - mse: 1.5496 - val_loss: 1.6270 - val_mse: 1.6287\n",
            "Epoch 58/100\n",
            "380/380 - 1s - loss: 1.5219 - mse: 1.5235 - val_loss: 1.6004 - val_mse: 1.6021\n",
            "Epoch 59/100\n",
            "380/380 - 1s - loss: 1.4993 - mse: 1.4982 - val_loss: 1.5744 - val_mse: 1.5761\n",
            "Epoch 60/100\n",
            "380/380 - 1s - loss: 1.4713 - mse: 1.4736 - val_loss: 1.5492 - val_mse: 1.5509\n",
            "Epoch 61/100\n",
            "380/380 - 1s - loss: 1.4472 - mse: 1.4498 - val_loss: 1.5248 - val_mse: 1.5266\n",
            "Epoch 62/100\n",
            "380/380 - 1s - loss: 1.4276 - mse: 1.4268 - val_loss: 1.5016 - val_mse: 1.5034\n",
            "Epoch 63/100\n",
            "380/380 - 1s - loss: 1.4033 - mse: 1.4045 - val_loss: 1.4788 - val_mse: 1.4805\n",
            "Epoch 64/100\n",
            "380/380 - 1s - loss: 1.3843 - mse: 1.3830 - val_loss: 1.4569 - val_mse: 1.4586\n",
            "Epoch 65/100\n",
            "380/380 - 1s - loss: 1.3614 - mse: 1.3621 - val_loss: 1.4354 - val_mse: 1.4372\n",
            "Epoch 66/100\n",
            "380/380 - 1s - loss: 1.3403 - mse: 1.3419 - val_loss: 1.4149 - val_mse: 1.4167\n",
            "Epoch 67/100\n",
            "380/380 - 1s - loss: 1.3230 - mse: 1.3225 - val_loss: 1.3951 - val_mse: 1.3969\n",
            "Epoch 68/100\n",
            "380/380 - 1s - loss: 1.3016 - mse: 1.3037 - val_loss: 1.3760 - val_mse: 1.3777\n",
            "Epoch 69/100\n",
            "380/380 - 1s - loss: 1.2867 - mse: 1.2855 - val_loss: 1.3575 - val_mse: 1.3593\n",
            "Epoch 70/100\n",
            "380/380 - 1s - loss: 1.2664 - mse: 1.2680 - val_loss: 1.3397 - val_mse: 1.3414\n",
            "Epoch 71/100\n",
            "380/380 - 1s - loss: 1.2629 - mse: 1.2511 - val_loss: 1.3226 - val_mse: 1.3243\n",
            "Epoch 72/100\n",
            "380/380 - 1s - loss: 1.2355 - mse: 1.2350 - val_loss: 1.3062 - val_mse: 1.3079\n",
            "Epoch 73/100\n",
            "380/380 - 1s - loss: 1.2184 - mse: 1.2194 - val_loss: 1.2904 - val_mse: 1.2921\n",
            "Epoch 74/100\n",
            "380/380 - 1s - loss: 1.2027 - mse: 1.2045 - val_loss: 1.2753 - val_mse: 1.2770\n",
            "Epoch 75/100\n",
            "380/380 - 1s - loss: 1.1927 - mse: 1.1899 - val_loss: 1.2606 - val_mse: 1.2623\n",
            "Epoch 76/100\n",
            "380/380 - 1s - loss: 1.1748 - mse: 1.1760 - val_loss: 1.2463 - val_mse: 1.2480\n",
            "Epoch 77/100\n",
            "380/380 - 1s - loss: 1.1629 - mse: 1.1625 - val_loss: 1.2327 - val_mse: 1.2344\n",
            "Epoch 78/100\n",
            "380/380 - 1s - loss: 1.1523 - mse: 1.1495 - val_loss: 1.2196 - val_mse: 1.2213\n",
            "Epoch 79/100\n",
            "380/380 - 1s - loss: 1.1353 - mse: 1.1369 - val_loss: 1.2070 - val_mse: 1.2087\n",
            "Epoch 80/100\n",
            "380/380 - 1s - loss: 1.1234 - mse: 1.1250 - val_loss: 1.1949 - val_mse: 1.1966\n",
            "Epoch 81/100\n",
            "380/380 - 1s - loss: 1.1131 - mse: 1.1134 - val_loss: 1.1833 - val_mse: 1.1850\n",
            "Epoch 82/100\n",
            "380/380 - 1s - loss: 1.1013 - mse: 1.1022 - val_loss: 1.1721 - val_mse: 1.1737\n",
            "Epoch 83/100\n",
            "380/380 - 1s - loss: 1.0899 - mse: 1.0915 - val_loss: 1.1612 - val_mse: 1.1628\n",
            "Epoch 84/100\n",
            "380/380 - 1s - loss: 1.0795 - mse: 1.0811 - val_loss: 1.1508 - val_mse: 1.1525\n",
            "Epoch 85/100\n",
            "380/380 - 1s - loss: 1.0746 - mse: 1.0712 - val_loss: 1.1409 - val_mse: 1.1426\n",
            "Epoch 86/100\n",
            "380/380 - 1s - loss: 1.0597 - mse: 1.0616 - val_loss: 1.1312 - val_mse: 1.1329\n",
            "Epoch 87/100\n",
            "380/380 - 1s - loss: 1.0514 - mse: 1.0523 - val_loss: 1.1221 - val_mse: 1.1237\n",
            "Epoch 88/100\n",
            "380/380 - 1s - loss: 1.0451 - mse: 1.0434 - val_loss: 1.1133 - val_mse: 1.1149\n",
            "Epoch 89/100\n",
            "380/380 - 1s - loss: 1.0348 - mse: 1.0348 - val_loss: 1.1047 - val_mse: 1.1063\n",
            "Epoch 90/100\n",
            "380/380 - 1s - loss: 1.0258 - mse: 1.0266 - val_loss: 1.0966 - val_mse: 1.0982\n",
            "Epoch 91/100\n",
            "380/380 - 1s - loss: 1.0172 - mse: 1.0187 - val_loss: 1.0887 - val_mse: 1.0903\n",
            "Epoch 92/100\n",
            "380/380 - 1s - loss: 1.0105 - mse: 1.0111 - val_loss: 1.0812 - val_mse: 1.0828\n",
            "Epoch 93/100\n",
            "380/380 - 1s - loss: 1.0021 - mse: 1.0037 - val_loss: 1.0740 - val_mse: 1.0755\n",
            "Epoch 94/100\n",
            "380/380 - 1s - loss: 0.9954 - mse: 0.9967 - val_loss: 1.0671 - val_mse: 1.0686\n",
            "Epoch 95/100\n",
            "380/380 - 1s - loss: 0.9883 - mse: 0.9900 - val_loss: 1.0604 - val_mse: 1.0619\n",
            "Epoch 96/100\n",
            "380/380 - 1s - loss: 0.9827 - mse: 0.9834 - val_loss: 1.0539 - val_mse: 1.0555\n",
            "Epoch 97/100\n",
            "380/380 - 1s - loss: 0.9782 - mse: 0.9771 - val_loss: 1.0477 - val_mse: 1.0493\n",
            "Epoch 98/100\n",
            "380/380 - 1s - loss: 0.9739 - mse: 0.9709 - val_loss: 1.0416 - val_mse: 1.0431\n",
            "Epoch 99/100\n",
            "380/380 - 1s - loss: 0.9637 - mse: 0.9650 - val_loss: 1.0357 - val_mse: 1.0373\n",
            "Epoch 100/100\n",
            "380/380 - 1s - loss: 0.9576 - mse: 0.9592 - val_loss: 1.0301 - val_mse: 1.0317\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 90%|█████████ | 19/21 [22:11<02:26, 73.26s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 64)                576       \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "Output (Dense)               (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 4,801\n",
            "Trainable params: 4,801\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/100\n",
            "380/380 - 1s - loss: 2.0443 - mse: 5.1327 - val_loss: 2.0520 - val_mse: 5.1972\n",
            "Epoch 2/100\n",
            "380/380 - 1s - loss: 2.0203 - mse: 5.0466 - val_loss: 2.0304 - val_mse: 5.1114\n",
            "Epoch 3/100\n",
            "380/380 - 1s - loss: 1.9987 - mse: 4.9613 - val_loss: 2.0087 - val_mse: 5.0270\n",
            "Epoch 4/100\n",
            "380/380 - 1s - loss: 1.9762 - mse: 4.8766 - val_loss: 1.9869 - val_mse: 4.9429\n",
            "Epoch 5/100\n",
            "380/380 - 1s - loss: 1.9557 - mse: 4.7925 - val_loss: 1.9651 - val_mse: 4.8598\n",
            "Epoch 6/100\n",
            "380/380 - 1s - loss: 1.9332 - mse: 4.7100 - val_loss: 1.9432 - val_mse: 4.7776\n",
            "Epoch 7/100\n",
            "380/380 - 1s - loss: 1.9106 - mse: 4.6276 - val_loss: 1.9211 - val_mse: 4.6959\n",
            "Epoch 8/100\n",
            "380/380 - 1s - loss: 1.8887 - mse: 4.5460 - val_loss: 1.8989 - val_mse: 4.6147\n",
            "Epoch 9/100\n",
            "380/380 - 1s - loss: 1.8652 - mse: 4.4645 - val_loss: 1.8765 - val_mse: 4.5339\n",
            "Epoch 10/100\n",
            "380/380 - 1s - loss: 1.8446 - mse: 4.3837 - val_loss: 1.8540 - val_mse: 4.4541\n",
            "Epoch 11/100\n",
            "380/380 - 1s - loss: 1.8226 - mse: 4.3033 - val_loss: 1.8313 - val_mse: 4.3746\n",
            "Epoch 12/100\n",
            "380/380 - 1s - loss: 1.7985 - mse: 4.2245 - val_loss: 1.8085 - val_mse: 4.2953\n",
            "Epoch 13/100\n",
            "380/380 - 1s - loss: 1.7747 - mse: 4.1459 - val_loss: 1.7855 - val_mse: 4.2170\n",
            "Epoch 14/100\n",
            "380/380 - 1s - loss: 1.7530 - mse: 4.0673 - val_loss: 1.7622 - val_mse: 4.1386\n",
            "Epoch 15/100\n",
            "380/380 - 1s - loss: 1.7272 - mse: 3.9893 - val_loss: 1.7387 - val_mse: 4.0607\n",
            "Epoch 16/100\n",
            "380/380 - 1s - loss: 1.7043 - mse: 3.9118 - val_loss: 1.7151 - val_mse: 3.9831\n",
            "Epoch 17/100\n",
            "380/380 - 1s - loss: 1.6799 - mse: 3.8338 - val_loss: 1.6913 - val_mse: 3.9061\n",
            "Epoch 18/100\n",
            "380/380 - 1s - loss: 1.6564 - mse: 3.7578 - val_loss: 1.6674 - val_mse: 3.8292\n",
            "Epoch 19/100\n",
            "380/380 - 1s - loss: 1.6320 - mse: 3.6803 - val_loss: 1.6435 - val_mse: 3.7522\n",
            "Epoch 20/100\n",
            "380/380 - 1s - loss: 1.6070 - mse: 3.6053 - val_loss: 1.6193 - val_mse: 3.6755\n",
            "Epoch 21/100\n",
            "380/380 - 1s - loss: 1.5829 - mse: 3.5275 - val_loss: 1.5949 - val_mse: 3.5985\n",
            "Epoch 22/100\n",
            "380/380 - 1s - loss: 1.5591 - mse: 3.4512 - val_loss: 1.5705 - val_mse: 3.5217\n",
            "Epoch 23/100\n",
            "380/380 - 1s - loss: 1.5351 - mse: 3.3757 - val_loss: 1.5459 - val_mse: 3.4448\n",
            "Epoch 24/100\n",
            "380/380 - 1s - loss: 1.5101 - mse: 3.2994 - val_loss: 1.5214 - val_mse: 3.3676\n",
            "Epoch 25/100\n",
            "380/380 - 1s - loss: 1.4854 - mse: 3.2239 - val_loss: 1.4967 - val_mse: 3.2905\n",
            "Epoch 26/100\n",
            "380/380 - 1s - loss: 1.4605 - mse: 3.1476 - val_loss: 1.4719 - val_mse: 3.2135\n",
            "Epoch 27/100\n",
            "380/380 - 1s - loss: 1.4370 - mse: 3.0719 - val_loss: 1.4471 - val_mse: 3.1363\n",
            "Epoch 28/100\n",
            "380/380 - 1s - loss: 1.4112 - mse: 2.9965 - val_loss: 1.4223 - val_mse: 3.0595\n",
            "Epoch 29/100\n",
            "380/380 - 1s - loss: 1.3858 - mse: 2.9220 - val_loss: 1.3974 - val_mse: 2.9826\n",
            "Epoch 30/100\n",
            "380/380 - 1s - loss: 1.3627 - mse: 2.8467 - val_loss: 1.3725 - val_mse: 2.9056\n",
            "Epoch 31/100\n",
            "380/380 - 1s - loss: 1.3367 - mse: 2.7719 - val_loss: 1.3474 - val_mse: 2.8282\n",
            "Epoch 32/100\n",
            "380/380 - 1s - loss: 1.3128 - mse: 2.6977 - val_loss: 1.3224 - val_mse: 2.7513\n",
            "Epoch 33/100\n",
            "380/380 - 1s - loss: 1.2862 - mse: 2.6230 - val_loss: 1.2972 - val_mse: 2.6736\n",
            "Epoch 34/100\n",
            "380/380 - 1s - loss: 1.2622 - mse: 2.5476 - val_loss: 1.2720 - val_mse: 2.5958\n",
            "Epoch 35/100\n",
            "380/380 - 1s - loss: 1.2372 - mse: 2.4729 - val_loss: 1.2469 - val_mse: 2.5189\n",
            "Epoch 36/100\n",
            "380/380 - 1s - loss: 1.2128 - mse: 2.3995 - val_loss: 1.2217 - val_mse: 2.4418\n",
            "Epoch 37/100\n",
            "380/380 - 1s - loss: 1.1878 - mse: 2.3253 - val_loss: 1.1965 - val_mse: 2.3649\n",
            "Epoch 38/100\n",
            "380/380 - 1s - loss: 1.1644 - mse: 2.2523 - val_loss: 1.1713 - val_mse: 2.2886\n",
            "Epoch 39/100\n",
            "380/380 - 1s - loss: 1.1393 - mse: 2.1795 - val_loss: 1.1460 - val_mse: 2.2127\n",
            "Epoch 40/100\n",
            "380/380 - 1s - loss: 1.1126 - mse: 2.1073 - val_loss: 1.1207 - val_mse: 2.1377\n",
            "Epoch 41/100\n",
            "380/380 - 1s - loss: 1.0892 - mse: 2.0372 - val_loss: 1.0953 - val_mse: 2.0636\n",
            "Epoch 42/100\n",
            "380/380 - 1s - loss: 1.0665 - mse: 1.9669 - val_loss: 1.0699 - val_mse: 1.9903\n",
            "Epoch 43/100\n",
            "380/380 - 1s - loss: 1.0412 - mse: 1.8974 - val_loss: 1.0446 - val_mse: 1.9187\n",
            "Epoch 44/100\n",
            "380/380 - 1s - loss: 1.0161 - mse: 1.8300 - val_loss: 1.0196 - val_mse: 1.8483\n",
            "Epoch 45/100\n",
            "380/380 - 1s - loss: 0.9936 - mse: 1.7646 - val_loss: 0.9949 - val_mse: 1.7800\n",
            "Epoch 46/100\n",
            "380/380 - 1s - loss: 0.9684 - mse: 1.7006 - val_loss: 0.9703 - val_mse: 1.7128\n",
            "Epoch 47/100\n",
            "380/380 - 1s - loss: 0.9447 - mse: 1.6372 - val_loss: 0.9460 - val_mse: 1.6478\n",
            "Epoch 48/100\n",
            "380/380 - 1s - loss: 0.9230 - mse: 1.5770 - val_loss: 0.9222 - val_mse: 1.5845\n",
            "Epoch 49/100\n",
            "380/380 - 1s - loss: 0.9011 - mse: 1.5182 - val_loss: 0.8991 - val_mse: 1.5235\n",
            "Epoch 50/100\n",
            "380/380 - 1s - loss: 0.8789 - mse: 1.4611 - val_loss: 0.8769 - val_mse: 1.4653\n",
            "Epoch 51/100\n",
            "380/380 - 1s - loss: 0.8574 - mse: 1.4071 - val_loss: 0.8554 - val_mse: 1.4093\n",
            "Epoch 52/100\n",
            "380/380 - 1s - loss: 0.8365 - mse: 1.3547 - val_loss: 0.8345 - val_mse: 1.3552\n",
            "Epoch 53/100\n",
            "380/380 - 1s - loss: 0.8164 - mse: 1.3043 - val_loss: 0.8143 - val_mse: 1.3037\n",
            "Epoch 54/100\n",
            "380/380 - 1s - loss: 0.7986 - mse: 1.2570 - val_loss: 0.7952 - val_mse: 1.2552\n",
            "Epoch 55/100\n",
            "380/380 - 1s - loss: 0.7806 - mse: 1.2118 - val_loss: 0.7768 - val_mse: 1.2092\n",
            "Epoch 56/100\n",
            "380/380 - 1s - loss: 0.7629 - mse: 1.1695 - val_loss: 0.7594 - val_mse: 1.1661\n",
            "Epoch 57/100\n",
            "380/380 - 1s - loss: 0.7464 - mse: 1.1293 - val_loss: 0.7429 - val_mse: 1.1255\n",
            "Epoch 58/100\n",
            "380/380 - 1s - loss: 0.7309 - mse: 1.0919 - val_loss: 0.7276 - val_mse: 1.0878\n",
            "Epoch 59/100\n",
            "380/380 - 1s - loss: 0.7176 - mse: 1.0565 - val_loss: 0.7132 - val_mse: 1.0521\n",
            "Epoch 60/100\n",
            "380/380 - 1s - loss: 0.7028 - mse: 1.0242 - val_loss: 0.6995 - val_mse: 1.0189\n",
            "Epoch 61/100\n",
            "380/380 - 1s - loss: 0.6895 - mse: 0.9936 - val_loss: 0.6866 - val_mse: 0.9880\n",
            "Epoch 62/100\n",
            "380/380 - 1s - loss: 0.6785 - mse: 0.9644 - val_loss: 0.6749 - val_mse: 0.9598\n",
            "Epoch 63/100\n",
            "380/380 - 1s - loss: 0.6672 - mse: 0.9389 - val_loss: 0.6641 - val_mse: 0.9336\n",
            "Epoch 64/100\n",
            "380/380 - 1s - loss: 0.6574 - mse: 0.9146 - val_loss: 0.6544 - val_mse: 0.9099\n",
            "Epoch 65/100\n",
            "380/380 - 1s - loss: 0.6492 - mse: 0.8930 - val_loss: 0.6455 - val_mse: 0.8881\n",
            "Epoch 66/100\n",
            "380/380 - 1s - loss: 0.6399 - mse: 0.8728 - val_loss: 0.6375 - val_mse: 0.8681\n",
            "Epoch 67/100\n",
            "380/380 - 1s - loss: 0.6331 - mse: 0.8542 - val_loss: 0.6303 - val_mse: 0.8498\n",
            "Epoch 68/100\n",
            "380/380 - 1s - loss: 0.6253 - mse: 0.8376 - val_loss: 0.6237 - val_mse: 0.8331\n",
            "Epoch 69/100\n",
            "380/380 - 1s - loss: 0.6199 - mse: 0.8220 - val_loss: 0.6179 - val_mse: 0.8179\n",
            "Epoch 70/100\n",
            "380/380 - 1s - loss: 0.6136 - mse: 0.8078 - val_loss: 0.6127 - val_mse: 0.8041\n",
            "Epoch 71/100\n",
            "380/380 - 1s - loss: 0.6118 - mse: 0.7952 - val_loss: 0.6080 - val_mse: 0.7916\n",
            "Epoch 72/100\n",
            "380/380 - 1s - loss: 0.6042 - mse: 0.7834 - val_loss: 0.6037 - val_mse: 0.7802\n",
            "Epoch 73/100\n",
            "380/380 - 1s - loss: 0.6003 - mse: 0.7732 - val_loss: 0.5999 - val_mse: 0.7698\n",
            "Epoch 74/100\n",
            "380/380 - 1s - loss: 0.5968 - mse: 0.7640 - val_loss: 0.5964 - val_mse: 0.7604\n",
            "Epoch 75/100\n",
            "380/380 - 1s - loss: 0.5945 - mse: 0.7553 - val_loss: 0.5933 - val_mse: 0.7517\n",
            "Epoch 76/100\n",
            "380/380 - 1s - loss: 0.5905 - mse: 0.7471 - val_loss: 0.5904 - val_mse: 0.7438\n",
            "Epoch 77/100\n",
            "380/380 - 1s - loss: 0.5881 - mse: 0.7399 - val_loss: 0.5877 - val_mse: 0.7365\n",
            "Epoch 78/100\n",
            "380/380 - 1s - loss: 0.5863 - mse: 0.7332 - val_loss: 0.5852 - val_mse: 0.7296\n",
            "Epoch 79/100\n",
            "380/380 - 1s - loss: 0.5829 - mse: 0.7268 - val_loss: 0.5829 - val_mse: 0.7235\n",
            "Epoch 80/100\n",
            "380/380 - 1s - loss: 0.5802 - mse: 0.7216 - val_loss: 0.5807 - val_mse: 0.7176\n",
            "Epoch 81/100\n",
            "380/380 - 1s - loss: 0.5789 - mse: 0.7159 - val_loss: 0.5786 - val_mse: 0.7120\n",
            "Epoch 82/100\n",
            "380/380 - 1s - loss: 0.5767 - mse: 0.7108 - val_loss: 0.5767 - val_mse: 0.7067\n",
            "Epoch 83/100\n",
            "380/380 - 1s - loss: 0.5744 - mse: 0.7056 - val_loss: 0.5748 - val_mse: 0.7016\n",
            "Epoch 84/100\n",
            "380/380 - 1s - loss: 0.5726 - mse: 0.7010 - val_loss: 0.5730 - val_mse: 0.6967\n",
            "Epoch 85/100\n",
            "380/380 - 1s - loss: 0.5722 - mse: 0.6960 - val_loss: 0.5713 - val_mse: 0.6922\n",
            "Epoch 86/100\n",
            "380/380 - 1s - loss: 0.5692 - mse: 0.6918 - val_loss: 0.5696 - val_mse: 0.6876\n",
            "Epoch 87/100\n",
            "380/380 - 1s - loss: 0.5676 - mse: 0.6877 - val_loss: 0.5680 - val_mse: 0.6835\n",
            "Epoch 88/100\n",
            "380/380 - 1s - loss: 0.5666 - mse: 0.6839 - val_loss: 0.5665 - val_mse: 0.6793\n",
            "Epoch 89/100\n",
            "380/380 - 1s - loss: 0.5651 - mse: 0.6795 - val_loss: 0.5649 - val_mse: 0.6753\n",
            "Epoch 90/100\n",
            "380/380 - 1s - loss: 0.5629 - mse: 0.6763 - val_loss: 0.5634 - val_mse: 0.6716\n",
            "Epoch 91/100\n",
            "380/380 - 1s - loss: 0.5615 - mse: 0.6729 - val_loss: 0.5620 - val_mse: 0.6680\n",
            "Epoch 92/100\n",
            "380/380 - 1s - loss: 0.5602 - mse: 0.6695 - val_loss: 0.5605 - val_mse: 0.6646\n",
            "Epoch 93/100\n",
            "380/380 - 1s - loss: 0.5584 - mse: 0.6658 - val_loss: 0.5592 - val_mse: 0.6611\n",
            "Epoch 94/100\n",
            "380/380 - 1s - loss: 0.5568 - mse: 0.6624 - val_loss: 0.5578 - val_mse: 0.6578\n",
            "Epoch 95/100\n",
            "380/380 - 1s - loss: 0.5558 - mse: 0.6595 - val_loss: 0.5565 - val_mse: 0.6546\n",
            "Epoch 96/100\n",
            "380/380 - 1s - loss: 0.5547 - mse: 0.6562 - val_loss: 0.5552 - val_mse: 0.6511\n",
            "Epoch 97/100\n",
            "380/380 - 1s - loss: 0.5541 - mse: 0.6530 - val_loss: 0.5539 - val_mse: 0.6481\n",
            "Epoch 98/100\n",
            "380/380 - 1s - loss: 0.5530 - mse: 0.6496 - val_loss: 0.5526 - val_mse: 0.6448\n",
            "Epoch 99/100\n",
            "380/380 - 1s - loss: 0.5503 - mse: 0.6464 - val_loss: 0.5514 - val_mse: 0.6417\n",
            "Epoch 100/100\n",
            "380/380 - 1s - loss: 0.5492 - mse: 0.6435 - val_loss: 0.5502 - val_mse: 0.6389\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 95%|█████████▌| 20/21 [23:27<01:14, 74.02s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 32)                288       \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "Output (Dense)               (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 321\n",
            "Trainable params: 321\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/100\n",
            "380/380 - 1s - loss: 5.3862 - mse: 5.3838 - val_loss: 5.6094 - val_mse: 5.6103\n",
            "Epoch 2/100\n",
            "380/380 - 1s - loss: 5.3362 - mse: 5.3404 - val_loss: 5.5641 - val_mse: 5.5650\n",
            "Epoch 3/100\n",
            "380/380 - 1s - loss: 5.2930 - mse: 5.2973 - val_loss: 5.5190 - val_mse: 5.5200\n",
            "Epoch 4/100\n",
            "380/380 - 1s - loss: 5.2477 - mse: 5.2546 - val_loss: 5.4744 - val_mse: 5.4754\n",
            "Epoch 5/100\n",
            "380/380 - 1s - loss: 5.2106 - mse: 5.2123 - val_loss: 5.4302 - val_mse: 5.4313\n",
            "Epoch 6/100\n",
            "380/380 - 1s - loss: 5.1681 - mse: 5.1703 - val_loss: 5.3865 - val_mse: 5.3876\n",
            "Epoch 7/100\n",
            "380/380 - 1s - loss: 5.1231 - mse: 5.1288 - val_loss: 5.3432 - val_mse: 5.3444\n",
            "Epoch 8/100\n",
            "380/380 - 1s - loss: 5.0846 - mse: 5.0877 - val_loss: 5.3003 - val_mse: 5.3015\n",
            "Epoch 9/100\n",
            "380/380 - 1s - loss: 5.0398 - mse: 5.0468 - val_loss: 5.2574 - val_mse: 5.2587\n",
            "Epoch 10/100\n",
            "380/380 - 1s - loss: 5.0080 - mse: 5.0062 - val_loss: 5.2150 - val_mse: 5.2163\n",
            "Epoch 11/100\n",
            "380/380 - 1s - loss: 4.9735 - mse: 4.9658 - val_loss: 5.1730 - val_mse: 5.1743\n",
            "Epoch 12/100\n",
            "380/380 - 1s - loss: 4.9275 - mse: 4.9257 - val_loss: 5.1309 - val_mse: 5.1323\n",
            "Epoch 13/100\n",
            "380/380 - 1s - loss: 4.8819 - mse: 4.8858 - val_loss: 5.0894 - val_mse: 5.0907\n",
            "Epoch 14/100\n",
            "380/380 - 1s - loss: 4.8542 - mse: 4.8462 - val_loss: 5.0480 - val_mse: 5.0494\n",
            "Epoch 15/100\n",
            "380/380 - 1s - loss: 4.8004 - mse: 4.8069 - val_loss: 5.0070 - val_mse: 5.0084\n",
            "Epoch 16/100\n",
            "380/380 - 1s - loss: 4.7649 - mse: 4.7679 - val_loss: 4.9661 - val_mse: 4.9676\n",
            "Epoch 17/100\n",
            "380/380 - 1s - loss: 4.7255 - mse: 4.7290 - val_loss: 4.9255 - val_mse: 4.9270\n",
            "Epoch 18/100\n",
            "380/380 - 1s - loss: 4.6893 - mse: 4.6905 - val_loss: 4.8852 - val_mse: 4.8868\n",
            "Epoch 19/100\n",
            "380/380 - 1s - loss: 4.6504 - mse: 4.6522 - val_loss: 4.8453 - val_mse: 4.8469\n",
            "Epoch 20/100\n",
            "380/380 - 1s - loss: 4.6082 - mse: 4.6141 - val_loss: 4.8055 - val_mse: 4.8071\n",
            "Epoch 21/100\n",
            "380/380 - 1s - loss: 4.5720 - mse: 4.5763 - val_loss: 4.7661 - val_mse: 4.7677\n",
            "Epoch 22/100\n",
            "380/380 - 1s - loss: 4.5357 - mse: 4.5388 - val_loss: 4.7268 - val_mse: 4.7285\n",
            "Epoch 23/100\n",
            "380/380 - 1s - loss: 4.5055 - mse: 4.5014 - val_loss: 4.6877 - val_mse: 4.6895\n",
            "Epoch 24/100\n",
            "380/380 - 1s - loss: 4.4674 - mse: 4.4642 - val_loss: 4.6489 - val_mse: 4.6507\n",
            "Epoch 25/100\n",
            "380/380 - 1s - loss: 4.4306 - mse: 4.4273 - val_loss: 4.6103 - val_mse: 4.6121\n",
            "Epoch 26/100\n",
            "380/380 - 1s - loss: 4.3876 - mse: 4.3907 - val_loss: 4.5721 - val_mse: 4.5740\n",
            "Epoch 27/100\n",
            "380/380 - 1s - loss: 4.3642 - mse: 4.3543 - val_loss: 4.5340 - val_mse: 4.5359\n",
            "Epoch 28/100\n",
            "380/380 - 1s - loss: 4.3193 - mse: 4.3182 - val_loss: 4.4966 - val_mse: 4.4984\n",
            "Epoch 29/100\n",
            "380/380 - 1s - loss: 4.2766 - mse: 4.2826 - val_loss: 4.4593 - val_mse: 4.4612\n",
            "Epoch 30/100\n",
            "380/380 - 1s - loss: 4.2534 - mse: 4.2471 - val_loss: 4.4222 - val_mse: 4.4241\n",
            "Epoch 31/100\n",
            "380/380 - 1s - loss: 4.2107 - mse: 4.2118 - val_loss: 4.3854 - val_mse: 4.3873\n",
            "Epoch 32/100\n",
            "380/380 - 1s - loss: 4.1782 - mse: 4.1768 - val_loss: 4.3488 - val_mse: 4.3508\n",
            "Epoch 33/100\n",
            "380/380 - 1s - loss: 4.1378 - mse: 4.1419 - val_loss: 4.3124 - val_mse: 4.3144\n",
            "Epoch 34/100\n",
            "380/380 - 1s - loss: 4.1064 - mse: 4.1073 - val_loss: 4.2763 - val_mse: 4.2783\n",
            "Epoch 35/100\n",
            "380/380 - 1s - loss: 4.0701 - mse: 4.0730 - val_loss: 4.2405 - val_mse: 4.2425\n",
            "Epoch 36/100\n",
            "380/380 - 1s - loss: 4.0360 - mse: 4.0390 - val_loss: 4.2048 - val_mse: 4.2069\n",
            "Epoch 37/100\n",
            "380/380 - 1s - loss: 4.0063 - mse: 4.0051 - val_loss: 4.1694 - val_mse: 4.1715\n",
            "Epoch 38/100\n",
            "380/380 - 1s - loss: 3.9777 - mse: 3.9714 - val_loss: 4.1343 - val_mse: 4.1364\n",
            "Epoch 39/100\n",
            "380/380 - 1s - loss: 3.9400 - mse: 3.9380 - val_loss: 4.0995 - val_mse: 4.1016\n",
            "Epoch 40/100\n",
            "380/380 - 1s - loss: 3.8986 - mse: 3.9050 - val_loss: 4.0650 - val_mse: 4.0671\n",
            "Epoch 41/100\n",
            "380/380 - 1s - loss: 3.8681 - mse: 3.8722 - val_loss: 4.0307 - val_mse: 4.0329\n",
            "Epoch 42/100\n",
            "380/380 - 1s - loss: 3.8442 - mse: 3.8396 - val_loss: 3.9966 - val_mse: 3.9988\n",
            "Epoch 43/100\n",
            "380/380 - 1s - loss: 3.8108 - mse: 3.8071 - val_loss: 3.9627 - val_mse: 3.9650\n",
            "Epoch 44/100\n",
            "380/380 - 1s - loss: 3.7708 - mse: 3.7749 - val_loss: 3.9291 - val_mse: 3.9314\n",
            "Epoch 45/100\n",
            "380/380 - 1s - loss: 3.7456 - mse: 3.7430 - val_loss: 3.8958 - val_mse: 3.8981\n",
            "Epoch 46/100\n",
            "380/380 - 1s - loss: 3.7097 - mse: 3.7113 - val_loss: 3.8627 - val_mse: 3.8650\n",
            "Epoch 47/100\n",
            "380/380 - 1s - loss: 3.6750 - mse: 3.6798 - val_loss: 3.8298 - val_mse: 3.8321\n",
            "Epoch 48/100\n",
            "380/380 - 1s - loss: 3.6486 - mse: 3.6486 - val_loss: 3.7972 - val_mse: 3.7995\n",
            "Epoch 49/100\n",
            "380/380 - 1s - loss: 3.6233 - mse: 3.6176 - val_loss: 3.7648 - val_mse: 3.7672\n",
            "Epoch 50/100\n",
            "380/380 - 1s - loss: 3.5889 - mse: 3.5868 - val_loss: 3.7327 - val_mse: 3.7351\n",
            "Epoch 51/100\n",
            "380/380 - 1s - loss: 3.5548 - mse: 3.5563 - val_loss: 3.7009 - val_mse: 3.7033\n",
            "Epoch 52/100\n",
            "380/380 - 1s - loss: 3.5237 - mse: 3.5260 - val_loss: 3.6692 - val_mse: 3.6716\n",
            "Epoch 53/100\n",
            "380/380 - 1s - loss: 3.4905 - mse: 3.4960 - val_loss: 3.6380 - val_mse: 3.6404\n",
            "Epoch 54/100\n",
            "380/380 - 1s - loss: 3.4669 - mse: 3.4662 - val_loss: 3.6069 - val_mse: 3.6093\n",
            "Epoch 55/100\n",
            "380/380 - 1s - loss: 3.4364 - mse: 3.4367 - val_loss: 3.5760 - val_mse: 3.5784\n",
            "Epoch 56/100\n",
            "380/380 - 1s - loss: 3.4065 - mse: 3.4074 - val_loss: 3.5454 - val_mse: 3.5479\n",
            "Epoch 57/100\n",
            "380/380 - 1s - loss: 3.3743 - mse: 3.3783 - val_loss: 3.5151 - val_mse: 3.5175\n",
            "Epoch 58/100\n",
            "380/380 - 1s - loss: 3.3476 - mse: 3.3495 - val_loss: 3.4849 - val_mse: 3.4874\n",
            "Epoch 59/100\n",
            "380/380 - 1s - loss: 3.3233 - mse: 3.3209 - val_loss: 3.4551 - val_mse: 3.4576\n",
            "Epoch 60/100\n",
            "380/380 - 1s - loss: 3.2883 - mse: 3.2925 - val_loss: 3.4253 - val_mse: 3.4278\n",
            "Epoch 61/100\n",
            "380/380 - 1s - loss: 3.2591 - mse: 3.2643 - val_loss: 3.3958 - val_mse: 3.3983\n",
            "Epoch 62/100\n",
            "380/380 - 1s - loss: 3.2378 - mse: 3.2363 - val_loss: 3.3667 - val_mse: 3.3692\n",
            "Epoch 63/100\n",
            "380/380 - 1s - loss: 3.2046 - mse: 3.2085 - val_loss: 3.3374 - val_mse: 3.3400\n",
            "Epoch 64/100\n",
            "380/380 - 1s - loss: 3.1855 - mse: 3.1809 - val_loss: 3.3086 - val_mse: 3.3111\n",
            "Epoch 65/100\n",
            "380/380 - 1s - loss: 3.1517 - mse: 3.1534 - val_loss: 3.2798 - val_mse: 3.2824\n",
            "Epoch 66/100\n",
            "380/380 - 1s - loss: 3.1218 - mse: 3.1263 - val_loss: 3.2513 - val_mse: 3.2539\n",
            "Epoch 67/100\n",
            "380/380 - 1s - loss: 3.1021 - mse: 3.0994 - val_loss: 3.2231 - val_mse: 3.2256\n",
            "Epoch 68/100\n",
            "380/380 - 1s - loss: 3.0682 - mse: 3.0726 - val_loss: 3.1950 - val_mse: 3.1976\n",
            "Epoch 69/100\n",
            "380/380 - 1s - loss: 3.0492 - mse: 3.0460 - val_loss: 3.1671 - val_mse: 3.1697\n",
            "Epoch 70/100\n",
            "380/380 - 1s - loss: 3.0220 - mse: 3.0195 - val_loss: 3.1394 - val_mse: 3.1420\n",
            "Epoch 71/100\n",
            "380/380 - 1s - loss: 3.0017 - mse: 2.9934 - val_loss: 3.1120 - val_mse: 3.1146\n",
            "Epoch 72/100\n",
            "380/380 - 1s - loss: 2.9696 - mse: 2.9675 - val_loss: 3.0849 - val_mse: 3.0875\n",
            "Epoch 73/100\n",
            "380/380 - 1s - loss: 2.9378 - mse: 2.9418 - val_loss: 3.0579 - val_mse: 3.0605\n",
            "Epoch 74/100\n",
            "380/380 - 1s - loss: 2.9132 - mse: 2.9163 - val_loss: 3.0313 - val_mse: 3.0339\n",
            "Epoch 75/100\n",
            "380/380 - 1s - loss: 2.8958 - mse: 2.8910 - val_loss: 3.0047 - val_mse: 3.0074\n",
            "Epoch 76/100\n",
            "380/380 - 1s - loss: 2.8645 - mse: 2.8659 - val_loss: 2.9785 - val_mse: 2.9811\n",
            "Epoch 77/100\n",
            "380/380 - 1s - loss: 2.8410 - mse: 2.8410 - val_loss: 2.9525 - val_mse: 2.9551\n",
            "Epoch 78/100\n",
            "380/380 - 1s - loss: 2.8242 - mse: 2.8164 - val_loss: 2.9267 - val_mse: 2.9294\n",
            "Epoch 79/100\n",
            "380/380 - 1s - loss: 2.7889 - mse: 2.7919 - val_loss: 2.9011 - val_mse: 2.9037\n",
            "Epoch 80/100\n",
            "380/380 - 1s - loss: 2.7655 - mse: 2.7676 - val_loss: 2.8756 - val_mse: 2.8782\n",
            "Epoch 81/100\n",
            "380/380 - 1s - loss: 2.7475 - mse: 2.7434 - val_loss: 2.8504 - val_mse: 2.8530\n",
            "Epoch 82/100\n",
            "380/380 - 1s - loss: 2.7183 - mse: 2.7195 - val_loss: 2.8253 - val_mse: 2.8280\n",
            "Epoch 83/100\n",
            "380/380 - 1s - loss: 2.6924 - mse: 2.6958 - val_loss: 2.8005 - val_mse: 2.8032\n",
            "Epoch 84/100\n",
            "380/380 - 1s - loss: 2.6691 - mse: 2.6724 - val_loss: 2.7761 - val_mse: 2.7787\n",
            "Epoch 85/100\n",
            "380/380 - 1s - loss: 2.6546 - mse: 2.6492 - val_loss: 2.7520 - val_mse: 2.7546\n",
            "Epoch 86/100\n",
            "380/380 - 1s - loss: 2.6227 - mse: 2.6262 - val_loss: 2.7280 - val_mse: 2.7306\n",
            "Epoch 87/100\n",
            "380/380 - 1s - loss: 2.6016 - mse: 2.6035 - val_loss: 2.7042 - val_mse: 2.7068\n",
            "Epoch 88/100\n",
            "380/380 - 1s - loss: 2.5859 - mse: 2.5809 - val_loss: 2.6806 - val_mse: 2.6832\n",
            "Epoch 89/100\n",
            "380/380 - 1s - loss: 2.5608 - mse: 2.5584 - val_loss: 2.6571 - val_mse: 2.6598\n",
            "Epoch 90/100\n",
            "380/380 - 1s - loss: 2.5386 - mse: 2.5361 - val_loss: 2.6339 - val_mse: 2.6365\n",
            "Epoch 91/100\n",
            "380/380 - 1s - loss: 2.5104 - mse: 2.5140 - val_loss: 2.6108 - val_mse: 2.6134\n",
            "Epoch 92/100\n",
            "380/380 - 1s - loss: 2.4926 - mse: 2.4922 - val_loss: 2.5881 - val_mse: 2.5907\n",
            "Epoch 93/100\n",
            "380/380 - 1s - loss: 2.4659 - mse: 2.4705 - val_loss: 2.5655 - val_mse: 2.5681\n",
            "Epoch 94/100\n",
            "380/380 - 1s - loss: 2.4451 - mse: 2.4491 - val_loss: 2.5432 - val_mse: 2.5459\n",
            "Epoch 95/100\n",
            "380/380 - 1s - loss: 2.4251 - mse: 2.4279 - val_loss: 2.5211 - val_mse: 2.5237\n",
            "Epoch 96/100\n",
            "380/380 - 1s - loss: 2.4028 - mse: 2.4069 - val_loss: 2.4992 - val_mse: 2.5018\n",
            "Epoch 97/100\n",
            "380/380 - 1s - loss: 2.3901 - mse: 2.3861 - val_loss: 2.4774 - val_mse: 2.4800\n",
            "Epoch 98/100\n",
            "380/380 - 1s - loss: 2.3691 - mse: 2.3653 - val_loss: 2.4558 - val_mse: 2.4584\n",
            "Epoch 99/100\n",
            "380/380 - 1s - loss: 2.3447 - mse: 2.3449 - val_loss: 2.4345 - val_mse: 2.4371\n",
            "Epoch 100/100\n",
            "380/380 - 1s - loss: 2.3211 - mse: 2.3246 - val_loss: 2.4134 - val_mse: 2.4160\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "100%|██████████| 21/21 [24:35<00:00, 70.25s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCdRvsGFMrtE",
        "colab_type": "text"
      },
      "source": [
        "### Check the results from Scan"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98Q4IzP8MrKm",
        "colab_type": "code",
        "outputId": "c3b4312f-dfca-4048-ccc0-c42286d381e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        }
      },
      "source": [
        "# accessing the results data frame\n",
        "scan_object.data.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>start</th>\n",
              "      <th>end</th>\n",
              "      <th>duration</th>\n",
              "      <th>round_epochs</th>\n",
              "      <th>loss</th>\n",
              "      <th>mse</th>\n",
              "      <th>val_loss</th>\n",
              "      <th>val_mse</th>\n",
              "      <th>activation</th>\n",
              "      <th>batch_size</th>\n",
              "      <th>dropout</th>\n",
              "      <th>epochs</th>\n",
              "      <th>first_neuron</th>\n",
              "      <th>hidden_layers</th>\n",
              "      <th>losses</th>\n",
              "      <th>lr</th>\n",
              "      <th>optimizer</th>\n",
              "      <th>shapes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>03/29/20-225448</td>\n",
              "      <td>03/29/20-225602</td>\n",
              "      <td>74.184704</td>\n",
              "      <td>100</td>\n",
              "      <td>0.701878</td>\n",
              "      <td>1.005612</td>\n",
              "      <td>0.707667</td>\n",
              "      <td>1.003745</td>\n",
              "      <td>elu</td>\n",
              "      <td>20</td>\n",
              "      <td>0</td>\n",
              "      <td>100</td>\n",
              "      <td>32</td>\n",
              "      <td>3</td>\n",
              "      <td>mae</td>\n",
              "      <td>0.001</td>\n",
              "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
              "      <td>brick</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>03/29/20-225602</td>\n",
              "      <td>03/29/20-225706</td>\n",
              "      <td>63.696198</td>\n",
              "      <td>100</td>\n",
              "      <td>0.729934</td>\n",
              "      <td>0.730945</td>\n",
              "      <td>0.743067</td>\n",
              "      <td>0.744447</td>\n",
              "      <td>elu</td>\n",
              "      <td>20</td>\n",
              "      <td>0</td>\n",
              "      <td>100</td>\n",
              "      <td>64</td>\n",
              "      <td>1</td>\n",
              "      <td>mse</td>\n",
              "      <td>0.001</td>\n",
              "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.g...</td>\n",
              "      <td>brick</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>03/29/20-225706</td>\n",
              "      <td>03/29/20-225825</td>\n",
              "      <td>78.316898</td>\n",
              "      <td>100</td>\n",
              "      <td>1.192151</td>\n",
              "      <td>1.194020</td>\n",
              "      <td>1.257920</td>\n",
              "      <td>1.259226</td>\n",
              "      <td>relu</td>\n",
              "      <td>20</td>\n",
              "      <td>0</td>\n",
              "      <td>100</td>\n",
              "      <td>64</td>\n",
              "      <td>3</td>\n",
              "      <td>mse</td>\n",
              "      <td>0.001</td>\n",
              "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
              "      <td>brick</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>03/29/20-225825</td>\n",
              "      <td>03/29/20-225931</td>\n",
              "      <td>66.415177</td>\n",
              "      <td>100</td>\n",
              "      <td>0.951792</td>\n",
              "      <td>0.953028</td>\n",
              "      <td>0.983768</td>\n",
              "      <td>0.984797</td>\n",
              "      <td>relu</td>\n",
              "      <td>20</td>\n",
              "      <td>0</td>\n",
              "      <td>100</td>\n",
              "      <td>32</td>\n",
              "      <td>2</td>\n",
              "      <td>mse</td>\n",
              "      <td>0.001</td>\n",
              "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.g...</td>\n",
              "      <td>brick</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>03/29/20-225932</td>\n",
              "      <td>03/29/20-230039</td>\n",
              "      <td>67.255532</td>\n",
              "      <td>100</td>\n",
              "      <td>1.685242</td>\n",
              "      <td>4.186103</td>\n",
              "      <td>1.722976</td>\n",
              "      <td>4.352967</td>\n",
              "      <td>relu</td>\n",
              "      <td>20</td>\n",
              "      <td>0</td>\n",
              "      <td>100</td>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "      <td>mae</td>\n",
              "      <td>0.001</td>\n",
              "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.n...</td>\n",
              "      <td>brick</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             start  ... shapes\n",
              "0  03/29/20-225448  ...  brick\n",
              "1  03/29/20-225602  ...  brick\n",
              "2  03/29/20-225706  ...  brick\n",
              "3  03/29/20-225825  ...  brick\n",
              "4  03/29/20-225932  ...  brick\n",
              "\n",
              "[5 rows x 18 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 386
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OeQv9PmjM8UT",
        "colab_type": "code",
        "outputId": "e68c10fb-c0fa-4125-fed7-64963235f38e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 700
        }
      },
      "source": [
        "# accessing epoch entropy values for each round\n",
        "scan_object.learning_entropy"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>loss</th>\n",
              "      <th>mse</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.000003</td>\n",
              "      <td>1.557824e-05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.000004</td>\n",
              "      <td>2.432996e-06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.000029</td>\n",
              "      <td>3.058036e-05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.000017</td>\n",
              "      <td>1.895860e-05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.000003</td>\n",
              "      <td>7.840127e-06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.000023</td>\n",
              "      <td>2.389976e-05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.000093</td>\n",
              "      <td>8.724167e-05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.000155</td>\n",
              "      <td>1.731936e-04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.000359</td>\n",
              "      <td>3.742007e-04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.000004</td>\n",
              "      <td>3.072257e-06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.000001</td>\n",
              "      <td>1.336087e-05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.000010</td>\n",
              "      <td>9.754266e-06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.000150</td>\n",
              "      <td>1.480389e-04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.000005</td>\n",
              "      <td>5.730105e-05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.000002</td>\n",
              "      <td>1.126553e-06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.000037</td>\n",
              "      <td>5.628746e-05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.000011</td>\n",
              "      <td>3.269313e-05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.001170</td>\n",
              "      <td>1.197469e-03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.000062</td>\n",
              "      <td>6.661934e-05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.000009</td>\n",
              "      <td>4.195818e-05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.000001</td>\n",
              "      <td>3.070435e-07</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        loss           mse\n",
              "0   0.000003  1.557824e-05\n",
              "1   0.000004  2.432996e-06\n",
              "2   0.000029  3.058036e-05\n",
              "3   0.000017  1.895860e-05\n",
              "4   0.000003  7.840127e-06\n",
              "5   0.000023  2.389976e-05\n",
              "6   0.000093  8.724167e-05\n",
              "7   0.000155  1.731936e-04\n",
              "8   0.000359  3.742007e-04\n",
              "9   0.000004  3.072257e-06\n",
              "10  0.000001  1.336087e-05\n",
              "11  0.000010  9.754266e-06\n",
              "12  0.000150  1.480389e-04\n",
              "13  0.000005  5.730105e-05\n",
              "14  0.000002  1.126553e-06\n",
              "15  0.000037  5.628746e-05\n",
              "16  0.000011  3.269313e-05\n",
              "17  0.001170  1.197469e-03\n",
              "18  0.000062  6.661934e-05\n",
              "19  0.000009  4.195818e-05\n",
              "20  0.000001  3.070435e-07"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 387
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDKJFhr0M-LD",
        "colab_type": "code",
        "outputId": "2eb2e379-9375-46ca-9e6b-4725eb5898bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "\n",
        "# access the summary details\n",
        "scan_object.details"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "experiment_name                  my_exp\n",
              "random_method          uniform_mersenne\n",
              "reduction_method                   None\n",
              "reduction_interval                   50\n",
              "reduction_window                     20\n",
              "reduction_threshold                 0.2\n",
              "reduction_metric                val_mse\n",
              "complete_time            03/29/20/23:19\n",
              "x_shape                      (15480, 8)\n",
              "y_shape                        (15480,)\n",
              "dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 388
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPworpFsNMXn",
        "colab_type": "text"
      },
      "source": [
        "In addition to statistics and meta-data related with the Scan, the used data (x and y) together with the saved model and model weights for each hyperparameter permutation is stored in the Scan object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzTpY-uWNNIU",
        "colab_type": "code",
        "outputId": "eb358897-1bfe-48d6-c3ef-0facd745b039",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        }
      },
      "source": [
        "# accessing the saved models\n",
        "scan_object.saved_models"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['{\"class_name\": \"Sequential\", \"config\": {\"name\": \"sequential_1\", \"layers\": [{\"class_name\": \"Dense\", \"config\": {\"name\": \"dense_2\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 32, \"activation\": \"elu\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null, \"batch_input_shape\": [null, 8]}}, {\"class_name\": \"Dropout\", \"config\": {\"name\": \"dropout_2\", \"trainable\": true, \"dtype\": \"float32\", \"rate\": 0, \"noise_shape\": null, \"seed\": null}}, {\"class_name\": \"Dense\", \"config\": {\"name\": \"dense_3\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 32, \"activation\": \"elu\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}}, {\"class_name\": \"Dropout\", \"config\": {\"name\": \"dropout_3\", \"trainable\": true, \"dtype\": \"float32\", \"rate\": 0, \"noise_shape\": null, \"seed\": null}}, {\"class_name\": \"Dense\", \"config\": {\"name\": \"dense_4\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 32, \"activation\": \"elu\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}}, {\"class_name\": \"Dropout\", \"config\": {\"name\": \"dropout_4\", \"trainable\": true, \"dtype\": \"float32\", \"rate\": 0, \"noise_shape\": null, \"seed\": null}}, {\"class_name\": \"Dense\", \"config\": {\"name\": \"Output\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 1, \"activation\": \"linear\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}}], \"build_input_shape\": [null, 8]}, \"keras_version\": \"2.2.4-tf\", \"backend\": \"tensorflow\"}',\n",
              " '{\"class_name\": \"Sequential\", \"config\": {\"name\": \"sequential\", \"layers\": [{\"class_name\": \"Dense\", \"config\": {\"name\": \"dense\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 64, \"activation\": \"elu\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null, \"batch_input_shape\": [null, 8]}}, {\"class_name\": \"Dropout\", \"config\": {\"name\": \"dropout\", \"trainable\": true, \"dtype\": \"float32\", \"rate\": 0, \"noise_shape\": null, \"seed\": null}}, {\"class_name\": \"Dense\", \"config\": {\"name\": \"Output\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 1, \"activation\": \"linear\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}}], \"build_input_shape\": [null, 8]}, \"keras_version\": \"2.2.4-tf\", \"backend\": \"tensorflow\"}',\n",
              " '{\"class_name\": \"Sequential\", \"config\": {\"name\": \"sequential\", \"layers\": [{\"class_name\": \"Dense\", \"config\": {\"name\": \"dense\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 64, \"activation\": \"relu\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null, \"batch_input_shape\": [null, 8]}}, {\"class_name\": \"Dropout\", \"config\": {\"name\": \"dropout\", \"trainable\": true, \"dtype\": \"float32\", \"rate\": 0, \"noise_shape\": null, \"seed\": null}}, {\"class_name\": \"Dense\", \"config\": {\"name\": \"dense_1\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 64, \"activation\": \"relu\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}}, {\"class_name\": \"Dropout\", \"config\": {\"name\": \"dropout_1\", \"trainable\": true, \"dtype\": \"float32\", \"rate\": 0, \"noise_shape\": null, \"seed\": null}}, {\"class_name\": \"Dense\", \"config\": {\"name\": \"dense_2\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 64, \"activation\": \"relu\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}}, {\"class_name\": \"Dropout\", \"config\": {\"name\": \"dropout_2\", \"trainable\": true, \"dtype\": \"float32\", \"rate\": 0, \"noise_shape\": null, \"seed\": null}}, {\"class_name\": \"Dense\", \"config\": {\"name\": \"Output\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 1, \"activation\": \"linear\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}}], \"build_input_shape\": [null, 8]}, \"keras_version\": \"2.2.4-tf\", \"backend\": \"tensorflow\"}',\n",
              " '{\"class_name\": \"Sequential\", \"config\": {\"name\": \"sequential\", \"layers\": [{\"class_name\": \"Dense\", \"config\": {\"name\": \"dense\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 32, \"activation\": \"relu\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null, \"batch_input_shape\": [null, 8]}}, {\"class_name\": \"Dropout\", \"config\": {\"name\": \"dropout\", \"trainable\": true, \"dtype\": \"float32\", \"rate\": 0, \"noise_shape\": null, \"seed\": null}}, {\"class_name\": \"Dense\", \"config\": {\"name\": \"dense_1\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 32, \"activation\": \"relu\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}}, {\"class_name\": \"Dropout\", \"config\": {\"name\": \"dropout_1\", \"trainable\": true, \"dtype\": \"float32\", \"rate\": 0, \"noise_shape\": null, \"seed\": null}}, {\"class_name\": \"Dense\", \"config\": {\"name\": \"Output\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 1, \"activation\": \"linear\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}}], \"build_input_shape\": [null, 8]}, \"keras_version\": \"2.2.4-tf\", \"backend\": \"tensorflow\"}',\n",
              " '{\"class_name\": \"Sequential\", \"config\": {\"name\": \"sequential\", \"layers\": [{\"class_name\": \"Dense\", \"config\": {\"name\": \"dense\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 32, \"activation\": \"relu\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null, \"batch_input_shape\": [null, 8]}}, {\"class_name\": \"Dropout\", \"config\": {\"name\": \"dropout\", \"trainable\": true, \"dtype\": \"float32\", \"rate\": 0, \"noise_shape\": null, \"seed\": null}}, {\"class_name\": \"Dense\", \"config\": {\"name\": \"Output\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 1, \"activation\": \"linear\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}}], \"build_input_shape\": [null, 8]}, \"keras_version\": \"2.2.4-tf\", \"backend\": \"tensorflow\"}',\n",
              " '{\"class_name\": \"Sequential\", \"config\": {\"name\": \"sequential\", \"layers\": [{\"class_name\": \"Dense\", \"config\": {\"name\": \"dense\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 32, \"activation\": \"relu\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null, \"batch_input_shape\": [null, 8]}}, {\"class_name\": \"Dropout\", \"config\": {\"name\": \"dropout\", \"trainable\": true, \"dtype\": \"float32\", \"rate\": 0, \"noise_shape\": null, \"seed\": null}}, {\"class_name\": \"Dense\", \"config\": {\"name\": \"Output\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 1, \"activation\": \"linear\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}}], \"build_input_shape\": [null, 8]}, \"keras_version\": \"2.2.4-tf\", \"backend\": \"tensorflow\"}',\n",
              " '{\"class_name\": \"Sequential\", \"config\": {\"name\": \"sequential\", \"layers\": [{\"class_name\": \"Dense\", \"config\": {\"name\": \"dense\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 32, \"activation\": \"elu\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null, \"batch_input_shape\": [null, 8]}}, {\"class_name\": \"Dropout\", \"config\": {\"name\": \"dropout\", \"trainable\": true, \"dtype\": \"float32\", \"rate\": 0, \"noise_shape\": null, \"seed\": null}}, {\"class_name\": \"Dense\", \"config\": {\"name\": \"dense_1\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 32, \"activation\": \"elu\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}}, {\"class_name\": \"Dropout\", \"config\": {\"name\": \"dropout_1\", \"trainable\": true, \"dtype\": \"float32\", \"rate\": 0, \"noise_shape\": null, \"seed\": null}}, {\"class_name\": \"Dense\", \"config\": {\"name\": \"dense_2\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 32, \"activation\": \"elu\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}}, {\"class_name\": \"Dropout\", \"config\": {\"name\": \"dropout_2\", \"trainable\": true, \"dtype\": \"float32\", \"rate\": 0, \"noise_shape\": null, \"seed\": null}}, {\"class_name\": \"Dense\", \"config\": {\"name\": \"Output\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 1, \"activation\": \"linear\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}}], \"build_input_shape\": [null, 8]}, \"keras_version\": \"2.2.4-tf\", \"backend\": \"tensorflow\"}',\n",
              " '{\"class_name\": \"Sequential\", \"config\": {\"name\": \"sequential\", \"layers\": [{\"class_name\": \"Dense\", \"config\": {\"name\": \"dense\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 32, \"activation\": \"elu\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null, \"batch_input_shape\": [null, 8]}}, {\"class_name\": \"Dropout\", \"config\": {\"name\": \"dropout\", \"trainable\": true, \"dtype\": \"float32\", \"rate\": 0, \"noise_shape\": null, \"seed\": null}}, {\"class_name\": \"Dense\", \"config\": {\"name\": \"dense_1\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 32, \"activation\": \"elu\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}}, {\"class_name\": \"Dropout\", \"config\": {\"name\": \"dropout_1\", \"trainable\": true, \"dtype\": \"float32\", \"rate\": 0, \"noise_shape\": null, \"seed\": null}}, {\"class_name\": \"Dense\", \"config\": {\"name\": \"Output\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 1, \"activation\": \"linear\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}}], \"build_input_shape\": [null, 8]}, \"keras_version\": \"2.2.4-tf\", \"backend\": \"tensorflow\"}',\n",
              " '{\"class_name\": \"Sequential\", \"config\": {\"name\": \"sequential\", \"layers\": [{\"class_name\": \"Dense\", \"config\": {\"name\": \"dense\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 64, \"activation\": \"relu\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null, \"batch_input_shape\": [null, 8]}}, {\"class_name\": \"Dropout\", \"config\": {\"name\": \"dropout\", \"trainable\": true, \"dtype\": \"float32\", \"rate\": 0, \"noise_shape\": null, \"seed\": null}}, {\"class_name\": \"Dense\", \"config\": {\"name\": \"dense_1\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 64, \"activation\": \"relu\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}}, {\"class_name\": \"Dropout\", \"config\": {\"name\": \"dropout_1\", \"trainable\": true, \"dtype\": \"float32\", \"rate\": 0, \"noise_shape\": null, \"seed\": null}}, {\"class_name\": \"Dense\", \"config\": {\"name\": \"Output\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 1, \"activation\": \"linear\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}}], \"build_input_shape\": [null, 8]}, \"keras_version\": \"2.2.4-tf\", \"backend\": \"tensorflow\"}',\n",
              " '{\"class_name\": \"Sequential\", \"config\": {\"name\": \"sequential\", \"layers\": [{\"class_name\": \"Dense\", \"config\": {\"name\": \"dense\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 64, \"activation\": \"relu\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null, \"batch_input_shape\": [null, 8]}}, {\"class_name\": \"Dropout\", \"config\": {\"name\": \"dropout\", \"trainable\": true, \"dtype\": \"float32\", \"rate\": 0, \"noise_shape\": null, \"seed\": null}}, {\"class_name\": \"Dense\", \"config\": {\"name\": \"dense_1\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 64, \"activation\": \"relu\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}}, {\"class_name\": \"Dropout\", \"config\": {\"name\": \"dropout_1\", \"trainable\": true, \"dtype\": \"float32\", \"rate\": 0, \"noise_shape\": null, \"seed\": null}}, {\"class_name\": \"Dense\", \"config\": {\"name\": \"Output\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 1, \"activation\": \"linear\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}}], \"build_input_shape\": [null, 8]}, \"keras_version\": \"2.2.4-tf\", \"backend\": \"tensorflow\"}',\n",
              " '{\"class_name\": \"Sequential\", \"config\": {\"name\": \"sequential\", \"layers\": [{\"class_name\": \"Dense\", \"config\": {\"name\": \"dense\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 32, \"activation\": \"relu\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null, \"batch_input_shape\": [null, 8]}}, {\"class_name\": \"Dropout\", \"config\": {\"name\": \"dropout\", \"trainable\": true, \"dtype\": \"float32\", \"rate\": 0, \"noise_shape\": null, \"seed\": null}}, {\"class_name\": \"Dense\", \"config\": {\"name\": \"Output\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 1, \"activation\": \"linear\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}}], \"build_input_shape\": [null, 8]}, \"keras_version\": \"2.2.4-tf\", \"backend\": \"tensorflow\"}',\n",
              " '{\"class_name\": \"Sequential\", \"config\": {\"name\": \"sequential\", \"layers\": [{\"class_name\": \"Dense\", \"config\": {\"name\": \"dense\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 32, \"activation\": \"relu\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null, \"batch_input_shape\": [null, 8]}}, {\"class_name\": \"Dropout\", \"config\": {\"name\": \"dropout\", \"trainable\": true, \"dtype\": \"float32\", \"rate\": 0, \"noise_shape\": null, \"seed\": null}}, {\"class_name\": \"Dense\", \"config\": {\"name\": \"Output\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 1, \"activation\": \"linear\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}}], \"build_input_shape\": [null, 8]}, \"keras_version\": \"2.2.4-tf\", \"backend\": \"tensorflow\"}',\n",
              " '{\"class_name\": \"Sequential\", \"config\": {\"name\": \"sequential\", \"layers\": [{\"class_name\": \"Dense\", \"config\": {\"name\": \"dense\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 32, \"activation\": \"relu\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null, \"batch_input_shape\": [null, 8]}}, {\"class_name\": \"Dropout\", \"config\": {\"name\": \"dropout\", \"trainable\": true, \"dtype\": \"float32\", \"rate\": 0, \"noise_shape\": null, \"seed\": null}}, {\"class_name\": \"Dense\", \"config\": {\"name\": \"dense_1\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 32, \"activation\": \"relu\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}}, {\"class_name\": \"Dropout\", \"config\": {\"name\": \"dropout_1\", \"trainable\": true, \"dtype\": \"float32\", \"rate\": 0, \"noise_shape\": null, \"seed\": null}}, {\"class_name\": \"Dense\", \"config\": {\"name\": \"Output\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 1, \"activation\": \"linear\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}}], \"build_input_shape\": [null, 8]}, \"keras_version\": \"2.2.4-tf\", \"backend\": \"tensorflow\"}',\n",
              " '{\"class_name\": \"Sequential\", \"config\": {\"name\": \"sequential\", \"layers\": [{\"class_name\": \"Dense\", \"config\": {\"name\": \"dense\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 32, \"activation\": \"elu\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null, \"batch_input_shape\": [null, 8]}}, {\"class_name\": \"Dropout\", \"config\": {\"name\": \"dropout\", \"trainable\": true, \"dtype\": \"float32\", \"rate\": 0, \"noise_shape\": null, \"seed\": null}}, {\"class_name\": \"Dense\", \"config\": {\"name\": \"Output\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 1, \"activation\": \"linear\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}}], \"build_input_shape\": [null, 8]}, \"keras_version\": \"2.2.4-tf\", \"backend\": \"tensorflow\"}',\n",
              " '{\"class_name\": \"Sequential\", \"config\": {\"name\": \"sequential\", \"layers\": [{\"class_name\": \"Dense\", \"config\": {\"name\": \"dense\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 64, \"activation\": \"elu\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null, \"batch_input_shape\": [null, 8]}}, {\"class_name\": \"Dropout\", \"config\": {\"name\": \"dropout\", \"trainable\": true, \"dtype\": \"float32\", \"rate\": 0, \"noise_shape\": null, \"seed\": null}}, {\"class_name\": \"Dense\", \"config\": {\"name\": \"Output\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 1, \"activation\": \"linear\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}}], \"build_input_shape\": [null, 8]}, \"keras_version\": \"2.2.4-tf\", \"backend\": \"tensorflow\"}',\n",
              " '{\"class_name\": \"Sequential\", \"config\": {\"name\": \"sequential\", \"layers\": [{\"class_name\": \"Dense\", \"config\": {\"name\": \"dense\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 32, \"activation\": \"elu\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null, \"batch_input_shape\": [null, 8]}}, {\"class_name\": \"Dropout\", \"config\": {\"name\": \"dropout\", \"trainable\": true, \"dtype\": \"float32\", \"rate\": 0, \"noise_shape\": null, \"seed\": null}}, {\"class_name\": \"Dense\", \"config\": {\"name\": \"dense_1\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 32, \"activation\": \"elu\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}}, {\"class_name\": \"Dropout\", \"config\": {\"name\": \"dropout_1\", \"trainable\": true, \"dtype\": \"float32\", \"rate\": 0, \"noise_shape\": null, \"seed\": null}}, {\"class_name\": \"Dense\", \"config\": {\"name\": \"Output\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 1, \"activation\": \"linear\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}}], \"build_input_shape\": [null, 8]}, \"keras_version\": \"2.2.4-tf\", \"backend\": \"tensorflow\"}',\n",
              " '{\"class_name\": \"Sequential\", \"config\": {\"name\": \"sequential\", \"layers\": [{\"class_name\": \"Dense\", \"config\": {\"name\": \"dense\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 64, \"activation\": \"elu\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null, \"batch_input_shape\": [null, 8]}}, {\"class_name\": \"Dropout\", \"config\": {\"name\": \"dropout\", \"trainable\": true, \"dtype\": \"float32\", \"rate\": 0, \"noise_shape\": null, \"seed\": null}}, {\"class_name\": \"Dense\", \"config\": {\"name\": \"dense_1\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 64, \"activation\": \"elu\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}}, {\"class_name\": \"Dropout\", \"config\": {\"name\": \"dropout_1\", \"trainable\": true, \"dtype\": \"float32\", \"rate\": 0, \"noise_shape\": null, \"seed\": null}}, {\"class_name\": \"Dense\", \"config\": {\"name\": \"dense_2\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 64, \"activation\": \"elu\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}}, {\"class_name\": \"Dropout\", \"config\": {\"name\": \"dropout_2\", \"trainable\": true, \"dtype\": \"float32\", \"rate\": 0, \"noise_shape\": null, \"seed\": null}}, {\"class_name\": \"Dense\", \"config\": {\"name\": \"Output\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 1, \"activation\": \"linear\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}}], \"build_input_shape\": [null, 8]}, \"keras_version\": \"2.2.4-tf\", \"backend\": \"tensorflow\"}',\n",
              " '{\"class_name\": \"Sequential\", \"config\": {\"name\": \"sequential\", \"layers\": [{\"class_name\": \"Dense\", \"config\": {\"name\": \"dense\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 64, \"activation\": \"relu\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null, \"batch_input_shape\": [null, 8]}}, {\"class_name\": \"Dropout\", \"config\": {\"name\": \"dropout\", \"trainable\": true, \"dtype\": \"float32\", \"rate\": 0, \"noise_shape\": null, \"seed\": null}}, {\"class_name\": \"Dense\", \"config\": {\"name\": \"dense_1\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 64, \"activation\": \"relu\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}}, {\"class_name\": \"Dropout\", \"config\": {\"name\": \"dropout_1\", \"trainable\": true, \"dtype\": \"float32\", \"rate\": 0, \"noise_shape\": null, \"seed\": null}}, {\"class_name\": \"Dense\", \"config\": {\"name\": \"dense_2\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 64, \"activation\": \"relu\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}}, {\"class_name\": \"Dropout\", \"config\": {\"name\": \"dropout_2\", \"trainable\": true, \"dtype\": \"float32\", \"rate\": 0, \"noise_shape\": null, \"seed\": null}}, {\"class_name\": \"Dense\", \"config\": {\"name\": \"Output\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 1, \"activation\": \"linear\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}}], \"build_input_shape\": [null, 8]}, \"keras_version\": \"2.2.4-tf\", \"backend\": \"tensorflow\"}',\n",
              " '{\"class_name\": \"Sequential\", \"config\": {\"name\": \"sequential\", \"layers\": [{\"class_name\": \"Dense\", \"config\": {\"name\": \"dense\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 64, \"activation\": \"relu\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null, \"batch_input_shape\": [null, 8]}}, {\"class_name\": \"Dropout\", \"config\": {\"name\": \"dropout\", \"trainable\": true, \"dtype\": \"float32\", \"rate\": 0, \"noise_shape\": null, \"seed\": null}}, {\"class_name\": \"Dense\", \"config\": {\"name\": \"dense_1\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 64, \"activation\": \"relu\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}}, {\"class_name\": \"Dropout\", \"config\": {\"name\": \"dropout_1\", \"trainable\": true, \"dtype\": \"float32\", \"rate\": 0, \"noise_shape\": null, \"seed\": null}}, {\"class_name\": \"Dense\", \"config\": {\"name\": \"Output\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 1, \"activation\": \"linear\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}}], \"build_input_shape\": [null, 8]}, \"keras_version\": \"2.2.4-tf\", \"backend\": \"tensorflow\"}',\n",
              " '{\"class_name\": \"Sequential\", \"config\": {\"name\": \"sequential\", \"layers\": [{\"class_name\": \"Dense\", \"config\": {\"name\": \"dense\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 64, \"activation\": \"elu\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null, \"batch_input_shape\": [null, 8]}}, {\"class_name\": \"Dropout\", \"config\": {\"name\": \"dropout\", \"trainable\": true, \"dtype\": \"float32\", \"rate\": 0, \"noise_shape\": null, \"seed\": null}}, {\"class_name\": \"Dense\", \"config\": {\"name\": \"dense_1\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 64, \"activation\": \"elu\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}}, {\"class_name\": \"Dropout\", \"config\": {\"name\": \"dropout_1\", \"trainable\": true, \"dtype\": \"float32\", \"rate\": 0, \"noise_shape\": null, \"seed\": null}}, {\"class_name\": \"Dense\", \"config\": {\"name\": \"Output\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 1, \"activation\": \"linear\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}}], \"build_input_shape\": [null, 8]}, \"keras_version\": \"2.2.4-tf\", \"backend\": \"tensorflow\"}',\n",
              " '{\"class_name\": \"Sequential\", \"config\": {\"name\": \"sequential\", \"layers\": [{\"class_name\": \"Dense\", \"config\": {\"name\": \"dense\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 32, \"activation\": \"relu\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null, \"batch_input_shape\": [null, 8]}}, {\"class_name\": \"Dropout\", \"config\": {\"name\": \"dropout\", \"trainable\": true, \"dtype\": \"float32\", \"rate\": 0, \"noise_shape\": null, \"seed\": null}}, {\"class_name\": \"Dense\", \"config\": {\"name\": \"Output\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 1, \"activation\": \"linear\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}}], \"build_input_shape\": [null, 8]}, \"keras_version\": \"2.2.4-tf\", \"backend\": \"tensorflow\"}']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 389
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LNzMXExNSJf",
        "colab_type": "code",
        "outputId": "cb47f995-e898-442d-e170-cca01510cbeb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# accessing the saved weights for models\n",
        "scan_object.saved_weights"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[array([[ 0.13790904, -0.33760086,  0.08386634, -0.03706633,  0.37529024,\n",
              "           0.34208703,  0.08438694, -0.33212733, -0.29940102, -0.09167956,\n",
              "           0.1439695 ,  0.04062584,  0.28474128, -0.01372045,  0.02852014,\n",
              "          -0.07566615,  0.3310445 , -0.22987725, -0.25982893, -0.32808533,\n",
              "          -0.00442374,  0.15660849, -0.3494835 , -0.3208565 ,  0.3578102 ,\n",
              "          -0.21375912,  0.28686765, -0.12571412,  0.29847762,  0.13801068,\n",
              "          -0.32599643,  0.05314669],\n",
              "         [-0.01840215,  0.13348971,  0.30220243, -0.21435821, -0.39327326,\n",
              "           0.19965827, -0.3239289 , -0.00202691, -0.2568516 ,  0.10753122,\n",
              "           0.18386522,  0.13797103, -0.0784177 ,  0.36907834,  0.27507246,\n",
              "           0.30391604, -0.40162173,  0.14887421,  0.07375609, -0.30301887,\n",
              "           0.3620058 , -0.0424925 , -0.08110006,  0.15641741,  0.23126683,\n",
              "          -0.32670546, -0.10358158,  0.21574935, -0.1894446 ,  0.13040686,\n",
              "           0.2982007 , -0.14699031],\n",
              "         [ 0.36428654,  0.32525203,  0.32190356,  0.06920181, -0.09823801,\n",
              "           0.15401293, -0.01317233, -0.33911017, -0.1379698 , -0.08689733,\n",
              "           0.24409813, -0.02046692, -0.25380492, -0.2909472 ,  0.30956385,\n",
              "          -0.20529203, -0.01871823, -0.13758451,  0.26391408,  0.2879544 ,\n",
              "          -0.25996134, -0.14339624,  0.27583987, -0.23908252,  0.21291108,\n",
              "          -0.22462587, -0.23928642, -0.2425032 , -0.21287261, -0.11117683,\n",
              "           0.12817867, -0.20409955],\n",
              "         [ 0.0596935 , -0.16833472, -0.2378496 ,  0.06187548, -0.03464355,\n",
              "           0.21998739,  0.38983163,  0.24155384,  0.21891181, -0.04104386,\n",
              "          -0.18214977, -0.30954087,  0.24709812, -0.4152915 ,  0.00120907,\n",
              "          -0.24873285,  0.11020546, -0.30634356, -0.2996064 ,  0.08711631,\n",
              "          -0.1511462 , -0.3350795 , -0.37341464, -0.29858115, -0.34477141,\n",
              "           0.03483094, -0.31233716,  0.12282318,  0.35387298, -0.06191141,\n",
              "           0.28309643, -0.39688239],\n",
              "         [ 0.02530808, -0.1129159 , -0.06209155,  0.38732266,  0.36368537,\n",
              "           0.18188332,  0.35930884, -0.11242498,  0.1642887 ,  0.37656268,\n",
              "           0.08898558, -0.12009825, -0.11291481,  0.07174738, -0.06970474,\n",
              "           0.33180627,  0.34649757,  0.09301893, -0.12253893,  0.19365633,\n",
              "          -0.21981548, -0.32953098,  0.07659502,  0.303487  , -0.24354935,\n",
              "          -0.02661673, -0.24803162,  0.17231202,  0.35733607, -0.17310344,\n",
              "          -0.15362428, -0.00566735],\n",
              "         [-0.10685636, -0.26503724,  0.0858161 , -0.03112746,  0.11924046,\n",
              "           0.30539933,  0.22068651, -0.21436326,  0.276576  , -0.03134196,\n",
              "          -0.386691  , -0.20132847,  0.13793735, -0.13536276,  0.00862073,\n",
              "          -0.06601183, -0.11844195, -0.36658862,  0.04228108, -0.32444945,\n",
              "          -0.14246476, -0.2684485 , -0.34723905, -0.28777078, -0.11391166,\n",
              "           0.15894714, -0.28205734,  0.06170198, -0.06488733,  0.21248935,\n",
              "           0.12934132,  0.22750953],\n",
              "         [ 0.01574376, -0.33369452,  0.04161354,  0.24003617,  0.10864335,\n",
              "           0.33847812,  0.02252414, -0.17593192, -0.21681976,  0.32032284,\n",
              "           0.2832683 ,  0.01356375,  0.10461849, -0.21228339, -0.35869995,\n",
              "           0.16738857, -0.26748884,  0.26861545,  0.31636435, -0.0605716 ,\n",
              "          -0.1986492 , -0.14128424, -0.13661048,  0.05661457,  0.32816607,\n",
              "           0.20225348,  0.13587108, -0.3604473 , -0.20634948,  0.02675179,\n",
              "          -0.06912129, -0.00133892],\n",
              "         [-0.20781994, -0.17208956, -0.26745284, -0.2601554 ,  0.05885516,\n",
              "          -0.39800563, -0.00558929, -0.20275939, -0.16208759,  0.1352526 ,\n",
              "          -0.1348861 , -0.14360976,  0.14941038,  0.14282179, -0.20650105,\n",
              "          -0.33988932, -0.04949355, -0.02942085, -0.20802532,  0.2542502 ,\n",
              "           0.14237213, -0.062219  ,  0.16776882,  0.27295437, -0.03065085,\n",
              "          -0.37826964,  0.36068213,  0.03238479, -0.02617582,  0.15691422,\n",
              "          -0.07358122, -0.1472753 ]], dtype=float32),\n",
              "  array([ 0.01269044,  0.03720072, -0.02875169,  0.0372306 ,  0.02218693,\n",
              "          0.03740945, -0.03657416, -0.03510065,  0.0370322 , -0.03760143,\n",
              "          0.03698291,  0.03732532,  0.03736907,  0.03729817, -0.012676  ,\n",
              "          0.03199371,  0.03781055, -0.03461995, -0.03658831,  0.0370873 ,\n",
              "          0.03823638, -0.01955356, -0.03618501, -0.02030838,  0.03816371,\n",
              "          0.03630799,  0.03817483,  0.03721422, -0.03395655, -0.03641587,\n",
              "         -0.03676917,  0.03519377], dtype=float32),\n",
              "  array([[ 0.2192371 , -0.3024379 , -0.02134031, ...,  0.24447267,\n",
              "           0.18596041,  0.16094132],\n",
              "         [-0.26630336,  0.30522454, -0.1528652 , ...,  0.20216388,\n",
              "           0.1659913 ,  0.15009274],\n",
              "         [-0.30180588, -0.03973057,  0.09024456, ..., -0.16346312,\n",
              "           0.29937452,  0.00429624],\n",
              "         ...,\n",
              "         [-0.28825134, -0.18484405, -0.00818717, ..., -0.13824037,\n",
              "           0.16962767,  0.12504797],\n",
              "         [-0.22642437, -0.13192983, -0.18321231, ..., -0.1696539 ,\n",
              "          -0.1088872 , -0.14262635],\n",
              "         [-0.12821592,  0.08450519, -0.1164346 , ...,  0.14544424,\n",
              "          -0.2647196 ,  0.01983975]], dtype=float32),\n",
              "  array([ 0.03716275,  0.03714924, -0.03581329,  0.01741107,  0.03676844,\n",
              "         -0.03734614,  0.0383809 , -0.0362481 , -0.03669753,  0.03692459,\n",
              "         -0.03680311, -0.03655403,  0.03682948, -0.03651246,  0.00850468,\n",
              "         -0.03663097,  0.0040155 ,  0.03655661, -0.03668995,  0.0371172 ,\n",
              "         -0.03541607,  0.03737664,  0.02084825,  0.03702119, -0.0367951 ,\n",
              "         -0.03702698,  0.03676831, -0.03709294, -0.029976  ,  0.03755915,\n",
              "          0.037046  ,  0.03693155], dtype=float32),\n",
              "  array([[ 0.15747955,  0.14258912, -0.2159889 , ..., -0.17663221,\n",
              "           0.27310717, -0.09377478],\n",
              "         [ 0.13851033,  0.06291161,  0.17299142, ...,  0.12409225,\n",
              "           0.14871863, -0.18958223],\n",
              "         [-0.14865728,  0.02060977, -0.15023261, ..., -0.27804384,\n",
              "           0.24739164,  0.2584372 ],\n",
              "         ...,\n",
              "         [ 0.0617869 ,  0.17051846, -0.1647448 , ..., -0.17676844,\n",
              "           0.208749  ,  0.22962673],\n",
              "         [ 0.07142852,  0.22161809,  0.22932598, ..., -0.02736822,\n",
              "          -0.01652539,  0.05722384],\n",
              "         [ 0.21928231,  0.02146594, -0.10280952, ..., -0.00605116,\n",
              "          -0.05627538,  0.14869274]], dtype=float32),\n",
              "  array([ 0.03670813,  0.03679192,  0.03665694, -0.03664763, -0.03653289,\n",
              "         -0.03664238,  0.03670435, -0.03635755,  0.03783051,  0.03702784,\n",
              "         -0.03675389, -0.03658178, -0.03662986, -0.03665838,  0.03686307,\n",
              "         -0.03651578, -0.036417  ,  0.03676961,  0.03684358, -0.03665202,\n",
              "         -0.03653496, -0.03663574, -0.03656661,  0.0368031 , -0.03661191,\n",
              "         -0.03653936, -0.03645413, -0.03656176,  0.03677541, -0.03660623,\n",
              "         -0.03657197, -0.03694262], dtype=float32),\n",
              "  array([[ 0.4202085 ],\n",
              "         [ 0.23876199],\n",
              "         [ 0.4013485 ],\n",
              "         [-0.34361658],\n",
              "         [-0.43502104],\n",
              "         [-0.27847275],\n",
              "         [ 0.35883725],\n",
              "         [-0.05589144],\n",
              "         [ 0.04751161],\n",
              "         [ 0.08857229],\n",
              "         [-0.18662496],\n",
              "         [-0.39470035],\n",
              "         [-0.07837535],\n",
              "         [-0.37662792],\n",
              "         [ 0.13636024],\n",
              "         [-0.45473576],\n",
              "         [-0.34786907],\n",
              "         [ 0.37867147],\n",
              "         [ 0.25500947],\n",
              "         [-0.28593582],\n",
              "         [-0.32244223],\n",
              "         [-0.16929965],\n",
              "         [-0.09627852],\n",
              "         [ 0.19637734],\n",
              "         [-0.3724621 ],\n",
              "         [-0.24379525],\n",
              "         [-0.27849987],\n",
              "         [-0.12389898],\n",
              "         [ 0.14622828],\n",
              "         [-0.288902  ],\n",
              "         [-0.35948196],\n",
              "         [-0.07433985]], dtype=float32),\n",
              "  array([0.0366381], dtype=float32)],\n",
              " [array([[-0.12127094,  0.1626697 , -0.14012504, -0.09986528, -0.2354172 ,\n",
              "           0.3034594 ,  0.36606842, -0.09023929, -0.22929002, -0.01886836,\n",
              "           0.03702735,  0.10372613,  0.27448228,  0.08606522,  0.09757479,\n",
              "          -0.05297248, -0.19433104,  0.2791967 ,  0.23833556, -0.1979703 ,\n",
              "           0.01231796, -0.1836621 , -0.19239667, -0.10257019,  0.02868395,\n",
              "          -0.15238775, -0.145746  , -0.2405656 , -0.16830528, -0.27766663,\n",
              "          -0.2354027 , -0.01495078,  0.21024084,  0.01623939, -0.06675302,\n",
              "          -0.02511256, -0.13279094,  0.09429684,  0.20760928, -0.14534067,\n",
              "          -0.11947923,  0.07787821,  0.01154228, -0.18694408, -0.10990355,\n",
              "          -0.01299968, -0.16615202,  0.06750373, -0.25956365, -0.13524961,\n",
              "          -0.2621483 , -0.14005582, -0.21926107, -0.21186031, -0.23414233,\n",
              "           0.10800596,  0.11471527,  0.2024441 , -0.20225586,  0.13403201,\n",
              "          -0.00876659, -0.15729131, -0.12606311, -0.14264311],\n",
              "         [ 0.00893944,  0.28637326,  0.2116006 ,  0.1294006 , -0.06319544,\n",
              "           0.20551053,  0.05266465, -0.11585034,  0.18126701, -0.265482  ,\n",
              "           0.21616113, -0.00531557, -0.09802474,  0.05348297, -0.15468745,\n",
              "           0.06337687, -0.07073615,  0.02080903,  0.2303546 ,  0.16902275,\n",
              "           0.23388842,  0.11090352, -0.05234135, -0.20492874, -0.28244838,\n",
              "           0.16375734,  0.18530658,  0.17667143,  0.16680901, -0.16282386,\n",
              "           0.1789164 ,  0.3064131 , -0.0284364 , -0.11592133, -0.21016926,\n",
              "           0.22071323,  0.05504371, -0.24854168,  0.19402185, -0.07380524,\n",
              "          -0.04822756,  0.1430649 ,  0.13323218, -0.17856945,  0.10115875,\n",
              "          -0.15025347,  0.00716417, -0.02786029,  0.0769878 , -0.1649291 ,\n",
              "           0.04087919, -0.26488006,  0.23783319, -0.1160678 , -0.05782455,\n",
              "          -0.26053408, -0.13911127, -0.28918657,  0.15735595, -0.22251885,\n",
              "           0.13182364,  0.0295067 , -0.1155709 ,  0.06557   ],\n",
              "         [ 0.1400586 , -0.02253473, -0.11699114, -0.24849705, -0.1608918 ,\n",
              "          -0.27040976, -0.23380265,  0.12983015,  0.25861466, -0.13418747,\n",
              "          -0.18170378,  0.13602312, -0.2368663 ,  0.03435757, -0.11650443,\n",
              "           0.07724407,  0.01896425, -0.03760959,  0.19790664, -0.12347727,\n",
              "          -0.05938889,  0.00797539,  0.27973273, -0.13207668, -0.26998806,\n",
              "           0.04111106, -0.0520624 ,  0.00652949,  0.0954436 ,  0.11497601,\n",
              "           0.1215014 , -0.09339539, -0.03519293, -0.16806069,  0.22126228,\n",
              "          -0.18865693, -0.25398088, -0.1064088 ,  0.04344175, -0.1611422 ,\n",
              "           0.22773379, -0.23047383,  0.2448702 , -0.18050812, -0.0434074 ,\n",
              "          -0.17328599,  0.21481782,  0.24736759,  0.12524812, -0.16691089,\n",
              "          -0.03601757, -0.26092467, -0.13267013,  0.126944  ,  0.03587848,\n",
              "           0.0267091 , -0.0736595 , -0.25154725,  0.08802904,  0.09093455,\n",
              "          -0.23902519, -0.02149609, -0.14280744,  0.00276875],\n",
              "         [ 0.08352307,  0.2838364 ,  0.27709776, -0.11003983,  0.24205658,\n",
              "           0.18171804,  0.07976304,  0.09588476, -0.00873935,  0.10320368,\n",
              "           0.17315938, -0.06396288,  0.01117024, -0.11043586,  0.24114862,\n",
              "          -0.18296379, -0.05389213,  0.0684443 ,  0.18650979, -0.21609402,\n",
              "          -0.0351751 ,  0.06331578,  0.01901011,  0.12803444,  0.07486925,\n",
              "           0.25484028,  0.18440592, -0.2605618 , -0.1301853 ,  0.01184943,\n",
              "           0.18383461,  0.24682274, -0.13511293, -0.24864891,  0.03222211,\n",
              "           0.0375995 , -0.08162377, -0.00408123, -0.01057255, -0.22157459,\n",
              "           0.02290024,  0.02352429, -0.2256475 , -0.03070646, -0.07306319,\n",
              "          -0.04494671,  0.2777471 ,  0.16011669, -0.25499126,  0.14364626,\n",
              "          -0.01338271,  0.12559916, -0.05676143,  0.17935476,  0.01386661,\n",
              "           0.06661005, -0.17909734,  0.04312102,  0.19106029, -0.10163201,\n",
              "           0.15790178,  0.27742663,  0.0529741 ,  0.18575124],\n",
              "         [-0.2178025 , -0.19213714, -0.01433827, -0.2871956 , -0.22372927,\n",
              "          -0.18891717,  0.24270707, -0.1515085 ,  0.26128575, -0.23292373,\n",
              "           0.11846107,  0.02924619,  0.28493088, -0.08415133, -0.1609723 ,\n",
              "          -0.27911887, -0.14237972,  0.06125428,  0.2686038 , -0.2451939 ,\n",
              "           0.05637468,  0.2483857 ,  0.01675983, -0.13792494, -0.21478146,\n",
              "          -0.03037378, -0.02102755,  0.2670742 , -0.08893244,  0.16523346,\n",
              "           0.06814857, -0.03726268, -0.14853512, -0.10822762, -0.16405809,\n",
              "           0.2427162 ,  0.09261458,  0.2080741 , -0.2719558 , -0.2573555 ,\n",
              "          -0.19586071, -0.24197534, -0.29676953, -0.02399438,  0.03228938,\n",
              "          -0.09917882, -0.23695992,  0.20953582,  0.02548132,  0.14978404,\n",
              "          -0.16951753, -0.07426841,  0.07716019,  0.11269492, -0.05496162,\n",
              "           0.24287118,  0.1137886 , -0.24310909, -0.11600754, -0.20710677,\n",
              "          -0.25536472,  0.25242907, -0.03279463, -0.10937125],\n",
              "         [ 0.03034688,  0.06340583,  0.19219474, -0.02662885, -0.18609336,\n",
              "          -0.20826836,  0.06196544,  0.24115828, -0.20671937,  0.21223854,\n",
              "          -0.19027483, -0.26753446,  0.18655026, -0.09012645, -0.10555904,\n",
              "          -0.13172522, -0.01293737, -0.01513487,  0.00523479, -0.21197419,\n",
              "          -0.14148499,  0.04217486, -0.18133813,  0.22009946, -0.02059252,\n",
              "          -0.23877114, -0.02203713, -0.21508056, -0.00843207,  0.05353007,\n",
              "           0.22779235, -0.02034777, -0.0751364 ,  0.23492143,  0.2024255 ,\n",
              "           0.24262458,  0.22365442, -0.11446573, -0.12273765, -0.14289166,\n",
              "           0.1745986 ,  0.04191519,  0.08077785, -0.08614541, -0.04808873,\n",
              "          -0.10085414,  0.27217725,  0.28209487,  0.14454232, -0.10042249,\n",
              "           0.16247116,  0.22546417, -0.15620734,  0.05362048, -0.13727605,\n",
              "          -0.19170323, -0.10003675,  0.02137784,  0.1913984 ,  0.01505169,\n",
              "          -0.22208527, -0.1136566 , -0.2863387 ,  0.10074832],\n",
              "         [-0.06538182, -0.0520965 , -0.12270135,  0.2296481 ,  0.25729755,\n",
              "           0.10972366,  0.20228356,  0.0578196 ,  0.1518288 , -0.23640686,\n",
              "           0.04512342,  0.28616616, -0.12292895, -0.19536725,  0.2306877 ,\n",
              "           0.19668281, -0.11672492, -0.09171563,  0.17245984,  0.1839997 ,\n",
              "          -0.17718466, -0.29676312,  0.12195402,  0.1563778 ,  0.21249382,\n",
              "           0.1274758 ,  0.11489093, -0.10755344, -0.10494073,  0.04157031,\n",
              "           0.23718765, -0.06501994,  0.10017543,  0.2733157 , -0.21213748,\n",
              "           0.26020622,  0.179232  ,  0.1458735 ,  0.26145542, -0.19236779,\n",
              "           0.15779373,  0.12167148,  0.02800274,  0.01170011,  0.00889458,\n",
              "           0.20910901, -0.14997235,  0.10474369,  0.1996536 ,  0.1698567 ,\n",
              "           0.14851011,  0.14165264, -0.09728282, -0.13764994,  0.01888825,\n",
              "           0.20205912,  0.2809048 ,  0.09716233, -0.2313281 , -0.2871846 ,\n",
              "           0.198249  , -0.02239457,  0.19424108, -0.03272411],\n",
              "         [-0.19824181,  0.14197138,  0.16741145,  0.14762722, -0.10781377,\n",
              "           0.15125778,  0.0901785 , -0.22586437, -0.10322864, -0.00140049,\n",
              "          -0.01665235,  0.17454058, -0.25596195, -0.29922798,  0.27209792,\n",
              "          -0.2603176 , -0.1487096 ,  0.21126103,  0.07503259,  0.2759641 ,\n",
              "           0.23926654, -0.18307161, -0.02908879,  0.0920508 ,  0.03628654,\n",
              "           0.25748292,  0.23061658,  0.23275155, -0.15984814, -0.14804578,\n",
              "          -0.01276803,  0.01157565, -0.18802276,  0.07254341, -0.14234877,\n",
              "           0.16871987, -0.1198767 ,  0.18772577, -0.05641254, -0.1553175 ,\n",
              "          -0.17541413, -0.18587711,  0.04901715,  0.18084358, -0.17534955,\n",
              "           0.12675315, -0.23536897,  0.2387271 , -0.0414799 ,  0.11475009,\n",
              "           0.01956427,  0.08436495, -0.09727734,  0.1809448 ,  0.01339322,\n",
              "          -0.21848576,  0.13167398, -0.04968947,  0.03612933,  0.20646323,\n",
              "           0.21014468, -0.01158175,  0.12319687, -0.14169689]],\n",
              "        dtype=float32),\n",
              "  array([ 0.13932018,  0.16308255,  0.10057264, -0.0107806 , -0.09083761,\n",
              "          0.086432  ,  0.1757178 ,  0.09654474, -0.11944604, -0.04773576,\n",
              "         -0.1046417 ,  0.08661301, -0.02577705,  0.15148233, -0.08799484,\n",
              "          0.16381353,  0.1494029 ,  0.15265721,  0.03907122,  0.00759411,\n",
              "          0.02811117,  0.12383918,  0.08792558, -0.08529009, -0.14820185,\n",
              "         -0.1289426 , -0.06486944,  0.02088455, -0.02771983, -0.05185661,\n",
              "          0.10021313,  0.13298088, -0.0354788 ,  0.12887593,  0.10694808,\n",
              "          0.0213671 ,  0.02911359,  0.1644273 ,  0.1476401 , -0.04373568,\n",
              "          0.11237717, -0.080365  ,  0.09615128, -0.06906053, -0.04563418,\n",
              "          0.12542228, -0.11263016, -0.11833775, -0.053618  ,  0.04593019,\n",
              "         -0.14580823, -0.00085515, -0.1422416 ,  0.12682183, -0.02038541,\n",
              "         -0.08378311,  0.12286182, -0.09184083, -0.04215605,  0.05891935,\n",
              "         -0.09110066, -0.12944221,  0.03842206, -0.15516561], dtype=float32),\n",
              "  array([[ 0.2681602 ],\n",
              "         [ 0.35144317],\n",
              "         [ 0.20134546],\n",
              "         [-0.03411962],\n",
              "         [-0.2137841 ],\n",
              "         [ 0.20790836],\n",
              "         [ 0.37421945],\n",
              "         [ 0.19094571],\n",
              "         [-0.2558571 ],\n",
              "         [-0.09721383],\n",
              "         [-0.18290615],\n",
              "         [ 0.1653946 ],\n",
              "         [ 0.00990025],\n",
              "         [ 0.3144108 ],\n",
              "         [-0.18251072],\n",
              "         [ 0.34699988],\n",
              "         [ 0.27439484],\n",
              "         [ 0.33141887],\n",
              "         [ 0.12433785],\n",
              "         [-0.00774713],\n",
              "         [ 0.07347668],\n",
              "         [ 0.24989866],\n",
              "         [ 0.15264387],\n",
              "         [-0.20649952],\n",
              "         [-0.31722215],\n",
              "         [-0.29015955],\n",
              "         [-0.15443291],\n",
              "         [ 0.03480313],\n",
              "         [-0.05612274],\n",
              "         [-0.12940222],\n",
              "         [ 0.18682733],\n",
              "         [ 0.2764426 ],\n",
              "         [-0.02739843],\n",
              "         [ 0.23360124],\n",
              "         [ 0.20727327],\n",
              "         [ 0.03665848],\n",
              "         [ 0.05595207],\n",
              "         [ 0.3281144 ],\n",
              "         [ 0.33050057],\n",
              "         [-0.08611386],\n",
              "         [ 0.22234857],\n",
              "         [-0.13188168],\n",
              "         [ 0.19371994],\n",
              "         [-0.17624515],\n",
              "         [-0.08661135],\n",
              "         [ 0.22257337],\n",
              "         [-0.22579138],\n",
              "         [-0.2387227 ],\n",
              "         [-0.14067991],\n",
              "         [ 0.0735333 ],\n",
              "         [-0.3474684 ],\n",
              "         [-0.03110024],\n",
              "         [-0.29762995],\n",
              "         [ 0.24716642],\n",
              "         [-0.06838783],\n",
              "         [-0.13855456],\n",
              "         [ 0.23637718],\n",
              "         [-0.16037291],\n",
              "         [-0.09462741],\n",
              "         [ 0.1473177 ],\n",
              "         [-0.18855676],\n",
              "         [-0.2722651 ],\n",
              "         [ 0.05124841],\n",
              "         [-0.31954068]], dtype=float32),\n",
              "  array([0.61784995], dtype=float32)],\n",
              " [array([[-0.11085243, -0.16826275, -0.13097936, -0.25848386,  0.05470273,\n",
              "           0.00675018,  0.24086301, -0.05930015, -0.05747706,  0.1665653 ,\n",
              "           0.2581736 , -0.26053083, -0.09619828, -0.26898056,  0.13476449,\n",
              "           0.01243765,  0.06430732,  0.10039897, -0.0693216 ,  0.09213156,\n",
              "           0.03219227,  0.14632772, -0.2563082 , -0.20799638, -0.05060867,\n",
              "          -0.22068635, -0.02940309, -0.01845325,  0.08531269, -0.14892437,\n",
              "          -0.16534428, -0.14436871, -0.14767243, -0.17098537,  0.16121164,\n",
              "           0.2034676 , -0.15025282,  0.19043854, -0.24756654, -0.27944958,\n",
              "           0.11270677, -0.04919349, -0.2736786 , -0.29471597, -0.04663517,\n",
              "           0.2826725 ,  0.07076447,  0.21186239,  0.27854237,  0.08237772,\n",
              "          -0.1097955 , -0.00424206,  0.23106228,  0.12164115, -0.15589929,\n",
              "          -0.0348464 ,  0.08105586, -0.13795872, -0.04385635, -0.28637204,\n",
              "          -0.04971441, -0.16760291,  0.09231414, -0.2492221 ],\n",
              "         [-0.21110211,  0.23395663,  0.07366161, -0.29786542,  0.03931069,\n",
              "           0.27855602, -0.26998198,  0.14765893, -0.04305984,  0.00952034,\n",
              "           0.21228236,  0.1363553 ,  0.19687714, -0.05576802, -0.25109172,\n",
              "           0.02683506,  0.00619226,  0.17328297, -0.2643427 , -0.15582879,\n",
              "          -0.13403137,  0.05221225,  0.30495426, -0.04016652,  0.10137124,\n",
              "          -0.20362923,  0.0919579 , -0.15272564,  0.07122927,  0.165065  ,\n",
              "           0.26314434, -0.09271891, -0.00639962,  0.01803715,  0.07451672,\n",
              "           0.01363914, -0.22046606, -0.05745265, -0.04318928, -0.21680535,\n",
              "           0.03017207,  0.12955429,  0.22165541, -0.06638696, -0.2335187 ,\n",
              "           0.19361603,  0.07978237,  0.24252096, -0.30411422,  0.29701576,\n",
              "          -0.26956758,  0.10177849, -0.00147362,  0.05971511,  0.10084306,\n",
              "           0.24890064, -0.14167552,  0.29233173, -0.26860112,  0.08583384,\n",
              "           0.02747089, -0.22261985, -0.12247435,  0.01270958],\n",
              "         [ 0.1797381 , -0.048833  , -0.0999552 , -0.14411737, -0.26699376,\n",
              "          -0.13116543, -0.00755986,  0.03852349, -0.13044344, -0.23781715,\n",
              "           0.12332048,  0.08396338, -0.14873336,  0.01870939,  0.02748179,\n",
              "           0.04841144, -0.14791465, -0.05390666,  0.28959635, -0.22379169,\n",
              "          -0.17744045, -0.13317165,  0.10948951, -0.20366544,  0.10828345,\n",
              "           0.05509751, -0.09168057,  0.02123452, -0.13763452,  0.03806609,\n",
              "           0.11170687,  0.07650488,  0.10056876, -0.23421246,  0.15197273,\n",
              "           0.05880611,  0.0669314 ,  0.05357928,  0.27250966, -0.28018513,\n",
              "           0.05191431, -0.24789922, -0.03900612, -0.25510353,  0.22653304,\n",
              "          -0.23316136,  0.06528212, -0.05995474, -0.00868299,  0.09921518,\n",
              "           0.28609213,  0.24212217,  0.12269934,  0.05362028,  0.19544713,\n",
              "          -0.06153789,  0.09119139,  0.19619228, -0.01352471,  0.2647171 ,\n",
              "           0.12737016,  0.22144653,  0.086983  , -0.05405533],\n",
              "         [ 0.12874591, -0.02820458, -0.16780433,  0.10073499,  0.08605878,\n",
              "          -0.05390716, -0.08432102,  0.05905213,  0.05662341,  0.22812179,\n",
              "           0.18512432,  0.1567001 ,  0.06876609,  0.18827814, -0.24017526,\n",
              "           0.02974454, -0.07245736,  0.17454351,  0.04028055, -0.13876861,\n",
              "          -0.07636368,  0.14176707, -0.2850192 , -0.11812342, -0.11666718,\n",
              "           0.00666731, -0.14478834,  0.17433836, -0.01239071,  0.23464477,\n",
              "           0.13238236, -0.187735  , -0.12876661, -0.12612621,  0.15258422,\n",
              "           0.04529803,  0.26637247,  0.25282001,  0.11490971, -0.16921504,\n",
              "           0.12282299,  0.27643606, -0.21014328, -0.11589022,  0.2434609 ,\n",
              "          -0.10156718,  0.02125996,  0.25155663,  0.24621262, -0.29223442,\n",
              "          -0.18193047,  0.01629906, -0.05599041, -0.14526576, -0.24603586,\n",
              "          -0.12535572, -0.21863972,  0.12987864, -0.15255779, -0.1724591 ,\n",
              "          -0.19387174,  0.15720437, -0.1752542 ,  0.22919372],\n",
              "         [ 0.2888705 , -0.130442  ,  0.22715025,  0.08444522, -0.00689607,\n",
              "           0.23524354,  0.08139338,  0.03235333, -0.12066555, -0.1691467 ,\n",
              "          -0.19600387, -0.1374856 ,  0.05003606, -0.15483129,  0.1423838 ,\n",
              "          -0.13941523,  0.04254366,  0.10111457,  0.01528201, -0.03542808,\n",
              "           0.17101082, -0.03463025, -0.14555281, -0.27067086,  0.05191803,\n",
              "          -0.27945054, -0.02418966, -0.2695961 , -0.24285004, -0.03179898,\n",
              "          -0.20483223, -0.08821655,  0.19538328, -0.05593618,  0.11371266,\n",
              "           0.13247666,  0.25571918,  0.07525872, -0.20995596,  0.06933345,\n",
              "           0.00531362, -0.08045153,  0.15022118,  0.0043055 , -0.06317292,\n",
              "           0.25553173, -0.18821898,  0.25887284,  0.01950318, -0.21684325,\n",
              "          -0.27393934, -0.19849335,  0.2050594 ,  0.01647816, -0.03901899,\n",
              "           0.16664924,  0.0365459 , -0.18298522, -0.29089472, -0.01878762,\n",
              "           0.00557942, -0.05768367, -0.11614717, -0.06020238],\n",
              "         [ 0.13599691,  0.25318012,  0.11856303,  0.14523906,  0.15699723,\n",
              "          -0.25340956,  0.13043834,  0.19995321,  0.17478064, -0.2555212 ,\n",
              "           0.12895028, -0.06176148, -0.09110448,  0.09444772,  0.27363184,\n",
              "          -0.20540564,  0.24327698,  0.25207385,  0.09735007, -0.15941232,\n",
              "          -0.04977601, -0.18982373, -0.13658412,  0.04362072,  0.22308695,\n",
              "           0.07183285,  0.14568809, -0.1173204 , -0.12160862, -0.00248463,\n",
              "          -0.09071423,  0.06862637,  0.1627237 ,  0.13682805, -0.14156608,\n",
              "           0.08362951, -0.10800286,  0.00612884, -0.10898647,  0.15825953,\n",
              "          -0.02968034,  0.22236218,  0.05700453, -0.24941744, -0.2519388 ,\n",
              "          -0.00253501,  0.10829592, -0.16113411,  0.07236438, -0.22440383,\n",
              "          -0.27076676, -0.16537528,  0.25128916, -0.16848902,  0.02971097,\n",
              "           0.11411064,  0.2184497 ,  0.2409353 , -0.3026934 ,  0.22564733,\n",
              "          -0.10941312,  0.2312488 ,  0.12258866, -0.04414375],\n",
              "         [ 0.13038193, -0.08194545,  0.10314566, -0.13200948,  0.28483608,\n",
              "          -0.20948787,  0.2644351 ,  0.00429185,  0.11676452,  0.29204094,\n",
              "          -0.06916595,  0.21496657,  0.13245346, -0.18272947, -0.02125752,\n",
              "          -0.29787493, -0.06925145, -0.08823527,  0.06723467, -0.29746163,\n",
              "           0.2145815 ,  0.09385515, -0.07810316, -0.03880874, -0.06304155,\n",
              "           0.26813346,  0.09634779,  0.17406525,  0.2175496 , -0.05447374,\n",
              "          -0.18424767,  0.29341492,  0.23493135,  0.06996272, -0.06138286,\n",
              "           0.25835067, -0.24265245, -0.29660836,  0.20612787, -0.00600484,\n",
              "           0.21998474,  0.09216951,  0.14703314, -0.05753163, -0.22100742,\n",
              "          -0.21206357, -0.30280122, -0.17114934,  0.03530148, -0.15538117,\n",
              "           0.27479145,  0.01404774,  0.08093008, -0.11213071, -0.17772889,\n",
              "          -0.2895109 , -0.07660836,  0.23312207, -0.13144578, -0.08649682,\n",
              "          -0.00536034,  0.07662224, -0.17656125,  0.10802187],\n",
              "         [ 0.2597959 ,  0.03440555,  0.11486645,  0.11443556, -0.15886088,\n",
              "           0.05616029, -0.28106612,  0.02232725,  0.27319604, -0.12052781,\n",
              "           0.257527  , -0.00884254, -0.25923762,  0.0965366 ,  0.21022163,\n",
              "          -0.03004154,  0.25559205, -0.02646633,  0.28640822,  0.21852979,\n",
              "          -0.08929863,  0.15623471, -0.07623686,  0.16729459,  0.13702483,\n",
              "          -0.2831629 ,  0.2538622 ,  0.06018452, -0.23362008,  0.06667591,\n",
              "          -0.15269329,  0.19727178, -0.12523954, -0.25447568, -0.18441674,\n",
              "           0.1609026 ,  0.13352783, -0.2451383 , -0.22523265, -0.02199629,\n",
              "          -0.20389868, -0.29778433,  0.2805316 , -0.03723789,  0.05415555,\n",
              "          -0.3042155 ,  0.1288785 ,  0.2500774 ,  0.09860164, -0.05455664,\n",
              "          -0.2166893 , -0.02940297, -0.08672435,  0.16194846,  0.19152449,\n",
              "          -0.04488805, -0.09829909, -0.09969689, -0.02152545,  0.19919495,\n",
              "           0.16819318,  0.00306643,  0.22963999,  0.09742737]],\n",
              "        dtype=float32),\n",
              "  array([ 0.01473979, -0.02259721,  0.00171324,  0.0259281 ,  0.01376557,\n",
              "          0.03136528,  0.01350579, -0.02179826,  0.02367342,  0.02217781,\n",
              "          0.03371715,  0.00754602,  0.00797436, -0.01012234, -0.01517941,\n",
              "          0.01976365,  0.0313411 ,  0.03230704, -0.00224275,  0.02766149,\n",
              "         -0.00920061, -0.02669757,  0.02554327,  0.02820393, -0.02477094,\n",
              "          0.00786655,  0.0124708 ,  0.00364129,  0.02018264, -0.01777655,\n",
              "          0.0256463 ,  0.01619711,  0.01535082,  0.00551873, -0.02786082,\n",
              "         -0.01144236,  0.00087508,  0.02962998,  0.01942776,  0.01880544,\n",
              "          0.01441021,  0.0158006 ,  0.0172106 , -0.00087102,  0.03147134,\n",
              "          0.03439945,  0.03107772,  0.03301038,  0.03178677,  0.01708386,\n",
              "          0.01994838,  0.02879509,  0.03160113,  0.02887032, -0.00673021,\n",
              "          0.00475706, -0.00949358,  0.01356489,  0.03207357, -0.01613606,\n",
              "          0.0316013 , -0.0092068 ,  0.02119635, -0.01191556], dtype=float32),\n",
              "  array([[-0.02901662,  0.0707932 ,  0.06577957, ..., -0.1646081 ,\n",
              "           0.09738912,  0.0958894 ],\n",
              "         [ 0.18495017, -0.10084525, -0.15179707, ...,  0.16807926,\n",
              "          -0.0576247 , -0.11392283],\n",
              "         [-0.14434993, -0.19370905, -0.07326509, ...,  0.04441944,\n",
              "           0.0666594 ,  0.00687911],\n",
              "         ...,\n",
              "         [-0.20187692, -0.01876226, -0.06044006, ...,  0.18070525,\n",
              "          -0.02124352, -0.0432003 ],\n",
              "         [ 0.01085986, -0.12648046, -0.19210275, ..., -0.02730688,\n",
              "           0.04517082, -0.1118002 ],\n",
              "         [-0.12190064,  0.08935463, -0.19387777, ...,  0.20888723,\n",
              "           0.11688825,  0.13191824]], dtype=float32),\n",
              "  array([ 0.01832454, -0.00643935, -0.01956243, -0.0046729 , -0.01800458,\n",
              "          0.03207697, -0.02644123,  0.02781145, -0.0059667 , -0.00111601,\n",
              "          0.03254569,  0.00994662, -0.02672318,  0.03066927, -0.01418448,\n",
              "         -0.02649141, -0.00271169,  0.0307485 ,  0.02119199, -0.00033099,\n",
              "          0.02241181, -0.00493813,  0.02848624,  0.01323748,  0.00785596,\n",
              "          0.03127929,  0.03204567, -0.00743541, -0.01620627,  0.0285751 ,\n",
              "          0.02815347,  0.01024112,  0.01356093,  0.02025623,  0.03168999,\n",
              "          0.01989776, -0.02492128,  0.03075633,  0.03017479, -0.02190182,\n",
              "          0.00614637,  0.03305949,  0.01095708,  0.03100191, -0.01272846,\n",
              "          0.03124775,  0.02047985, -0.01220276,  0.02822585,  0.03438501,\n",
              "         -0.01591003,  0.03351558,  0.02493754, -0.01069159, -0.01528117,\n",
              "         -0.01683628,  0.02828368,  0.03228487,  0.03163794,  0.03061578,\n",
              "          0.03111338, -0.02493839,  0.02557596,  0.02698184], dtype=float32),\n",
              "  array([[-0.16642183, -0.17763159,  0.15773645, ..., -0.17264806,\n",
              "           0.09496829,  0.15037172],\n",
              "         [ 0.16365612,  0.12493636,  0.14085191, ...,  0.1179117 ,\n",
              "          -0.17214368, -0.14415109],\n",
              "         [ 0.19165653,  0.09829784, -0.19467486, ..., -0.19703272,\n",
              "           0.04463812, -0.17361468],\n",
              "         ...,\n",
              "         [-0.14839517, -0.13031472, -0.02661787, ...,  0.07010944,\n",
              "          -0.1933433 , -0.04411964],\n",
              "         [-0.17561679,  0.02531006, -0.08932842, ...,  0.09205018,\n",
              "          -0.22289084,  0.09798905],\n",
              "         [ 0.09912715, -0.19837244, -0.00668953, ..., -0.09161787,\n",
              "           0.06054365,  0.07811578]], dtype=float32),\n",
              "  array([ 0.01073214, -0.0100533 ,  0.03240174,  0.01985378,  0.03146324,\n",
              "          0.03273104,  0.0317794 ,  0.03153096,  0.0324    , -0.02040018,\n",
              "          0.00142998,  0.03310036, -0.0212235 ,  0.03118891,  0.03127725,\n",
              "          0.03165994,  0.03164313,  0.00892127,  0.03167805, -0.02111434,\n",
              "         -0.02850291,  0.03177928, -0.01827379,  0.03101229,  0.03124346,\n",
              "          0.03163324,  0.03294779,  0.0331656 , -0.02547479, -0.00884268,\n",
              "         -0.01610627,  0.01411765,  0.03179202,  0.03177748,  0.0322894 ,\n",
              "         -0.02414624, -0.03195683, -0.02650626, -0.0250451 ,  0.0325216 ,\n",
              "         -0.00507114, -0.02521484, -0.01640166, -0.0116525 , -0.027793  ,\n",
              "          0.03133927,  0.03126071,  0.02441758, -0.03125131, -0.03026656,\n",
              "          0.03127654, -0.03078927, -0.02215324,  0.03176131,  0.03120415,\n",
              "          0.02998352,  0.03161698, -0.00967934, -0.0190231 , -0.0137482 ,\n",
              "         -0.01573965,  0.03169587, -0.01759302,  0.02807098], dtype=float32),\n",
              "  array([[ 0.2720782 ],\n",
              "         [-0.09501743],\n",
              "         [ 0.23795375],\n",
              "         [ 0.02245251],\n",
              "         [ 0.05962216],\n",
              "         [ 0.0021066 ],\n",
              "         [ 0.2136933 ],\n",
              "         [ 0.17660004],\n",
              "         [ 0.04952041],\n",
              "         [-0.05886797],\n",
              "         [-0.3128223 ],\n",
              "         [ 0.08023243],\n",
              "         [-0.1148052 ],\n",
              "         [ 0.29904544],\n",
              "         [ 0.18289922],\n",
              "         [ 0.11261754],\n",
              "         [ 0.06454295],\n",
              "         [ 0.03984599],\n",
              "         [ 0.12095514],\n",
              "         [-0.29541585],\n",
              "         [-0.14020002],\n",
              "         [ 0.26160714],\n",
              "         [-0.26564643],\n",
              "         [ 0.16519628],\n",
              "         [ 0.24774161],\n",
              "         [ 0.28963152],\n",
              "         [ 0.02429818],\n",
              "         [ 0.3051953 ],\n",
              "         [-0.17783587],\n",
              "         [-0.23798095],\n",
              "         [-0.21817878],\n",
              "         [ 0.09697614],\n",
              "         [ 0.05257543],\n",
              "         [ 0.14341658],\n",
              "         [ 0.03893274],\n",
              "         [-0.27243134],\n",
              "         [-0.101657  ],\n",
              "         [-0.13435082],\n",
              "         [-0.01630574],\n",
              "         [ 0.19246797],\n",
              "         [-0.0763801 ],\n",
              "         [-0.11295015],\n",
              "         [-0.22373177],\n",
              "         [-0.1648343 ],\n",
              "         [-0.10529611],\n",
              "         [ 0.28467432],\n",
              "         [ 0.16734011],\n",
              "         [ 0.3065612 ],\n",
              "         [-0.09773909],\n",
              "         [-0.23233972],\n",
              "         [ 0.19683911],\n",
              "         [-0.01834025],\n",
              "         [-0.236495  ],\n",
              "         [ 0.20838808],\n",
              "         [ 0.25613624],\n",
              "         [ 0.20278561],\n",
              "         [ 0.04700513],\n",
              "         [-0.12370467],\n",
              "         [-0.27961946],\n",
              "         [-0.21823926],\n",
              "         [-0.28455237],\n",
              "         [ 0.06721084],\n",
              "         [-0.19128948],\n",
              "         [ 0.13506763]], dtype=float32),\n",
              "  array([0.03115519], dtype=float32)],\n",
              " [array([[ 0.33413458, -0.09543898, -0.32270616,  0.24994084,  0.05775968,\n",
              "          -0.37474838, -0.11838768, -0.04377915,  0.46770322,  0.25805327,\n",
              "           0.21865925,  0.10384799, -0.14959967, -0.30015504,  0.30083457,\n",
              "           0.35144794, -0.2560579 ,  0.17524306, -0.01350286,  0.25822586,\n",
              "          -0.26931918, -0.13350873,  0.03804699,  0.23136245, -0.3572786 ,\n",
              "          -0.10145441,  0.3448924 ,  0.22138953,  0.22292839, -0.2991597 ,\n",
              "          -0.34329072,  0.089397  ],\n",
              "         [ 0.04532028,  0.17181088,  0.21850489, -0.17491908,  0.22890607,\n",
              "          -0.16970553, -0.26300946, -0.07254145, -0.21274358, -0.33537653,\n",
              "           0.02150091,  0.3104654 , -0.18057369,  0.411052  , -0.21635859,\n",
              "          -0.12661524,  0.33976606,  0.12758523, -0.12562965,  0.01045041,\n",
              "           0.1134028 ,  0.26733145, -0.32473344, -0.30907914,  0.371242  ,\n",
              "           0.00426826,  0.1515994 ,  0.28476155,  0.24880229,  0.0649449 ,\n",
              "           0.08378901, -0.36308494],\n",
              "         [ 0.01702989,  0.02654213,  0.38984755,  0.023445  , -0.3581315 ,\n",
              "          -0.2725627 , -0.35873586,  0.18388253, -0.13611972, -0.24180451,\n",
              "          -0.15757546,  0.25239152,  0.26693988,  0.06986353,  0.3223818 ,\n",
              "           0.21540128,  0.1608843 , -0.37601268,  0.2956989 ,  0.1517329 ,\n",
              "           0.36313105, -0.19013996,  0.3466843 , -0.26097223,  0.12064882,\n",
              "          -0.10244903,  0.292414  ,  0.00539703, -0.27757177, -0.25833896,\n",
              "           0.18879162,  0.04876187],\n",
              "         [ 0.00361761,  0.15640618, -0.34724435, -0.2491135 ,  0.04368994,\n",
              "          -0.07232042, -0.06019891,  0.22547333, -0.3359368 , -0.25398707,\n",
              "           0.01151725,  0.1801197 ,  0.06680311,  0.13634357, -0.01051758,\n",
              "          -0.2380389 , -0.26717702, -0.21136118, -0.26240474, -0.28625795,\n",
              "          -0.2123596 ,  0.05696806, -0.12916031,  0.35460648, -0.14564613,\n",
              "          -0.26008067, -0.23929176, -0.21511276,  0.26475805,  0.04573499,\n",
              "          -0.30647478, -0.15316191],\n",
              "         [ 0.21707542,  0.26304403,  0.33981544, -0.05786684, -0.03582771,\n",
              "          -0.35477394, -0.28439564, -0.20060262, -0.398661  , -0.01055561,\n",
              "           0.17293565, -0.34778443,  0.25542372, -0.07555241, -0.15491332,\n",
              "           0.23807192,  0.05727369,  0.30438137,  0.32376584,  0.2955382 ,\n",
              "           0.22083195, -0.32324633, -0.11685208, -0.20905168, -0.02278104,\n",
              "           0.11158954,  0.1123205 , -0.15510897,  0.13898635, -0.27928784,\n",
              "           0.2698238 , -0.4030143 ],\n",
              "         [-0.3243239 ,  0.37743118, -0.1438589 , -0.28576633, -0.0614536 ,\n",
              "          -0.32880473,  0.06149399, -0.14434892, -0.2617426 , -0.02941914,\n",
              "          -0.2331854 , -0.15727466,  0.13998254,  0.19287989,  0.36770868,\n",
              "           0.33561286, -0.26813015,  0.35009205,  0.20181689, -0.30048752,\n",
              "          -0.32436505,  0.34829074,  0.2108294 , -0.00306288, -0.30798596,\n",
              "          -0.2703673 , -0.30319405,  0.25670168, -0.11511912, -0.26221672,\n",
              "          -0.35734296,  0.2729304 ],\n",
              "         [ 0.2621762 , -0.15071562, -0.15882055,  0.2931471 , -0.17866021,\n",
              "          -0.02482705,  0.04967642, -0.04866814,  0.26156542,  0.36332   ,\n",
              "          -0.05500478,  0.00494931, -0.33854404,  0.06304435,  0.14220153,\n",
              "           0.11462159,  0.07398286,  0.30636305,  0.16574383, -0.33012712,\n",
              "           0.2953845 , -0.41232392, -0.12415836,  0.16428025, -0.26330367,\n",
              "          -0.06204696, -0.27864236,  0.19922404, -0.31937632, -0.37441382,\n",
              "          -0.2979248 , -0.22142613],\n",
              "         [ 0.15462081, -0.07618155, -0.35101026,  0.08358586,  0.09179628,\n",
              "          -0.11468473,  0.33399343, -0.09999302, -0.08575529, -0.29889596,\n",
              "           0.31077442, -0.37960297, -0.09482779, -0.2414965 , -0.07630375,\n",
              "          -0.39233118, -0.12166042, -0.19920819,  0.3364921 ,  0.35055578,\n",
              "           0.2883004 , -0.33610842,  0.16789207,  0.0702152 ,  0.36351734,\n",
              "          -0.11772849,  0.3116525 ,  0.2608026 , -0.28760284, -0.06976384,\n",
              "           0.08591673, -0.21995787]], dtype=float32),\n",
              "  array([ 0.04962444,  0.00552491,  0.05122646, -0.09276371, -0.03468741,\n",
              "         -0.04703208, -0.05659029, -0.02710639,  0.07101575,  0.03308345,\n",
              "         -0.01013682,  0.02741572, -0.03630494,  0.06727593, -0.04057712,\n",
              "          0.07147036,  0.01248819, -0.06401752,  0.01309012, -0.00576249,\n",
              "         -0.01360911,  0.01901082,  0.05529667, -0.06097737,  0.02074685,\n",
              "          0.01898417,  0.08391465, -0.04676742,  0.03517089,  0.08031809,\n",
              "         -0.00730991,  0.0613776 ], dtype=float32),\n",
              "  array([[-0.2688194 , -0.22003599, -0.08719306, ..., -0.03111256,\n",
              "           0.09861946,  0.20242693],\n",
              "         [-0.07348712,  0.22824243,  0.1874907 , ..., -0.06684597,\n",
              "          -0.17130888, -0.10299285],\n",
              "         [ 0.05968035, -0.10053083,  0.08053359, ..., -0.13600655,\n",
              "           0.12409101, -0.11498801],\n",
              "         ...,\n",
              "         [-0.03537607, -0.2054675 ,  0.0804691 , ..., -0.2717465 ,\n",
              "           0.22208129,  0.2207336 ],\n",
              "         [ 0.175501  , -0.21074142,  0.1736654 , ..., -0.23861152,\n",
              "          -0.0490549 , -0.2624729 ],\n",
              "         [-0.29680482, -0.10399219,  0.04502072, ..., -0.24107118,\n",
              "           0.28480634, -0.16348243]], dtype=float32),\n",
              "  array([-5.8437323e-05, -3.8241405e-02,  8.8676661e-03,  6.4246631e-03,\n",
              "          3.1700939e-02, -2.9833321e-02,  2.2396922e-02, -6.5533444e-02,\n",
              "         -9.1351077e-02, -6.3419230e-02, -8.0346726e-02,  3.1424978e-01,\n",
              "          1.3441945e-02, -9.1870213e-03,  8.7477900e-02,  1.2072059e-02,\n",
              "         -6.7431234e-02, -6.7345366e-02,  1.5903372e-01, -5.5059824e-02,\n",
              "         -4.4792370e-04,  4.7590386e-04, -1.1996493e-01,  1.0317305e-01,\n",
              "          1.0006997e-01, -3.6632758e-02,  4.5302617e-03, -2.2999281e-03,\n",
              "          7.1623046e-03, -7.8632772e-02, -8.7351277e-03, -3.6497518e-02],\n",
              "        dtype=float32),\n",
              "  array([[-0.02009046],\n",
              "         [-0.378518  ],\n",
              "         [ 0.06464123],\n",
              "         [ 0.08194945],\n",
              "         [ 0.12826629],\n",
              "         [-0.03787174],\n",
              "         [ 0.13437054],\n",
              "         [-0.30031088],\n",
              "         [-0.2744185 ],\n",
              "         [-0.28344658],\n",
              "         [-0.18037087],\n",
              "         [ 0.7896202 ],\n",
              "         [ 0.0920366 ],\n",
              "         [-0.1526455 ],\n",
              "         [ 0.28543296],\n",
              "         [ 0.38987124],\n",
              "         [-0.31065074],\n",
              "         [-0.2597298 ],\n",
              "         [ 0.4422477 ],\n",
              "         [-0.32851067],\n",
              "         [ 0.07468028],\n",
              "         [ 0.2368291 ],\n",
              "         [-0.22770672],\n",
              "         [ 0.38777527],\n",
              "         [ 0.34523553],\n",
              "         [-0.39060864],\n",
              "         [ 0.04700046],\n",
              "         [-0.2513488 ],\n",
              "         [ 0.04872532],\n",
              "         [-0.28636318],\n",
              "         [ 0.00541267],\n",
              "         [-0.21708073]], dtype=float32),\n",
              "  array([0.60328805], dtype=float32)],\n",
              " [array([[ 0.0084941 , -0.1250689 , -0.38456312,  0.12815179, -0.1391926 ,\n",
              "           0.30047077, -0.07002866, -0.05269454,  0.23386271,  0.08289269,\n",
              "           0.09525576,  0.02418982,  0.05811959, -0.0740139 , -0.2714169 ,\n",
              "           0.2278433 ,  0.05296069, -0.0279266 , -0.04594303, -0.18218938,\n",
              "          -0.32522175, -0.02500219,  0.05201217,  0.08078036,  0.23726764,\n",
              "           0.04814284,  0.2755781 ,  0.386495  , -0.0040072 , -0.21725275,\n",
              "          -0.19007476,  0.04546385],\n",
              "         [-0.04389489,  0.22192107, -0.1339041 , -0.10720171,  0.19371633,\n",
              "           0.24408084,  0.04974907,  0.24166372, -0.3089262 , -0.35271782,\n",
              "           0.1201012 ,  0.23253278, -0.29484218,  0.03720469,  0.12811048,\n",
              "           0.33663803,  0.08588132, -0.07221755, -0.06248819, -0.41402045,\n",
              "          -0.39849305,  0.14281788, -0.12497461,  0.01454584,  0.02564145,\n",
              "          -0.23859985, -0.19869368, -0.1947941 , -0.06588554,  0.15893328,\n",
              "          -0.28642985,  0.12587586],\n",
              "         [-0.23502555, -0.3717965 ,  0.08962785, -0.11320882, -0.1472987 ,\n",
              "          -0.01692767, -0.00950664,  0.31377462,  0.33349735,  0.03252733,\n",
              "           0.32306245, -0.16201909, -0.04051774,  0.25643268,  0.01715642,\n",
              "           0.00375691, -0.15660007, -0.26183754,  0.11757681, -0.34153453,\n",
              "           0.1231482 ,  0.1641738 ,  0.30543852, -0.11730046, -0.16974956,\n",
              "          -0.03528337,  0.00261185, -0.08878946, -0.13374002,  0.20966934,\n",
              "          -0.29437482, -0.24131888],\n",
              "         [-0.00898121,  0.3462269 ,  0.35172305, -0.01352937,  0.19006684,\n",
              "           0.16564932, -0.2901616 ,  0.08289246, -0.27881658,  0.20437539,\n",
              "          -0.3558144 , -0.37119412, -0.3516162 ,  0.32979432,  0.27860934,\n",
              "           0.10737278,  0.2304039 , -0.17455436, -0.22558807, -0.13213909,\n",
              "           0.14626473, -0.12724352, -0.24689864,  0.26956815, -0.09139217,\n",
              "          -0.00832166, -0.03400739, -0.38492456, -0.18442114,  0.18734002,\n",
              "          -0.05742246, -0.07579632],\n",
              "         [-0.0639208 ,  0.03088196,  0.06744217, -0.3876178 , -0.30703935,\n",
              "          -0.13714688, -0.26494527, -0.12080949,  0.13298817, -0.3888267 ,\n",
              "          -0.29666102, -0.06774639, -0.27393132, -0.25551394,  0.24524716,\n",
              "          -0.31914485,  0.18007147, -0.25457194, -0.2210696 , -0.10694485,\n",
              "          -0.08072831,  0.05585362,  0.09771504,  0.00134944,  0.02721458,\n",
              "           0.13248572, -0.08628547,  0.23216335,  0.13748346, -0.09016727,\n",
              "           0.02184712,  0.1054007 ],\n",
              "         [-0.17121677, -0.16817647, -0.2927405 ,  0.14806238,  0.02832888,\n",
              "          -0.26857358,  0.09980906,  0.17634574,  0.00835613, -0.32330835,\n",
              "           0.17035377, -0.3312757 , -0.26562032,  0.25860524, -0.29944643,\n",
              "          -0.10699052, -0.1264566 ,  0.01254793, -0.04098978, -0.17868133,\n",
              "           0.16387056,  0.12193381,  0.00953536,  0.015772  , -0.27610025,\n",
              "          -0.05753458, -0.10954023, -0.3144836 , -0.21136872, -0.19456671,\n",
              "           0.22185175,  0.17599264],\n",
              "         [-0.08248097, -0.10528914,  0.43999353,  0.10714576, -0.15774171,\n",
              "           0.43238008,  0.11025558,  0.27006236,  0.02070531,  0.0319367 ,\n",
              "           0.13882984,  0.22903082, -0.34681082,  0.05533427,  0.13883562,\n",
              "          -0.2242769 , -0.11333062, -0.1101472 , -0.04746056,  0.0675554 ,\n",
              "           0.06831653, -0.04048832,  0.02129329, -0.05550892, -0.06074153,\n",
              "           0.28267914, -0.25141865,  0.25255555, -0.25147355,  0.26671022,\n",
              "          -0.28898877, -0.37719458],\n",
              "         [-0.10027189,  0.10986669,  0.06957993, -0.1757023 , -0.2805142 ,\n",
              "           0.05010345,  0.10804779, -0.0802853 ,  0.00821322, -0.2449165 ,\n",
              "          -0.00122475, -0.09124476, -0.09078795, -0.11285522,  0.16332729,\n",
              "           0.0131017 ,  0.26207876, -0.2653681 , -0.2484266 , -0.33580697,\n",
              "          -0.30856434,  0.06580152, -0.07281485, -0.04811184, -0.16436411,\n",
              "           0.23944333,  0.2941478 ,  0.21847926, -0.29400817, -0.23684196,\n",
              "          -0.26949155, -0.04648133]], dtype=float32),\n",
              "  array([-0.07214566, -0.01571212,  0.07238552,  0.07535094, -0.06986495,\n",
              "          0.07487483, -0.07369421, -0.07286772, -0.07284186,  0.07598625,\n",
              "         -0.07294272,  0.07509007, -0.01846842, -0.07271306, -0.07319454,\n",
              "          0.07519402, -0.04118299, -0.07351013, -0.07288261,  0.07512209,\n",
              "          0.07471593, -0.07217399, -0.07272966, -0.06815229, -0.07260548,\n",
              "         -0.07167723, -0.07253547,  0.07489093, -0.07233916, -0.07304067,\n",
              "         -0.07055286,  0.07502578], dtype=float32),\n",
              "  array([[-0.16144209],\n",
              "         [ 0.02408797],\n",
              "         [ 0.06475797],\n",
              "         [ 0.22980197],\n",
              "         [-0.00806033],\n",
              "         [ 0.26215205],\n",
              "         [-0.2819365 ],\n",
              "         [-0.26449627],\n",
              "         [-0.23390755],\n",
              "         [ 0.12441268],\n",
              "         [-0.11629126],\n",
              "         [ 0.4114217 ],\n",
              "         [ 0.02269273],\n",
              "         [-0.3407911 ],\n",
              "         [-0.25834158],\n",
              "         [ 0.29879308],\n",
              "         [ 0.01205956],\n",
              "         [-0.34518424],\n",
              "         [-0.25417396],\n",
              "         [ 0.24964345],\n",
              "         [ 0.3136719 ],\n",
              "         [-0.09755085],\n",
              "         [-0.32618073],\n",
              "         [-0.19844845],\n",
              "         [-0.3201438 ],\n",
              "         [-0.04802183],\n",
              "         [-0.05416632],\n",
              "         [ 0.291562  ],\n",
              "         [-0.13125378],\n",
              "         [-0.28747788],\n",
              "         [-0.01510632],\n",
              "         [ 0.49981976]], dtype=float32),\n",
              "  array([0.07597502], dtype=float32)],\n",
              " [array([[-0.3828447 ,  0.2659129 ,  0.12218434,  0.18798964, -0.30510566,\n",
              "           0.04159992,  0.28411016, -0.2723308 , -0.1712206 , -0.21963401,\n",
              "           0.08696181, -0.05523548,  0.15349993, -0.06376853,  0.22667341,\n",
              "           0.34698606, -0.2870579 ,  0.17834595, -0.27989423,  0.260708  ,\n",
              "          -0.07100474, -0.05790436,  0.06127176,  0.1547216 , -0.30652034,\n",
              "          -0.13301365, -0.24242847, -0.01074086,  0.14388925,  0.06384833,\n",
              "          -0.04046605, -0.33722708],\n",
              "         [ 0.38990727, -0.0560601 ,  0.32975495,  0.3218962 ,  0.23752236,\n",
              "          -0.09094593, -0.15712969,  0.33658877, -0.15768903,  0.2342833 ,\n",
              "           0.12687293,  0.33269897, -0.07418136, -0.26642668,  0.12643258,\n",
              "          -0.3126337 , -0.19279811,  0.10951298,  0.2400007 ,  0.25233555,\n",
              "           0.03751095, -0.08593259, -0.3083548 , -0.12941153, -0.16730583,\n",
              "          -0.34000054, -0.29934138, -0.1351328 ,  0.17934841,  0.10270346,\n",
              "          -0.32554537, -0.10639786],\n",
              "         [-0.24957532, -0.03457493, -0.37387142, -0.2581998 , -0.09799708,\n",
              "          -0.14281172, -0.29464808,  0.3373391 ,  0.20127486, -0.19896063,\n",
              "          -0.20985423, -0.05738861, -0.33321488, -0.3669892 , -0.19221643,\n",
              "          -0.13726845, -0.12176111,  0.3409163 ,  0.00567527, -0.1300983 ,\n",
              "           0.09601216, -0.17476346, -0.30735818,  0.0187504 , -0.26061133,\n",
              "           0.1322991 , -0.20956777, -0.16860059, -0.19382836, -0.08293726,\n",
              "           0.18322185,  0.03405897],\n",
              "         [-0.29958922, -0.08272217,  0.09933586, -0.19537675, -0.35200694,\n",
              "          -0.04652484,  0.08576296, -0.34626558, -0.22286573,  0.05172155,\n",
              "           0.00465635,  0.12360634,  0.02024987, -0.37065658,  0.13937238,\n",
              "          -0.09733286, -0.39517576, -0.08414317, -0.39498502, -0.1794814 ,\n",
              "           0.18535796, -0.3044805 , -0.06915704,  0.11811934, -0.05697409,\n",
              "           0.19821122,  0.30041796,  0.08713371, -0.15537378, -0.18902425,\n",
              "          -0.36694506,  0.06701856],\n",
              "         [-0.3058878 ,  0.1120549 ,  0.3811351 ,  0.33818755, -0.34994856,\n",
              "           0.11089239, -0.20635422,  0.19832012, -0.09203727,  0.16974917,\n",
              "          -0.24237794,  0.17656866, -0.03580864,  0.01175656, -0.082095  ,\n",
              "          -0.32717735,  0.15371634,  0.06615134, -0.27029145, -0.41515818,\n",
              "          -0.01551482, -0.1496533 , -0.23822124,  0.16442421, -0.03660281,\n",
              "           0.21812558,  0.40305066,  0.25394145, -0.09305938, -0.01242885,\n",
              "          -0.23116153,  0.13025197],\n",
              "         [ 0.04028959,  0.00654001,  0.26450577,  0.03388887,  0.2730468 ,\n",
              "          -0.08998239,  0.15572304, -0.12079106, -0.20196524, -0.02101661,\n",
              "           0.03052848,  0.3126439 ,  0.12351014,  0.33866978, -0.02152478,\n",
              "          -0.12276408, -0.21210685,  0.2846884 ,  0.11137708, -0.0420724 ,\n",
              "           0.32703146,  0.21303892, -0.15177104,  0.08809021, -0.11537144,\n",
              "          -0.22601895,  0.22738098,  0.25815293,  0.28578922,  0.29904446,\n",
              "           0.2798672 ,  0.15513   ],\n",
              "         [ 0.24234407,  0.0978801 , -0.18473764,  0.11651953,  0.15318018,\n",
              "           0.07291389, -0.15873192,  0.19937402,  0.00632692,  0.11116444,\n",
              "          -0.16732426,  0.07320129,  0.31985497, -0.03986092, -0.2723682 ,\n",
              "          -0.00171928, -0.10286943, -0.41544527,  0.30607542,  0.2718131 ,\n",
              "           0.01341705,  0.2051563 , -0.31990963,  0.14985204, -0.16405825,\n",
              "           0.04564951, -0.37944394, -0.09358495,  0.26587975,  0.25858027,\n",
              "          -0.37324065, -0.23721461],\n",
              "         [-0.16466084,  0.31334695,  0.30083922,  0.07251761, -0.38428637,\n",
              "           0.28386858, -0.05647245, -0.11609776,  0.28341103, -0.08440272,\n",
              "          -0.09447382, -0.0486121 ,  0.05575837, -0.13341981,  0.31033447,\n",
              "           0.17195015, -0.2747884 ,  0.07391185,  0.29047197,  0.11880421,\n",
              "           0.19258592,  0.29268172, -0.27345318, -0.37570605,  0.33033097,\n",
              "           0.07717142,  0.15260746, -0.2109038 ,  0.39924812,  0.17606454,\n",
              "           0.31365514,  0.06637101]], dtype=float32),\n",
              "  array([ 0.0351824 , -0.03561573, -0.00307374,  0.03610792,  0.03502338,\n",
              "         -0.0356262 ,  0.03669735,  0.0352053 , -0.0356737 , -0.03452765,\n",
              "         -0.03560449, -0.03440968, -0.03458931,  0.03594968,  0.03610007,\n",
              "          0.03675991,  0.03631172,  0.0360419 ,  0.03520249,  0.03600055,\n",
              "          0.03588183, -0.03501417, -0.03558218,  0.03546562, -0.03547008,\n",
              "         -0.03531042,  0.03573813, -0.03487971,  0.03567119, -0.0346261 ,\n",
              "          0.03608031,  0.03557508], dtype=float32),\n",
              "  array([[ 0.18591915],\n",
              "         [-0.13847704],\n",
              "         [ 0.00595515],\n",
              "         [ 0.11715553],\n",
              "         [ 0.3777234 ],\n",
              "         [-0.15932696],\n",
              "         [ 0.06507765],\n",
              "         [ 0.2109836 ],\n",
              "         [-0.36083278],\n",
              "         [-0.08346935],\n",
              "         [-0.38558304],\n",
              "         [-0.07801334],\n",
              "         [-0.20370771],\n",
              "         [ 0.29566702],\n",
              "         [ 0.40027678],\n",
              "         [ 0.07261326],\n",
              "         [ 0.08498768],\n",
              "         [ 0.26485777],\n",
              "         [ 0.39617282],\n",
              "         [ 0.15550599],\n",
              "         [ 0.2513938 ],\n",
              "         [-0.08895531],\n",
              "         [-0.08834679],\n",
              "         [ 0.13979048],\n",
              "         [-0.19389568],\n",
              "         [-0.13017076],\n",
              "         [ 0.18070173],\n",
              "         [-0.02933423],\n",
              "         [ 0.30429822],\n",
              "         [-0.08804153],\n",
              "         [ 0.4300949 ],\n",
              "         [ 0.32623845]], dtype=float32),\n",
              "  array([0.03698122], dtype=float32)],\n",
              " [array([[ 1.09263130e-01,  3.93608272e-01,  1.95519507e-01,\n",
              "           1.67848319e-01,  3.98116887e-01, -2.37003177e-01,\n",
              "           3.23912129e-02,  1.77493870e-01, -3.43384862e-01,\n",
              "           2.49715410e-02,  1.42776549e-01, -2.44834945e-01,\n",
              "          -2.28867635e-01,  2.42302299e-01, -1.42380133e-01,\n",
              "          -3.30890641e-02, -3.22543919e-01, -1.10453710e-01,\n",
              "          -1.66752879e-02,  1.36153772e-01, -3.09726596e-01,\n",
              "          -1.59675568e-01,  3.49771559e-01,  2.53447682e-01,\n",
              "           5.15526719e-02, -2.76722103e-01, -1.60214424e-01,\n",
              "          -1.61420181e-01,  2.91237891e-01, -1.00111693e-01,\n",
              "           3.27011049e-01,  3.31136584e-01],\n",
              "         [-2.40333512e-01, -2.46381670e-01, -1.55342951e-01,\n",
              "          -4.00668502e-01,  3.53531390e-01,  9.92706344e-02,\n",
              "          -9.04752016e-02, -3.23114008e-01,  1.22826302e-03,\n",
              "           3.19212258e-01, -8.09080973e-02, -1.87769294e-01,\n",
              "           1.72401935e-01,  3.77759308e-01, -6.10943325e-02,\n",
              "          -6.61638603e-02,  2.30201215e-01, -3.17728132e-01,\n",
              "          -2.57893000e-02, -1.41656265e-01, -5.81847653e-02,\n",
              "           2.32460067e-01,  3.76201302e-01, -1.10018402e-01,\n",
              "           3.74172896e-01,  2.43708189e-03,  1.78655624e-01,\n",
              "          -1.61537217e-04,  1.13156147e-01, -1.37794524e-01,\n",
              "          -1.01357035e-01,  1.46091148e-01],\n",
              "         [ 2.11164817e-01,  3.89479369e-01, -2.69997418e-01,\n",
              "           9.76626799e-02,  2.05855127e-02, -5.94695285e-02,\n",
              "          -2.32712850e-01,  2.24483728e-01,  9.83058289e-02,\n",
              "           1.53173536e-01,  3.21938962e-01,  1.66757420e-01,\n",
              "           9.46302786e-02, -1.89524412e-01, -1.21272355e-01,\n",
              "          -2.70154715e-01,  1.89439267e-01, -2.43743986e-01,\n",
              "           1.89661488e-01,  1.28266871e-01,  2.95903414e-01,\n",
              "           5.75785637e-02,  1.87830970e-01,  2.76743084e-01,\n",
              "           7.19218887e-03, -1.49734199e-01, -2.09805205e-01,\n",
              "          -1.62740499e-02, -8.56198892e-02,  6.63708150e-02,\n",
              "          -2.97433317e-01, -7.81772360e-02],\n",
              "         [-3.29620808e-01, -7.66519755e-02, -1.13958627e-01,\n",
              "          -1.66178882e-01,  1.84061918e-02,  5.14264666e-02,\n",
              "          -3.31339002e-01,  3.66872638e-01, -3.56429398e-01,\n",
              "          -5.58400825e-02, -3.79412144e-01, -1.05578497e-01,\n",
              "          -1.99357986e-01, -1.20588459e-01, -5.94069846e-02,\n",
              "           3.32347974e-02, -3.47076416e-01,  2.35181034e-01,\n",
              "           3.00048321e-01,  5.36205620e-02,  2.81960011e-01,\n",
              "          -3.13025534e-01, -2.94300973e-01,  3.60711426e-01,\n",
              "          -9.49033648e-02, -3.03672612e-01, -2.72934288e-01,\n",
              "          -2.57063508e-01,  4.53805886e-02,  1.24679849e-01,\n",
              "           2.92400956e-01, -3.15815091e-01],\n",
              "         [ 3.75732899e-01,  3.05444628e-01, -6.86428025e-02,\n",
              "          -6.89704940e-02,  1.15602566e-02, -4.00113463e-01,\n",
              "          -2.29002699e-01,  2.37075284e-01,  1.04328483e-01,\n",
              "          -2.08659157e-01, -9.49137658e-02, -9.01506376e-03,\n",
              "          -2.60211766e-01,  3.31560910e-01,  1.59640640e-01,\n",
              "           1.27629012e-01, -3.12233660e-02,  2.02148661e-01,\n",
              "           1.82751462e-01,  3.37355435e-01,  3.92497003e-01,\n",
              "          -2.70944327e-01,  1.07609913e-01, -1.63107485e-01,\n",
              "           2.93304056e-01, -2.18653262e-01,  2.05356017e-01,\n",
              "          -2.58916825e-01, -1.88528046e-01,  1.73978582e-01,\n",
              "          -2.46267691e-01, -6.95687719e-03],\n",
              "         [-1.62536025e-01,  6.05787747e-02, -1.71525106e-01,\n",
              "          -2.17601150e-01,  3.97240557e-02,  8.09407532e-02,\n",
              "          -1.64042503e-01,  2.34716147e-01, -2.51679361e-01,\n",
              "           2.74965137e-01, -9.68154743e-02, -1.51956053e-02,\n",
              "          -5.16697159e-03,  1.42145708e-01,  3.14436823e-01,\n",
              "          -5.35693131e-02, -2.55303174e-01,  1.83043838e-01,\n",
              "           2.81943619e-01,  2.74887443e-01,  9.26178098e-02,\n",
              "           3.69364083e-01, -1.22536212e-01, -8.79207850e-02,\n",
              "          -2.93525964e-01,  1.14157908e-01, -3.02239895e-01,\n",
              "          -1.08973674e-01,  1.50303647e-01,  3.45893688e-02,\n",
              "          -3.46891135e-01,  2.65284181e-01],\n",
              "         [ 9.75315347e-02,  2.65305117e-02,  5.68678863e-02,\n",
              "          -2.29424551e-01,  1.81780741e-01,  1.97038054e-01,\n",
              "           6.81719333e-02,  1.45434737e-01,  2.13502899e-01,\n",
              "          -2.94743717e-01, -1.41596243e-01, -3.83571625e-01,\n",
              "           1.21457212e-01, -9.99776646e-02, -3.41829032e-01,\n",
              "           3.34010571e-02,  2.11810499e-01, -3.76291096e-01,\n",
              "          -3.38441044e-01, -1.77067593e-01,  6.93399757e-02,\n",
              "          -2.09337537e-04,  6.24026312e-03, -1.62103742e-01,\n",
              "          -2.75307775e-01,  3.38654459e-01, -2.87116200e-01,\n",
              "           8.51951614e-02, -1.66932300e-01,  4.10208523e-01,\n",
              "           3.53644133e-01,  1.28030315e-01],\n",
              "         [ 1.98982075e-01, -5.28201833e-03,  1.23769484e-01,\n",
              "           5.25861457e-02,  1.09991699e-01, -2.38886014e-01,\n",
              "          -7.32417731e-03,  4.75611575e-02, -8.64569172e-02,\n",
              "          -2.45455667e-01, -1.17881276e-01, -4.14005853e-02,\n",
              "          -2.72138298e-01, -3.82269531e-01,  5.72382845e-02,\n",
              "           2.88186818e-01, -5.31847700e-02, -3.20349306e-01,\n",
              "           2.85329938e-01, -2.69969910e-01,  1.52912259e-01,\n",
              "           2.14779284e-04, -3.46629947e-01,  9.08583105e-02,\n",
              "           5.57982223e-03,  9.65285301e-02, -1.89237267e-01,\n",
              "          -4.14889038e-01, -3.83882046e-01, -8.38317126e-02,\n",
              "           3.06758702e-01, -1.40796676e-01]], dtype=float32),\n",
              "  array([ 0.01958476,  0.03466763,  0.03792164, -0.00646459, -0.03151251,\n",
              "          0.0367606 ,  0.03862821, -0.03848325, -0.03771928, -0.03692993,\n",
              "          0.03818753,  0.03771297, -0.0370148 ,  0.0258763 ,  0.03573781,\n",
              "         -0.03677886, -0.03743222, -0.03646683,  0.03317616, -0.03646188,\n",
              "         -0.00971175, -0.03762212,  0.03940089,  0.03826899,  0.03126066,\n",
              "         -0.03243272,  0.03774668,  0.03757403, -0.03302139, -0.02891619,\n",
              "         -0.0379465 ,  0.03826474], dtype=float32),\n",
              "  array([[ 0.01864091,  0.2671355 ,  0.08728978, ...,  0.0228576 ,\n",
              "           0.03163736, -0.22939579],\n",
              "         [ 0.02749051,  0.16506481, -0.27849808, ..., -0.22130659,\n",
              "           0.16993901, -0.05200107],\n",
              "         [ 0.16461796,  0.21003298,  0.07876903, ..., -0.25826976,\n",
              "          -0.28997648,  0.22516127],\n",
              "         ...,\n",
              "         [-0.04267186, -0.08484735, -0.05120112, ..., -0.09215045,\n",
              "           0.24019611, -0.22219436],\n",
              "         [-0.0953778 , -0.24756095, -0.26192886, ..., -0.26312378,\n",
              "           0.19079679, -0.17479214],\n",
              "         [ 0.08776913, -0.03726176,  0.22732876, ..., -0.15086876,\n",
              "          -0.06183207,  0.20255122]], dtype=float32),\n",
              "  array([-0.03813215, -0.03717976,  0.0378789 ,  0.03714899,  0.01518894,\n",
              "         -0.0014224 , -0.03760966, -0.03738747, -0.03657244, -0.03789449,\n",
              "          0.03815667,  0.03153967, -0.0370358 ,  0.0367186 ,  0.03678017,\n",
              "          0.03747198, -0.03748126,  0.0384712 , -0.0290822 , -0.03669544,\n",
              "         -0.03736944, -0.03709849,  0.03777466, -0.03742484, -0.03722116,\n",
              "         -0.03105787, -0.00907252,  0.03774966,  0.03761356,  0.03713385,\n",
              "         -0.03365431,  0.03615826], dtype=float32),\n",
              "  array([[-0.21164824, -0.12473655, -0.23497592, ..., -0.30761012,\n",
              "           0.15007414,  0.1855663 ],\n",
              "         [-0.11981183,  0.23986904, -0.18335453, ...,  0.17249316,\n",
              "          -0.23021929, -0.16630623],\n",
              "         [ 0.09521481, -0.23615268, -0.07672221, ...,  0.27930453,\n",
              "           0.01849161,  0.18319778],\n",
              "         ...,\n",
              "         [ 0.2803003 , -0.23836507,  0.3099449 , ..., -0.2581817 ,\n",
              "          -0.12265336, -0.0108005 ],\n",
              "         [ 0.13474913, -0.00644968,  0.03023572, ..., -0.11965378,\n",
              "           0.23688532,  0.2008564 ],\n",
              "         [-0.32146686, -0.11764844,  0.11814637, ..., -0.20361713,\n",
              "          -0.24181269, -0.03929474]], dtype=float32),\n",
              "  array([-0.03671444, -0.03646043,  0.03717352, -0.03693323,  0.03679128,\n",
              "          0.03693665,  0.03696063, -0.03721068, -0.03684253,  0.03699321,\n",
              "         -0.03716539,  0.03710074,  0.03712619, -0.0372573 , -0.03689403,\n",
              "         -0.0370139 ,  0.037097  , -0.03688895, -0.03674857,  0.03743695,\n",
              "         -0.03682451,  0.03726724,  0.03702347, -0.03696659,  0.03707173,\n",
              "         -0.03692141, -0.03694016,  0.03698759, -0.03721021,  0.03685429,\n",
              "         -0.03697965,  0.03704707], dtype=float32),\n",
              "  array([[-0.09818248],\n",
              "         [-0.02080749],\n",
              "         [ 0.16044559],\n",
              "         [-0.3425129 ],\n",
              "         [ 0.3406881 ],\n",
              "         [ 0.04376687],\n",
              "         [ 0.39563054],\n",
              "         [-0.12914374],\n",
              "         [-0.0448402 ],\n",
              "         [ 0.22770657],\n",
              "         [-0.05169296],\n",
              "         [ 0.14280565],\n",
              "         [ 0.0696694 ],\n",
              "         [-0.1384195 ],\n",
              "         [-0.15441912],\n",
              "         [-0.14348565],\n",
              "         [ 0.25330603],\n",
              "         [-0.39640868],\n",
              "         [-0.31624305],\n",
              "         [ 0.0898018 ],\n",
              "         [-0.21488634],\n",
              "         [ 0.13489828],\n",
              "         [ 0.30488932],\n",
              "         [-0.38416842],\n",
              "         [ 0.25361168],\n",
              "         [-0.16623475],\n",
              "         [-0.18704203],\n",
              "         [ 0.21093677],\n",
              "         [-0.10218164],\n",
              "         [ 0.1661652 ],\n",
              "         [-0.43542722],\n",
              "         [ 0.4272882 ]], dtype=float32),\n",
              "  array([0.03701882], dtype=float32)],\n",
              " [array([[-2.00427487e-01, -2.89145023e-01,  3.09744596e-01,\n",
              "          -2.09126964e-01, -1.59197316e-01,  3.70809108e-01,\n",
              "           2.71057546e-01, -8.89456794e-02,  1.74366891e-01,\n",
              "           5.55284843e-02,  2.39783153e-01,  2.95177013e-01,\n",
              "           3.56673330e-01,  9.13349679e-04,  2.55194157e-01,\n",
              "          -2.18106553e-01,  2.31665261e-02, -6.21843189e-02,\n",
              "           5.88466898e-02, -2.52951682e-01, -2.19130181e-02,\n",
              "          -5.51122986e-02, -2.98472971e-01,  1.04773991e-01,\n",
              "           2.61735082e-01,  2.17088368e-02,  2.00165287e-02,\n",
              "           3.68890762e-01,  3.70383203e-01, -2.15207815e-01,\n",
              "          -2.51230985e-01, -1.06662111e-02],\n",
              "         [-2.32487470e-01,  7.24736080e-02, -4.85574864e-02,\n",
              "           2.24822536e-01, -3.48576099e-01, -5.14203981e-02,\n",
              "          -2.84870535e-01,  1.20075293e-01, -1.53930530e-01,\n",
              "          -1.50966272e-01, -2.51275629e-01, -5.47198728e-02,\n",
              "           5.25401579e-03,  2.05618903e-01,  4.06738281e-01,\n",
              "          -2.28852220e-02, -1.63710136e-02, -2.31811911e-01,\n",
              "           7.02230483e-02,  1.49761721e-01,  9.82745215e-02,\n",
              "          -1.70215711e-01, -2.65785575e-01,  3.27644914e-01,\n",
              "           1.07964978e-01, -3.66686136e-01,  1.71128854e-01,\n",
              "          -1.57276705e-01,  5.89558929e-02,  2.77707517e-01,\n",
              "          -1.73902139e-01, -2.29141489e-01],\n",
              "         [ 2.54584461e-01, -2.36018151e-01,  3.34242910e-01,\n",
              "           2.30559513e-01,  3.45374733e-01, -2.72976428e-01,\n",
              "          -3.83386612e-01, -9.45982039e-02, -8.64896998e-02,\n",
              "           1.36701271e-01, -3.88651103e-01, -3.91469926e-01,\n",
              "           1.33214816e-01, -3.76693428e-01, -1.65539589e-02,\n",
              "          -2.35529095e-01,  1.75751641e-01,  1.60435271e-02,\n",
              "          -3.63513410e-01,  3.41185391e-01,  2.35227495e-01,\n",
              "          -2.03761205e-01, -3.54713291e-01,  2.28603967e-02,\n",
              "           1.10936403e-01, -1.41752467e-01, -3.04865420e-01,\n",
              "          -2.68583417e-01, -2.85314143e-01,  1.55430734e-01,\n",
              "          -1.59381613e-01, -4.69250791e-02],\n",
              "         [ 2.63906247e-03, -1.03852719e-01,  1.23234287e-01,\n",
              "           1.22822143e-01,  1.85237855e-01, -1.48715630e-01,\n",
              "          -2.86428720e-01,  2.79668681e-02,  3.66037756e-01,\n",
              "           1.40250087e-01,  2.16603484e-02,  1.30685225e-01,\n",
              "          -3.21727455e-01,  3.77839953e-01, -7.25898668e-02,\n",
              "          -1.91475511e-01, -2.21144021e-01, -6.20545782e-02,\n",
              "           3.34420860e-01,  1.72618285e-01,  1.07332543e-01,\n",
              "          -3.79521430e-01,  1.18128657e-01, -9.40747093e-03,\n",
              "          -7.83603489e-02,  2.97430038e-01,  6.44975379e-02,\n",
              "          -9.73550975e-02,  7.30172098e-02, -2.33992025e-01,\n",
              "          -2.94482380e-01, -3.27182770e-01],\n",
              "         [ 3.88343609e-03,  3.79289001e-01, -4.76030409e-02,\n",
              "          -3.26103956e-01, -3.24372351e-01,  1.36229515e-01,\n",
              "           6.96295351e-02, -3.43523979e-01, -1.18371993e-01,\n",
              "          -4.72826371e-03,  1.56575650e-01, -2.83838034e-01,\n",
              "           5.89294322e-02, -1.87088605e-02,  7.32469410e-02,\n",
              "          -8.60783458e-02, -2.22548023e-01,  8.11769739e-02,\n",
              "          -3.39754492e-01, -1.17643468e-01,  1.47798523e-01,\n",
              "          -3.77498001e-01, -1.03228845e-01, -1.56317651e-01,\n",
              "          -4.14528102e-02,  1.38403394e-03,  4.14345898e-02,\n",
              "          -1.10082179e-01,  3.44548225e-01, -3.78231376e-01,\n",
              "          -1.05890855e-01,  3.29906285e-01],\n",
              "         [-9.16682482e-02,  2.15281963e-01,  3.31909508e-01,\n",
              "           1.55801609e-01, -3.53266150e-01, -1.28900096e-01,\n",
              "          -1.63883194e-01, -2.23628521e-01,  3.70460659e-01,\n",
              "           2.73817256e-02, -2.59832084e-01,  3.60572059e-03,\n",
              "           1.15179449e-01,  3.12363505e-01,  1.34717867e-01,\n",
              "           5.12139313e-02,  2.84889061e-02,  1.34763597e-02,\n",
              "           1.90608099e-01, -2.02887893e-01, -1.50876328e-01,\n",
              "           1.44633623e-02, -3.27751040e-01,  9.52358320e-02,\n",
              "          -1.89351626e-02,  1.35306552e-01,  3.53475630e-01,\n",
              "           3.63687724e-01, -2.09607750e-01,  4.85015772e-02,\n",
              "           3.68128955e-01,  3.52006704e-01],\n",
              "         [-9.07415822e-02, -9.00595263e-02, -3.38333175e-02,\n",
              "          -3.68756384e-01, -2.15436682e-01, -3.14930826e-01,\n",
              "           2.94613857e-02,  1.09216578e-01, -1.54961810e-01,\n",
              "          -3.24601471e-01,  2.68333465e-01, -1.06910028e-01,\n",
              "          -2.57098585e-01, -3.77083004e-01,  7.69335479e-02,\n",
              "           1.50638416e-01,  7.13187233e-02,  1.46227390e-01,\n",
              "           6.94097811e-03, -2.16928020e-01,  9.34624895e-02,\n",
              "          -4.01430368e-01,  1.24104679e-01, -8.40343311e-02,\n",
              "           3.44740033e-01,  1.69945642e-01, -7.29824975e-02,\n",
              "          -1.27929568e-01, -3.37875128e-01, -1.50262594e-01,\n",
              "          -3.60563308e-01, -1.30732149e-01],\n",
              "         [ 3.22503060e-01,  1.63614720e-01, -1.42472818e-01,\n",
              "          -7.89632276e-03,  2.02126056e-01, -1.57153420e-02,\n",
              "           3.24163139e-01,  4.39299420e-02, -3.95162463e-01,\n",
              "          -1.58555180e-01, -1.26079142e-01,  2.62885451e-01,\n",
              "           2.00798362e-02,  1.96688399e-01,  3.83097142e-01,\n",
              "          -3.95324647e-01,  6.13778643e-02,  1.06424406e-01,\n",
              "           4.86534908e-02,  2.64039427e-01,  8.62543806e-02,\n",
              "           1.14254937e-01, -9.31748226e-02,  3.44970644e-01,\n",
              "          -3.34958851e-01, -3.49270970e-01, -1.87166825e-01,\n",
              "          -1.84558127e-02,  3.61309201e-02, -4.08546954e-01,\n",
              "           1.29823759e-01, -1.27675943e-04]], dtype=float32),\n",
              "  array([-0.09008007, -0.02944671,  0.09009197, -0.07054292, -0.00787343,\n",
              "          0.00807185, -0.03871791, -0.01753492,  0.01628195,  0.05086666,\n",
              "          0.05033281, -0.09379821,  0.20680903,  0.03530979,  0.20682752,\n",
              "          0.06025986,  0.02694487,  0.02819233, -0.07098615, -0.08207637,\n",
              "         -0.09420902,  0.11579984,  0.08111946,  0.00971255, -0.07160682,\n",
              "         -0.05153321, -0.08409902, -0.02345436, -0.0096188 ,  0.0948538 ,\n",
              "         -0.03555834, -0.06129041], dtype=float32),\n",
              "  array([[-0.30229968, -0.19854632,  0.22788613, ..., -0.17750221,\n",
              "          -0.12502828, -0.1245306 ],\n",
              "         [ 0.04173176, -0.18555547,  0.22027141, ..., -0.1720918 ,\n",
              "          -0.10810019, -0.285756  ],\n",
              "         [-0.02663319, -0.07629334,  0.300214  , ...,  0.20546316,\n",
              "          -0.22928715,  0.01601495],\n",
              "         ...,\n",
              "         [ 0.13370135, -0.21109173, -0.09404193, ...,  0.03361483,\n",
              "          -0.19616146,  0.27402896],\n",
              "         [-0.17018694, -0.12953392, -0.25459406, ...,  0.15821806,\n",
              "          -0.14084709, -0.23897707],\n",
              "         [-0.06586322,  0.05486022,  0.26531145, ...,  0.18912885,\n",
              "           0.02415036,  0.25433308]], dtype=float32),\n",
              "  array([ 0.10542344, -0.00438047,  0.11985807, -0.09228483, -0.13694678,\n",
              "         -0.10115328, -0.05852235,  0.05585133,  0.12982486,  0.16265434,\n",
              "         -0.08603913,  0.07264158, -0.04900056, -0.00383089,  0.0062832 ,\n",
              "          0.05232038, -0.09857107, -0.05545732,  0.04887808, -0.09215598,\n",
              "          0.13495758,  0.0354194 , -0.07541732,  0.06916136, -0.10371783,\n",
              "          0.10534336,  0.03897156,  0.1482654 ,  0.09895568,  0.15315512,\n",
              "         -0.11070354,  0.17196782], dtype=float32),\n",
              "  array([[ 0.33599833],\n",
              "         [-0.02569225],\n",
              "         [ 0.37803945],\n",
              "         [-0.2325403 ],\n",
              "         [-0.2769573 ],\n",
              "         [-0.4061705 ],\n",
              "         [-0.24670188],\n",
              "         [ 0.14733139],\n",
              "         [ 0.38075614],\n",
              "         [ 0.48636976],\n",
              "         [-0.30093747],\n",
              "         [ 0.23696658],\n",
              "         [-0.1807864 ],\n",
              "         [-0.04875574],\n",
              "         [ 0.011658  ],\n",
              "         [ 0.17213775],\n",
              "         [-0.3162715 ],\n",
              "         [-0.17373344],\n",
              "         [ 0.16206688],\n",
              "         [-0.28785717],\n",
              "         [ 0.39544415],\n",
              "         [ 0.11278904],\n",
              "         [-0.27464986],\n",
              "         [ 0.19193068],\n",
              "         [-0.31339374],\n",
              "         [ 0.3126984 ],\n",
              "         [ 0.10478941],\n",
              "         [ 0.4482903 ],\n",
              "         [ 0.29348102],\n",
              "         [ 0.45021963],\n",
              "         [-0.34474114],\n",
              "         [ 0.5362469 ]], dtype=float32),\n",
              "  array([0.4073912], dtype=float32)],\n",
              " [array([[ 1.98631763e-01, -2.41659939e-01,  1.22528695e-01,\n",
              "           1.85212642e-01, -7.40200654e-02, -4.81524551e-03,\n",
              "           2.83748269e-01,  1.35744169e-01,  1.78336322e-01,\n",
              "          -9.37580988e-02,  6.21831324e-03, -1.20940395e-02,\n",
              "           2.79550791e-01, -7.31533617e-02, -3.24868038e-02,\n",
              "          -1.94411963e-01,  1.32663861e-01, -2.35281155e-01,\n",
              "          -2.52352089e-01,  2.67877728e-01,  2.55221307e-01,\n",
              "          -3.04452837e-01,  1.13890387e-01,  4.64075021e-02,\n",
              "           1.32104397e-01,  2.70975262e-01, -5.67893349e-02,\n",
              "           2.00479001e-01,  1.01332944e-02,  3.32233310e-01,\n",
              "           4.66060489e-02, -5.62539324e-02, -3.80648673e-02,\n",
              "          -2.67155409e-01,  2.66836613e-01,  2.35471874e-01,\n",
              "          -2.04514623e-01, -2.66675174e-01,  1.99530467e-01,\n",
              "          -1.94971543e-02,  1.62525415e-01,  1.83773488e-01,\n",
              "          -4.83841561e-02, -5.68343289e-02, -2.28664398e-01,\n",
              "          -1.07523903e-01,  9.21714604e-02, -1.94706067e-01,\n",
              "           1.16532475e-01,  6.38832748e-02,  7.96182677e-02,\n",
              "          -7.42345769e-03,  1.15955554e-01,  8.84612575e-02,\n",
              "          -2.72131324e-01,  9.96938273e-02,  2.29834754e-04,\n",
              "          -4.40897234e-03,  2.97818512e-01, -1.24282904e-01,\n",
              "           9.52520594e-02, -1.81669533e-01, -2.22629443e-01,\n",
              "           1.28726482e-01],\n",
              "         [ 1.59043893e-01,  6.86892048e-02, -2.26060584e-01,\n",
              "          -1.29076868e-01, -2.94793514e-03,  3.03184003e-01,\n",
              "           1.72432631e-01, -2.68949330e-01,  1.88693315e-01,\n",
              "          -8.56870413e-03, -9.76288226e-03,  2.30688393e-01,\n",
              "           2.44392484e-01,  1.07072145e-01,  3.00513953e-01,\n",
              "          -2.47502640e-01, -9.37801152e-02,  2.61133492e-01,\n",
              "           1.92593515e-01,  1.30416021e-01,  1.48596197e-01,\n",
              "           1.43209130e-01,  8.26104656e-02,  6.89546987e-02,\n",
              "           1.05307251e-01, -2.24452421e-01, -2.44456485e-01,\n",
              "           1.03971273e-01, -1.25798687e-01, -1.02483027e-01,\n",
              "           1.33583948e-01, -2.19209328e-01,  2.95097321e-01,\n",
              "          -1.99001506e-02, -1.85216710e-01, -2.25270554e-01,\n",
              "           2.17402160e-01, -2.63079673e-01,  5.70212752e-02,\n",
              "          -1.45220563e-01, -3.51825878e-02,  2.87139136e-02,\n",
              "           3.13724391e-02, -2.19631568e-01, -1.29557759e-01,\n",
              "          -1.42529055e-01, -5.49277999e-02,  5.75287715e-02,\n",
              "           2.32164949e-01,  2.41338834e-01,  2.26393357e-01,\n",
              "           3.22596282e-02, -2.73362901e-02,  8.53356272e-02,\n",
              "           2.60121644e-01, -1.22296534e-01, -2.01028079e-01,\n",
              "          -2.41996214e-01, -1.13133602e-01,  1.77839190e-01,\n",
              "           1.27338946e-01,  6.98172972e-02,  2.50260174e-01,\n",
              "          -1.00473277e-02],\n",
              "         [-2.61244178e-01,  2.27279574e-01, -6.38378086e-03,\n",
              "          -1.93719462e-01,  2.81803071e-01,  9.70415492e-03,\n",
              "          -1.97620705e-01,  2.75747091e-01, -1.09815918e-01,\n",
              "           1.40643090e-01, -1.92851976e-01,  2.37138271e-01,\n",
              "          -2.31117368e-01, -7.36706033e-02,  1.46877140e-01,\n",
              "          -5.62366880e-02, -4.06754687e-02,  9.73139927e-02,\n",
              "          -3.02269340e-01,  4.95261922e-02, -7.76396319e-02,\n",
              "           1.74096614e-01,  2.81650070e-02, -2.29975998e-01,\n",
              "          -1.48438349e-01, -7.92658946e-04,  1.54035792e-01,\n",
              "           1.57170951e-01, -4.33449596e-02,  1.18630469e-01,\n",
              "          -2.43604407e-01, -1.33421540e-01,  7.21843019e-02,\n",
              "          -1.20626055e-01, -2.27887839e-01,  9.50197130e-02,\n",
              "          -2.17559278e-01, -3.05317730e-01, -3.53108421e-02,\n",
              "           2.20691353e-01, -2.35481877e-02, -1.83311999e-01,\n",
              "          -1.81703761e-01,  7.02228546e-02, -1.43882111e-01,\n",
              "           1.07287258e-01,  3.91238444e-02, -2.89505213e-01,\n",
              "          -5.04442453e-02,  2.12705836e-01, -8.50975737e-02,\n",
              "          -1.81543872e-01, -2.44036496e-01, -8.63525048e-02,\n",
              "          -3.00329030e-01, -1.33314461e-01, -2.40473123e-03,\n",
              "           2.64896363e-01, -2.22211286e-01, -3.10184783e-03,\n",
              "           2.35817850e-01,  1.81776941e-01,  2.25801349e-01,\n",
              "          -9.74339321e-02],\n",
              "         [ 1.27605721e-01, -2.14062095e-01,  2.01232865e-01,\n",
              "           1.58123299e-01,  3.99583429e-02, -7.83906970e-03,\n",
              "          -1.40179873e-01,  6.23356961e-02,  2.06051588e-01,\n",
              "          -2.61172265e-01,  7.54386634e-02,  8.81010666e-02,\n",
              "           1.48103967e-01, -1.77780896e-01, -2.65804261e-01,\n",
              "          -2.67963588e-01, -1.55335531e-01,  9.84674543e-02,\n",
              "          -2.65711874e-01, -3.80244069e-02,  1.30415456e-02,\n",
              "           2.23794207e-01,  7.29773119e-02, -8.60425755e-02,\n",
              "           1.37871075e-02,  8.56037159e-03,  2.05976024e-01,\n",
              "          -1.41791895e-01, -1.56206727e-01, -1.48791701e-01,\n",
              "           2.76177794e-01,  1.74839664e-02, -1.42671093e-01,\n",
              "           2.15366483e-01, -1.28659099e-01,  7.96356052e-02,\n",
              "          -2.41985619e-01, -2.84958929e-01, -3.65611464e-02,\n",
              "           1.61576018e-01, -1.88322484e-01,  1.90770388e-01,\n",
              "          -6.11285940e-02,  1.70053244e-02,  2.70390928e-01,\n",
              "           1.09639540e-01, -1.69141546e-01,  1.28826261e-01,\n",
              "           4.83620465e-02, -6.54868782e-02,  1.11337841e-01,\n",
              "           2.54483581e-01, -1.96165487e-01, -3.79131138e-02,\n",
              "           2.09698468e-01,  2.09684804e-01, -2.11352274e-01,\n",
              "           1.95945770e-01,  2.21289083e-01,  2.51850933e-01,\n",
              "          -2.89377838e-01,  1.26737773e-01,  2.48971477e-01,\n",
              "           1.54594839e-01],\n",
              "         [ 5.10868691e-02, -2.34192893e-01,  2.82725185e-01,\n",
              "           2.59093225e-01, -1.46682575e-01, -1.14735149e-01,\n",
              "          -1.90736100e-01,  2.30075479e-01, -2.30209008e-01,\n",
              "           2.47959971e-01, -1.61159113e-02, -2.08166078e-01,\n",
              "          -5.98905087e-02, -2.39322893e-02,  5.99059500e-02,\n",
              "          -1.14904523e-01,  9.85063985e-02, -1.31934613e-01,\n",
              "           1.15576535e-02, -1.94452897e-01,  1.27134621e-01,\n",
              "          -1.76272869e-01, -2.67823905e-01, -2.05666691e-01,\n",
              "           5.39664039e-03, -2.65335292e-01, -2.81472623e-01,\n",
              "           2.45942429e-01, -7.53538981e-02, -1.57776773e-01,\n",
              "           6.43919408e-03, -1.99681111e-02,  1.13605648e-01,\n",
              "           1.61807492e-01,  5.71556166e-02, -2.20487654e-01,\n",
              "           6.77723885e-02,  2.36478150e-01, -4.98204231e-02,\n",
              "           1.78359494e-01,  2.83184677e-01, -1.79266125e-01,\n",
              "          -2.62557209e-01, -3.15099470e-02, -2.07093060e-01,\n",
              "           1.12972543e-01,  1.06978081e-01, -2.10780248e-01,\n",
              "          -1.43582582e-01,  2.64704376e-01, -1.24256454e-01,\n",
              "          -2.20230743e-02, -1.19027585e-01,  2.57947529e-03,\n",
              "           2.65166312e-01, -2.75416416e-03,  1.68012366e-01,\n",
              "           2.93401152e-01,  8.23265985e-02, -2.19528139e-01,\n",
              "          -8.74675363e-02,  1.93656832e-01,  7.76974931e-02,\n",
              "           5.77043779e-02],\n",
              "         [-1.40289977e-01,  1.41072720e-01, -1.76769793e-01,\n",
              "           2.46498659e-01, -4.12266627e-02, -2.29488760e-01,\n",
              "          -1.77090615e-01, -4.73632244e-03, -3.25808413e-02,\n",
              "          -1.06459364e-01, -1.53559700e-01,  2.81163603e-01,\n",
              "           1.44793510e-01,  1.59685016e-01, -3.00461680e-01,\n",
              "          -1.56801313e-01,  1.64580211e-04,  2.65904486e-01,\n",
              "           1.27358496e-01, -2.56661683e-01, -1.30404383e-02,\n",
              "           6.62916293e-03,  2.51458943e-01,  1.02415264e-01,\n",
              "           7.50511512e-02, -1.03326693e-01, -9.12576169e-02,\n",
              "          -2.80610979e-01,  2.09964290e-01, -3.36129278e-01,\n",
              "           9.09178182e-02,  1.70051277e-01, -9.64257680e-03,\n",
              "           1.02557063e-01,  2.53684491e-01,  2.78229892e-01,\n",
              "           2.31679127e-01, -2.79864699e-01, -9.19838771e-02,\n",
              "          -1.26859054e-01,  7.12664351e-02,  2.17953116e-01,\n",
              "           5.98197393e-02,  2.42157876e-01,  1.15032107e-01,\n",
              "          -1.99768811e-01, -2.49040887e-01, -2.93804407e-01,\n",
              "           7.26699382e-02,  1.38332039e-01, -2.10412994e-01,\n",
              "          -1.09631650e-01,  1.12534620e-01, -2.49739930e-01,\n",
              "           1.83923155e-01, -9.33463201e-02, -1.56580850e-01,\n",
              "           2.59827942e-01,  1.15817003e-01,  1.72533859e-02,\n",
              "          -2.79875129e-01,  2.06715822e-01,  2.88502961e-01,\n",
              "          -2.70846605e-01],\n",
              "         [-3.49596106e-02, -1.96128622e-01,  2.06419602e-01,\n",
              "          -1.60376057e-01,  1.45922527e-01, -9.28675234e-02,\n",
              "           1.13069313e-02,  9.77512971e-02,  1.72843516e-01,\n",
              "           1.29319295e-01, -1.77238777e-01,  1.48673430e-01,\n",
              "          -1.14094496e-01,  1.37774348e-01,  1.68483198e-01,\n",
              "           2.81623542e-01,  1.96788743e-01,  5.66413905e-03,\n",
              "          -3.03897947e-01, -4.24512150e-03,  1.06227405e-01,\n",
              "           1.39043128e-04,  4.53108624e-02,  1.84976280e-01,\n",
              "          -2.17107773e-01,  2.55920082e-01,  9.22634676e-02,\n",
              "          -3.10583621e-01,  1.51458427e-01,  1.86413616e-01,\n",
              "           2.62866974e-01,  9.00735706e-03, -1.44382566e-01,\n",
              "          -1.48391768e-01, -2.70254612e-01, -2.97616065e-01,\n",
              "          -1.03790089e-01,  1.09907500e-01, -4.60910611e-02,\n",
              "           3.08831275e-01,  2.89404005e-01, -2.06559837e-01,\n",
              "          -9.05708671e-02, -2.57236630e-01, -2.34827727e-01,\n",
              "           6.14582747e-02, -2.00090423e-01, -4.61681448e-02,\n",
              "          -1.55397519e-01, -6.42641783e-02,  1.27099127e-01,\n",
              "          -1.13039441e-01,  1.15688182e-01, -1.47547305e-01,\n",
              "           1.23036340e-01, -1.52270740e-03,  3.25683784e-03,\n",
              "          -1.54133871e-01, -8.33505765e-02,  2.13025901e-02,\n",
              "           3.04479059e-02, -1.11098483e-01,  1.77495643e-01,\n",
              "           2.41209045e-01],\n",
              "         [-1.10604810e-02, -1.18179895e-01,  2.68725812e-01,\n",
              "          -1.58458620e-01, -3.84291410e-02, -2.05430061e-01,\n",
              "          -2.08042100e-01,  1.41593218e-01, -1.83603521e-02,\n",
              "          -9.65083018e-02, -1.57051101e-01,  1.06382184e-01,\n",
              "          -1.68179452e-01,  4.49240319e-02,  3.92997973e-02,\n",
              "          -1.33957684e-01, -1.60415381e-01, -9.75113586e-02,\n",
              "           1.62367254e-01,  5.70934452e-03, -1.68495059e-01,\n",
              "          -1.63778946e-01, -2.54230827e-01, -1.81512251e-01,\n",
              "           8.43455121e-02, -1.11114897e-01,  2.52313644e-01,\n",
              "          -1.27555236e-01, -1.29706398e-01, -4.44956198e-02,\n",
              "          -1.85970575e-01, -2.26572067e-01,  3.02408393e-02,\n",
              "           1.41159803e-01, -2.53309041e-01,  1.26415879e-01,\n",
              "           1.60753980e-01,  2.15348884e-01, -5.86407445e-02,\n",
              "           7.02727661e-02, -2.01385602e-01,  1.12934791e-01,\n",
              "           1.07992239e-01,  2.09402498e-02, -1.05969701e-02,\n",
              "          -4.94486503e-02,  8.27317908e-02,  1.85677499e-01,\n",
              "           7.59280249e-02, -2.26580307e-01, -1.99749127e-01,\n",
              "          -2.85582375e-02,  2.77568430e-01, -2.88139462e-01,\n",
              "           4.20928039e-02,  1.67277530e-01,  2.23415568e-01,\n",
              "          -2.65113592e-01,  1.84786767e-01,  1.99776441e-01,\n",
              "           2.59077251e-01,  8.45492706e-02,  2.08129153e-01,\n",
              "           2.07952261e-01]], dtype=float32),\n",
              "  array([-0.03813338,  0.02847569,  0.04078231,  0.0589394 ,  0.03466824,\n",
              "          0.04927693,  0.03449696,  0.01478274,  0.03294048, -0.0241767 ,\n",
              "         -0.03282572,  0.03421204,  0.0189412 , -0.01318786,  0.03065545,\n",
              "          0.03674961,  0.03552529,  0.00216815,  0.04230698,  0.05931092,\n",
              "          0.04575222, -0.02626315, -0.04153602, -0.03135661,  0.04993858,\n",
              "          0.0489259 ,  0.03901529,  0.03840415,  0.00523883,  0.0601499 ,\n",
              "          0.02618113,  0.03392629,  0.04360639, -0.03640987,  0.06340388,\n",
              "          0.05282277,  0.02864895,  0.03354897,  0.00037075, -0.0146436 ,\n",
              "          0.03625029,  0.05559869, -0.02669472,  0.05091728, -0.01311445,\n",
              "         -0.03115834,  0.05319718,  0.02346119,  0.04241909,  0.02482112,\n",
              "          0.03207756, -0.03123086,  0.04833195,  0.05745796,  0.02161064,\n",
              "          0.04416239,  0.05294547,  0.0560723 ,  0.04188329,  0.02749781,\n",
              "          0.0506611 , -0.02257742, -0.00481643,  0.01798941], dtype=float32),\n",
              "  array([[-0.02370152, -0.2123125 ,  0.0393003 , ...,  0.08456425,\n",
              "           0.01064049, -0.12166209],\n",
              "         [-0.19235684,  0.05181683,  0.1328398 , ..., -0.15771572,\n",
              "          -0.07333754, -0.03241592],\n",
              "         [ 0.16425535, -0.19366612, -0.12466077, ...,  0.09019431,\n",
              "           0.11507373, -0.0053116 ],\n",
              "         ...,\n",
              "         [ 0.14179075,  0.05356848,  0.04642591, ..., -0.13446043,\n",
              "          -0.07326173,  0.12621032],\n",
              "         [-0.05865864,  0.0757227 ,  0.07094675, ...,  0.00760158,\n",
              "          -0.0743089 ,  0.1478343 ],\n",
              "         [ 0.02621505,  0.10098327, -0.1935976 , ...,  0.15884334,\n",
              "          -0.06767212,  0.04206207]], dtype=float32),\n",
              "  array([ 0.04990875, -0.03895089,  0.04872386,  0.05751419,  0.04236078,\n",
              "          0.05404356, -0.03669957, -0.02974222,  0.04938596,  0.0505291 ,\n",
              "         -0.04457082,  0.0508514 , -0.01447509, -0.00036525,  0.04392636,\n",
              "          0.05022639, -0.01553842,  0.05216705, -0.02008772,  0.0521766 ,\n",
              "          0.05200267, -0.02848884,  0.05815521, -0.02484677, -0.03710167,\n",
              "          0.04987058,  0.00996264,  0.05692809,  0.05069111, -0.00233331,\n",
              "          0.05035317, -0.00306066, -0.01672145,  0.0476001 ,  0.05355137,\n",
              "         -0.00418731,  0.05243922,  0.0480942 , -0.01351734, -0.01153765,\n",
              "          0.0541327 , -0.03708629, -0.01558327, -0.02493407,  0.05067764,\n",
              "          0.04128132, -0.03374561,  0.03882749, -0.02869055,  0.05448424,\n",
              "          0.03954766,  0.05814183,  0.0545725 ,  0.0561615 ,  0.04943806,\n",
              "          0.04995609, -0.03839656,  0.04771709, -0.01485764, -0.03506691,\n",
              "         -0.03894632, -0.0160604 ,  0.05349349, -0.03569489], dtype=float32),\n",
              "  array([[ 0.22311382],\n",
              "         [-0.17008173],\n",
              "         [ 0.20616584],\n",
              "         [ 0.23403017],\n",
              "         [ 0.28068277],\n",
              "         [ 0.1716139 ],\n",
              "         [-0.20189422],\n",
              "         [-0.02371364],\n",
              "         [ 0.14433451],\n",
              "         [ 0.28975758],\n",
              "         [-0.01056196],\n",
              "         [ 0.17906907],\n",
              "         [-0.07362741],\n",
              "         [-0.27205783],\n",
              "         [ 0.08068629],\n",
              "         [ 0.2303949 ],\n",
              "         [-0.26524168],\n",
              "         [ 0.28544086],\n",
              "         [-0.20900169],\n",
              "         [ 0.11898797],\n",
              "         [ 0.04582905],\n",
              "         [-0.17186655],\n",
              "         [ 0.27311376],\n",
              "         [-0.08249684],\n",
              "         [-0.01458099],\n",
              "         [ 0.13253655],\n",
              "         [ 0.02528276],\n",
              "         [ 0.2352185 ],\n",
              "         [ 0.07848597],\n",
              "         [-0.03498595],\n",
              "         [ 0.08184792],\n",
              "         [-0.18914337],\n",
              "         [-0.14571995],\n",
              "         [ 0.13988726],\n",
              "         [ 0.2780957 ],\n",
              "         [-0.28606325],\n",
              "         [ 0.28302708],\n",
              "         [ 0.1881269 ],\n",
              "         [-0.16971967],\n",
              "         [-0.07784514],\n",
              "         [ 0.16915578],\n",
              "         [-0.2841553 ],\n",
              "         [-0.0680076 ],\n",
              "         [-0.1761395 ],\n",
              "         [ 0.0286478 ],\n",
              "         [ 0.27998173],\n",
              "         [-0.04172926],\n",
              "         [ 0.15063368],\n",
              "         [-0.04125142],\n",
              "         [ 0.22200385],\n",
              "         [ 0.01140772],\n",
              "         [ 0.05797393],\n",
              "         [ 0.29457167],\n",
              "         [ 0.04193923],\n",
              "         [ 0.23552969],\n",
              "         [ 0.27010682],\n",
              "         [-0.05477911],\n",
              "         [ 0.309235  ],\n",
              "         [-0.12585098],\n",
              "         [-0.18826272],\n",
              "         [-0.2968334 ],\n",
              "         [-0.14789066],\n",
              "         [ 0.23518954],\n",
              "         [-0.13588084]], dtype=float32),\n",
              "  array([0.04955793], dtype=float32)],\n",
              " [array([[-1.86900929e-01,  3.49566005e-02, -7.14926573e-04,\n",
              "           4.25343551e-02, -9.83185172e-02, -1.65956706e-01,\n",
              "           8.02967623e-02,  1.02863293e-02,  1.18844286e-01,\n",
              "           1.29850283e-01, -2.84711421e-01, -2.68462867e-01,\n",
              "           2.54351377e-01, -1.79644868e-01,  2.79099971e-01,\n",
              "           1.28155621e-02,  2.84412444e-01,  2.61636674e-01,\n",
              "          -4.57812026e-02,  1.38853118e-01,  1.33399248e-01,\n",
              "           1.43323407e-01,  1.67097673e-01, -2.18931079e-01,\n",
              "           2.59976387e-01, -6.61123842e-02, -2.92053729e-01,\n",
              "           6.49124011e-02,  1.46988183e-01, -4.19807248e-02,\n",
              "           1.71751846e-02,  1.69888198e-01, -5.21525703e-02,\n",
              "          -2.72261918e-01, -1.54773638e-01,  1.76657170e-01,\n",
              "          -2.24244043e-01,  7.56685510e-02, -1.09246433e-01,\n",
              "          -4.22927290e-02,  5.43398112e-02,  1.79469302e-01,\n",
              "           2.91590095e-01, -2.04274300e-02,  4.05114144e-02,\n",
              "          -2.40665495e-01, -2.76189178e-01, -4.16658400e-03,\n",
              "           1.72079504e-01, -7.87361041e-02, -1.08022861e-01,\n",
              "          -3.62774022e-02, -2.92134911e-01,  2.35898986e-01,\n",
              "           1.31601736e-01,  2.02427004e-02, -1.04651779e-01,\n",
              "           5.88060580e-02,  2.50637859e-01, -7.94313997e-02,\n",
              "          -1.26025360e-02,  2.25378922e-03, -1.41882047e-01,\n",
              "          -2.29798630e-01],\n",
              "         [ 1.17303655e-01,  2.22153485e-01,  2.16196761e-01,\n",
              "          -2.21895158e-01,  1.52381867e-01, -1.88439116e-01,\n",
              "           1.97712570e-01, -1.86103687e-01,  2.70366669e-01,\n",
              "          -8.76302868e-02, -7.29185194e-02, -1.27406389e-01,\n",
              "          -2.29265973e-01,  8.14620405e-03, -2.28255928e-01,\n",
              "          -1.29885614e-01,  3.25279720e-02, -1.36462867e-01,\n",
              "           8.47690925e-02,  2.90348262e-01, -1.35346398e-01,\n",
              "           2.36258358e-01,  1.77156225e-01, -1.03550307e-01,\n",
              "          -8.90061036e-02,  3.99017558e-02, -1.08791322e-01,\n",
              "           5.83457425e-02,  2.27872267e-01,  1.92725360e-01,\n",
              "          -1.66166380e-01,  2.59939164e-01, -1.24746859e-01,\n",
              "           1.09909870e-01,  5.56587577e-02,  2.29372084e-03,\n",
              "          -2.17357904e-01,  2.73442984e-01, -2.49288697e-03,\n",
              "           1.15036862e-02, -7.66091421e-02,  2.11928695e-01,\n",
              "          -1.07041642e-01,  2.65778184e-01, -1.45031303e-01,\n",
              "          -9.92698893e-02, -2.23365709e-01,  1.97776899e-01,\n",
              "           8.26141760e-02,  1.03534188e-03, -1.84358597e-01,\n",
              "          -2.63474435e-01, -7.17552006e-02,  2.81282037e-01,\n",
              "           1.03044868e-01,  2.44879186e-01, -1.36102960e-01,\n",
              "          -1.47795692e-01, -2.73449659e-01,  1.02539390e-01,\n",
              "           1.89395189e-01, -1.39465585e-01,  2.64687777e-01,\n",
              "          -1.95613042e-01],\n",
              "         [-1.75099120e-01,  8.16325843e-02,  2.26920322e-01,\n",
              "           2.47616410e-01,  5.27863503e-02,  1.92061216e-01,\n",
              "          -2.26260379e-01, -1.30435042e-02,  2.22409964e-01,\n",
              "           2.02035438e-02,  2.59716064e-01, -2.28629969e-02,\n",
              "          -3.29701304e-02, -1.38552412e-01,  1.67731315e-01,\n",
              "          -2.73462385e-01,  5.48373386e-02, -1.95903629e-01,\n",
              "          -3.24919932e-02, -9.54308659e-02, -1.08844772e-01,\n",
              "           2.75503457e-01, -1.52276188e-01,  1.53052017e-01,\n",
              "           2.65199035e-01,  1.16042420e-01,  4.93363403e-02,\n",
              "           1.20877981e-01,  2.55992115e-02,  2.19127044e-01,\n",
              "          -3.25638801e-02,  2.79786915e-01,  6.92230314e-02,\n",
              "          -4.02341746e-02, -1.07405066e-01,  2.24227265e-01,\n",
              "          -2.59694070e-01, -1.99073121e-01,  1.45778358e-01,\n",
              "          -6.31065816e-02,  8.35006535e-02, -2.36820236e-01,\n",
              "           3.52020077e-02,  7.83803537e-02,  1.09505013e-01,\n",
              "          -3.89859080e-03, -2.65271422e-02, -1.39420405e-01,\n",
              "           1.17158398e-01, -1.88923523e-01, -1.18579753e-01,\n",
              "          -1.45799994e-01,  6.41097054e-02, -2.69312292e-01,\n",
              "          -9.32065621e-02, -1.63660586e-01, -4.84586246e-02,\n",
              "           1.02997586e-01, -2.63042108e-04, -7.02288523e-02,\n",
              "           2.05400601e-01,  1.34765401e-01, -6.60859793e-02,\n",
              "           1.95360839e-01],\n",
              "         [ 1.70863882e-01,  2.27884844e-01, -7.07615316e-02,\n",
              "          -1.41577199e-01, -8.18191245e-02, -2.80924201e-01,\n",
              "          -1.18925489e-01,  5.37744686e-02,  8.13267380e-02,\n",
              "           2.68626541e-01, -1.44228846e-01,  2.62335986e-01,\n",
              "          -7.65845180e-02,  3.78115810e-02, -2.69818068e-01,\n",
              "          -2.58192509e-01, -1.14730358e-01,  1.23037063e-01,\n",
              "          -1.20105617e-01,  1.98090717e-01, -6.22670315e-02,\n",
              "           4.93614674e-02,  2.37536877e-01,  2.39051983e-01,\n",
              "           2.71557868e-01, -1.11278638e-01, -2.55623907e-01,\n",
              "          -1.62959456e-01,  8.68850872e-02, -2.90483590e-02,\n",
              "          -2.09438756e-01,  1.30890474e-01,  2.23444417e-01,\n",
              "          -1.84944019e-01, -9.14760083e-02, -1.19053766e-01,\n",
              "           1.04362570e-01,  2.69341618e-01,  2.51397826e-02,\n",
              "          -2.10852727e-01,  3.20927496e-03,  2.38364235e-01,\n",
              "          -1.05825692e-01,  1.32928878e-01,  1.15562379e-01,\n",
              "          -2.34310955e-01, -1.59952909e-01, -1.97402537e-01,\n",
              "           3.30356061e-02, -4.03951183e-02, -1.05042674e-01,\n",
              "           1.42663538e-01,  1.60110239e-02, -1.03165418e-01,\n",
              "           2.82741129e-01, -1.71148449e-01, -2.13132158e-01,\n",
              "          -2.82023907e-01,  1.62391022e-01,  9.92540866e-02,\n",
              "           9.46530774e-02, -5.30220605e-02,  2.51601990e-02,\n",
              "          -1.13072775e-01],\n",
              "         [-2.52377599e-01, -1.23081943e-02, -1.02207914e-01,\n",
              "           1.66565366e-02,  2.29647636e-01, -1.75367922e-01,\n",
              "          -1.94213003e-01, -1.11407496e-01, -2.38203466e-01,\n",
              "          -2.57864594e-01, -8.47286656e-02, -2.03444019e-01,\n",
              "           1.78284153e-01,  5.41362055e-02, -1.88607544e-01,\n",
              "           2.18571261e-01,  6.04144596e-02, -1.23996891e-01,\n",
              "           1.18907318e-01,  1.32694602e-01, -1.18279882e-01,\n",
              "           1.10397115e-01,  1.04531877e-01,  2.19257921e-02,\n",
              "          -8.65471512e-02, -2.49971151e-01,  1.07997991e-01,\n",
              "           2.83069257e-03, -1.50120035e-01,  4.17081602e-02,\n",
              "          -1.12898182e-02, -2.39210784e-01,  1.99492738e-01,\n",
              "          -1.81975998e-02, -2.84247965e-01,  2.30666846e-01,\n",
              "          -1.61304787e-01,  6.37989044e-02, -1.57332987e-01,\n",
              "           2.77639806e-01,  4.07913327e-02, -4.79119085e-02,\n",
              "           8.01318809e-02,  2.70736456e-01,  2.32748270e-01,\n",
              "          -2.32485443e-01, -1.95952967e-01,  2.52409697e-01,\n",
              "          -1.53105557e-01,  1.10908084e-01,  4.60716411e-02,\n",
              "          -1.70226142e-01,  2.45796949e-01,  1.59258559e-01,\n",
              "           1.52445540e-01, -1.35411635e-01, -1.23816006e-01,\n",
              "          -1.11813270e-01,  2.34001637e-01, -1.29753932e-01,\n",
              "           2.01047376e-01, -1.16275750e-01,  2.93851104e-02,\n",
              "           9.79749486e-02],\n",
              "         [ 5.93901910e-02,  1.26249380e-02, -1.28350258e-01,\n",
              "          -3.84712555e-02,  1.98952481e-01,  2.55555332e-01,\n",
              "          -2.39000767e-01, -9.83868018e-02,  1.91626206e-01,\n",
              "          -1.96776003e-01, -2.24317476e-01, -2.45800287e-01,\n",
              "          -2.26356998e-01, -2.45203197e-01,  1.76857278e-01,\n",
              "          -5.98377474e-02,  6.78328201e-02, -2.61619329e-01,\n",
              "           2.41328016e-01, -2.39098892e-01,  6.47413880e-02,\n",
              "           1.90736391e-02, -1.53869227e-01, -1.97176591e-01,\n",
              "          -8.73495191e-02, -1.12615593e-01,  1.46602139e-01,\n",
              "          -8.33573192e-02, -1.02753587e-01, -1.29466623e-01,\n",
              "          -1.24812923e-01,  6.66986778e-02, -1.90104008e-01,\n",
              "          -2.70378292e-01,  2.64161732e-02,  7.08530843e-02,\n",
              "           1.90266952e-01,  9.19615626e-02,  8.66418481e-02,\n",
              "          -1.60129786e-01, -2.11368784e-01, -8.09998736e-02,\n",
              "          -8.17211717e-02, -1.26721054e-01,  1.93119079e-01,\n",
              "           2.28127122e-01, -2.12916449e-01, -2.78243661e-01,\n",
              "           2.58519292e-01,  1.04216300e-01,  1.97971299e-01,\n",
              "           2.87526578e-01,  1.00211851e-01,  3.73833850e-02,\n",
              "          -1.88045740e-01,  1.92777723e-01, -2.86430299e-01,\n",
              "          -4.61476743e-02,  2.70275585e-02,  6.94265589e-02,\n",
              "          -6.64758906e-02,  2.84066916e-01, -1.48069058e-02,\n",
              "          -2.38190502e-01],\n",
              "         [ 9.14330408e-02,  2.40089878e-01,  4.39850539e-02,\n",
              "           8.67273733e-02, -1.23370200e-01,  2.71569014e-01,\n",
              "          -2.79933840e-01, -5.24317920e-02,  5.21393381e-02,\n",
              "          -2.65729547e-01, -3.85069922e-02,  4.53669243e-02,\n",
              "          -2.22955167e-01, -2.67192274e-01,  3.20287095e-03,\n",
              "          -1.09860543e-02, -1.07945547e-01, -1.03705190e-01,\n",
              "           5.38063087e-02,  9.05789733e-02,  1.44419894e-01,\n",
              "           2.09365487e-01, -8.01075026e-02, -2.80041665e-01,\n",
              "           8.33178312e-02, -6.05866648e-02, -2.60565639e-01,\n",
              "           3.57366987e-02, -6.21760450e-02, -2.46841386e-01,\n",
              "          -1.11111969e-01, -6.41766377e-03, -2.40982890e-01,\n",
              "           1.43639520e-01,  1.81359738e-01, -1.79172084e-01,\n",
              "           2.41746590e-01, -6.99980110e-02, -1.52945956e-02,\n",
              "          -1.60752594e-01,  6.65209070e-02, -2.72734046e-01,\n",
              "          -6.20074533e-02, -2.75835782e-01, -1.67153105e-01,\n",
              "          -2.74068058e-01, -2.77167827e-01,  1.49530113e-01,\n",
              "           1.71273947e-01,  1.68729387e-02, -5.18062115e-02,\n",
              "          -2.75716692e-01, -2.09146708e-01,  2.55043805e-01,\n",
              "          -1.19057313e-01,  1.15314119e-01,  1.35771036e-01,\n",
              "           1.43524110e-01, -6.29179329e-02, -2.36358978e-02,\n",
              "          -5.92280589e-02, -9.39490646e-02, -1.33954361e-01,\n",
              "           1.04587488e-01],\n",
              "         [ 2.71101803e-01, -4.83716726e-02, -2.14798927e-01,\n",
              "           2.13929459e-01,  4.84991148e-02, -2.62019932e-01,\n",
              "          -2.71343082e-01, -2.04426870e-01, -2.07240298e-01,\n",
              "          -1.17784932e-01,  2.64715075e-01, -1.05232894e-01,\n",
              "           2.14457177e-02, -2.61135936e-01,  1.73574582e-01,\n",
              "           3.15375067e-02, -1.67427305e-02,  2.93127030e-01,\n",
              "          -1.75172240e-01, -2.50722796e-01, -2.24355906e-01,\n",
              "           4.01478596e-02, -1.23495974e-01, -9.78669152e-03,\n",
              "           2.80835181e-01, -8.63612443e-02, -8.82856175e-02,\n",
              "           1.98336244e-01,  2.43254200e-01, -2.34268736e-02,\n",
              "          -2.21413411e-02, -1.06925696e-01, -1.47221908e-01,\n",
              "           8.76073241e-02, -1.78497523e-01, -2.46245459e-01,\n",
              "           2.43388399e-01,  2.44869426e-01, -2.98822999e-01,\n",
              "           2.95185089e-01,  2.60903507e-01,  1.22375883e-01,\n",
              "          -1.98726401e-01,  1.78798124e-01, -1.06776200e-01,\n",
              "           2.52287179e-01, -2.89753098e-02, -2.08065167e-01,\n",
              "           2.12645471e-01, -7.49648288e-02,  2.51623511e-01,\n",
              "           2.28362158e-01,  3.44686061e-02,  1.75072968e-01,\n",
              "           2.62263864e-01, -3.87737416e-02, -1.30174920e-01,\n",
              "           2.59929389e-01,  8.75303745e-02,  1.98004201e-01,\n",
              "          -4.30508927e-02,  2.20134988e-01,  8.89540911e-02,\n",
              "           7.25984722e-02]], dtype=float32),\n",
              "  array([-8.66371393e-03, -4.00118064e-03,  1.99236292e-02, -2.76543647e-02,\n",
              "          4.68846876e-03, -1.06070954e-02,  2.10346226e-02, -1.67280156e-02,\n",
              "         -6.81530545e-03, -8.36329442e-03,  1.23123955e-02,  6.91226823e-03,\n",
              "         -2.29568556e-02, -2.93118577e-03, -1.03493901e-02, -3.85388942e-03,\n",
              "          7.90653750e-03,  1.27313398e-02, -4.81740251e-04,  1.46077694e-02,\n",
              "          2.10409891e-02, -5.58374124e-03,  3.49559495e-03, -3.68158112e-06,\n",
              "         -5.51616447e-03,  1.68545730e-02,  3.04468460e-02, -3.70835210e-03,\n",
              "          1.45271001e-02,  9.89507046e-03, -8.81459191e-03,  7.75256939e-03,\n",
              "         -1.85065698e-02,  2.25932524e-02,  1.78635344e-02,  1.11420630e-02,\n",
              "          2.25033164e-02, -8.31696019e-03,  1.16357049e-02,  1.97377782e-02,\n",
              "         -1.38921095e-02, -7.70860026e-03,  1.69351641e-02,  3.74726206e-02,\n",
              "         -9.96832177e-03, -1.35768708e-02,  1.24224611e-02,  1.27349282e-02,\n",
              "          2.38297181e-03,  2.06670957e-03, -9.70254466e-03, -1.05115417e-02,\n",
              "          1.04503697e-02, -4.78952052e-03,  2.48087738e-02,  8.39525647e-03,\n",
              "          9.81690083e-03, -9.22909193e-03,  1.92379765e-02,  2.66554859e-02,\n",
              "          3.23745562e-03, -3.60300252e-03, -1.33721856e-02,  6.95755647e-04],\n",
              "        dtype=float32),\n",
              "  array([[-0.14294812,  0.20612761, -0.03979276, ...,  0.1711625 ,\n",
              "          -0.21339013, -0.0645163 ],\n",
              "         [ 0.04656368, -0.0772614 , -0.19033156, ...,  0.06044288,\n",
              "           0.07331383, -0.19724981],\n",
              "         [ 0.07469829, -0.01077921,  0.1862643 , ...,  0.09944698,\n",
              "           0.03940363, -0.08504459],\n",
              "         ...,\n",
              "         [ 0.21997407,  0.19137423, -0.00346658, ...,  0.20925575,\n",
              "           0.0220034 , -0.20830113],\n",
              "         [-0.09149597, -0.10931115,  0.08650611, ..., -0.02804355,\n",
              "          -0.04025552, -0.11797898],\n",
              "         [-0.09130313,  0.03382574, -0.05267756, ..., -0.16311884,\n",
              "          -0.09268352, -0.01900584]], dtype=float32),\n",
              "  array([ 0.02846902,  0.00294763,  0.07320394, -0.00490479,  0.00870469,\n",
              "          0.02408927,  0.00176555,  0.01245855, -0.00862102, -0.00084381,\n",
              "          0.03666974,  0.00855661,  0.02076425, -0.03693391,  0.02843918,\n",
              "         -0.02448941, -0.06614065,  0.01002095,  0.04722244,  0.03958912,\n",
              "          0.03215671,  0.00452224,  0.00454916, -0.02043083,  0.00874067,\n",
              "         -0.00125701, -0.01365007,  0.00244609, -0.01170699, -0.00576225,\n",
              "          0.05407715, -0.02967934, -0.00796514,  0.01772334, -0.00704832,\n",
              "          0.05618455,  0.04349393, -0.01371266, -0.00436868, -0.01021265,\n",
              "          0.00548676,  0.02672636, -0.04956747, -0.00543694, -0.01042954,\n",
              "         -0.00099728, -0.0095494 , -0.00255301,  0.00609148, -0.0486075 ,\n",
              "         -0.00364403, -0.03667315, -0.00875248, -0.01809763,  0.01813545,\n",
              "          0.00960406, -0.00083311,  0.00048809,  0.05980574, -0.01695719,\n",
              "          0.02997188,  0.01016598, -0.00256022, -0.0229451 ], dtype=float32),\n",
              "  array([[ 0.2019727 ],\n",
              "         [ 0.04050594],\n",
              "         [ 0.30173087],\n",
              "         [-0.07340129],\n",
              "         [ 0.05641182],\n",
              "         [ 0.12542729],\n",
              "         [ 0.08372805],\n",
              "         [ 0.09335887],\n",
              "         [-0.06557925],\n",
              "         [-0.00034822],\n",
              "         [ 0.17869143],\n",
              "         [ 0.24935347],\n",
              "         [ 0.11274302],\n",
              "         [-0.24300294],\n",
              "         [ 0.29829964],\n",
              "         [-0.10052133],\n",
              "         [-0.26210678],\n",
              "         [ 0.14605467],\n",
              "         [ 0.21592674],\n",
              "         [ 0.29555064],\n",
              "         [ 0.16356283],\n",
              "         [ 0.03328782],\n",
              "         [ 0.05026637],\n",
              "         [-0.07211797],\n",
              "         [ 0.06627532],\n",
              "         [-0.03668814],\n",
              "         [-0.09382112],\n",
              "         [ 0.05973442],\n",
              "         [-0.01582447],\n",
              "         [-0.26040465],\n",
              "         [ 0.21365406],\n",
              "         [-0.23113276],\n",
              "         [-0.10886415],\n",
              "         [ 0.20141347],\n",
              "         [-0.03495697],\n",
              "         [ 0.22724614],\n",
              "         [ 0.32703403],\n",
              "         [-0.03122967],\n",
              "         [-0.04347778],\n",
              "         [-0.01025032],\n",
              "         [ 0.04789411],\n",
              "         [ 0.21829315],\n",
              "         [-0.19203827],\n",
              "         [-0.08436174],\n",
              "         [-0.03693875],\n",
              "         [-0.14744283],\n",
              "         [-0.03966938],\n",
              "         [-0.07093882],\n",
              "         [ 0.04623403],\n",
              "         [-0.21299209],\n",
              "         [-0.12026372],\n",
              "         [-0.16726233],\n",
              "         [-0.02803866],\n",
              "         [-0.29835898],\n",
              "         [ 0.17759795],\n",
              "         [ 0.17516163],\n",
              "         [ 0.0073467 ],\n",
              "         [ 0.01171267],\n",
              "         [ 0.25554967],\n",
              "         [-0.1859978 ],\n",
              "         [ 0.16214247],\n",
              "         [ 0.11832787],\n",
              "         [-0.01824472],\n",
              "         [-0.18885787]], dtype=float32),\n",
              "  array([0.32847396], dtype=float32)],\n",
              " [array([[ 5.93601353e-02,  3.61974269e-01, -3.19479525e-01,\n",
              "          -2.18380857e-02, -3.06739658e-01, -3.19952965e-01,\n",
              "           7.67673075e-04,  1.33195102e-01, -4.03057903e-01,\n",
              "           1.97927594e-01,  3.68452035e-02, -1.27812132e-01,\n",
              "          -3.14504117e-01,  2.07297996e-01, -2.80653954e-01,\n",
              "          -2.32653953e-02, -2.70059615e-01,  1.72619253e-01,\n",
              "          -6.04591258e-02, -2.24843651e-01, -3.47943068e-01,\n",
              "          -1.95407525e-01, -2.46195450e-01,  2.79441416e-01,\n",
              "           5.09307608e-02, -2.60089308e-01, -2.98497856e-01,\n",
              "          -2.57550448e-01, -3.58953848e-02, -2.60170966e-01,\n",
              "           3.24129850e-01, -7.15758204e-02],\n",
              "         [ 1.61713481e-01,  3.13923419e-01, -2.17648789e-01,\n",
              "           7.77489739e-03, -7.30738267e-02, -3.29014987e-01,\n",
              "          -7.33191296e-02,  4.00763959e-01,  9.96112302e-02,\n",
              "          -5.57813048e-02, -2.28377089e-01, -1.69341683e-01,\n",
              "          -1.87915266e-01, -5.85024338e-03,  2.67048240e-01,\n",
              "          -1.12981007e-01,  3.28222424e-01,  3.93082440e-01,\n",
              "           1.39881209e-01,  2.60260073e-04,  2.79206097e-01,\n",
              "          -2.15200111e-01,  2.53927737e-01, -2.22815350e-01,\n",
              "           1.59131095e-01, -3.36390793e-01,  3.25444609e-01,\n",
              "          -2.68948436e-01,  1.13454022e-01, -1.26620352e-01,\n",
              "           1.43863648e-01, -3.67931798e-02],\n",
              "         [-1.67342335e-01, -8.09694752e-02,  2.69940794e-01,\n",
              "          -4.50034812e-03, -3.30239683e-02, -2.91319311e-01,\n",
              "           4.01033871e-02,  1.17192827e-01,  2.47944936e-01,\n",
              "          -1.30583286e-01, -4.02846217e-01,  3.27800065e-01,\n",
              "           3.33423354e-02,  6.71493411e-02,  7.42520094e-02,\n",
              "           8.42908099e-02,  5.49915694e-02,  2.58403808e-01,\n",
              "           1.80543575e-03, -8.70977119e-02, -4.06727046e-01,\n",
              "          -3.55625927e-01, -1.42517075e-01, -1.74275681e-01,\n",
              "          -1.81526780e-01, -8.31186771e-02,  2.22430363e-01,\n",
              "           2.80694157e-01, -4.06911433e-01, -2.42847309e-01,\n",
              "           9.67083126e-02, -3.32638174e-01],\n",
              "         [-5.25680967e-02, -2.20154315e-01, -1.89528584e-01,\n",
              "           1.81007698e-01,  2.38456056e-01, -1.79879531e-01,\n",
              "          -3.83375376e-01, -6.92423880e-02,  4.79406826e-02,\n",
              "           2.77449220e-01, -3.52896243e-01, -3.60626370e-01,\n",
              "          -1.74537525e-02,  3.39897513e-01,  3.48325282e-01,\n",
              "          -3.08413148e-01,  1.43280715e-01, -5.59945442e-02,\n",
              "           2.50244103e-02, -2.61775911e-01, -3.73559803e-01,\n",
              "          -1.41172856e-01,  1.41269207e-01, -1.61612071e-02,\n",
              "          -3.32632601e-01,  2.62112409e-01, -1.58029705e-01,\n",
              "           4.39311005e-02, -2.62462527e-01, -3.45679611e-01,\n",
              "           3.49398822e-01, -9.39893536e-04],\n",
              "         [-2.53675938e-01, -1.91362262e-01, -3.41726035e-01,\n",
              "           1.39282927e-01, -2.95155585e-01,  3.58763218e-01,\n",
              "           5.11048138e-02,  3.30677867e-01,  2.30160460e-01,\n",
              "          -3.37025344e-01,  3.30791958e-02, -2.71501660e-01,\n",
              "          -6.09343825e-03, -4.55336273e-02,  2.67587572e-01,\n",
              "           1.42279547e-02,  3.34159285e-01,  1.08283751e-01,\n",
              "           2.64007449e-01, -1.23752110e-01, -6.83977157e-02,\n",
              "           3.69500071e-01,  3.25303942e-01,  2.04107180e-01,\n",
              "           1.47138372e-01,  3.05294548e-03, -1.72465399e-01,\n",
              "          -1.29883975e-01, -1.27975628e-01,  1.01839140e-01,\n",
              "          -2.19158798e-01, -2.75649279e-01],\n",
              "         [ 3.69463086e-01,  3.52498621e-01,  1.62849009e-01,\n",
              "          -5.95525689e-02,  3.75258587e-02,  3.14457268e-01,\n",
              "          -3.39744911e-02, -4.04450268e-01, -1.03787668e-01,\n",
              "           2.24988773e-01,  3.71741205e-01, -2.50244617e-01,\n",
              "           2.80088127e-01, -1.23075724e-01, -3.92493963e-01,\n",
              "          -7.84927383e-02, -7.84018189e-02,  2.46054113e-01,\n",
              "           3.40810627e-01,  2.38791540e-01, -1.05747625e-01,\n",
              "           3.39429267e-02, -1.32655516e-01,  3.43338579e-01,\n",
              "           1.01943994e-02,  1.81652352e-01, -1.79083049e-01,\n",
              "          -9.81621072e-02, -3.47495109e-01,  1.53733909e-01,\n",
              "          -1.90660775e-01, -2.27800697e-01],\n",
              "         [ 8.07261541e-02,  2.79278010e-01, -2.98222955e-02,\n",
              "           2.90670902e-01, -2.87944049e-01, -7.02491626e-02,\n",
              "           2.76386470e-01,  2.24507347e-01, -5.21411672e-02,\n",
              "          -2.31059387e-01, -5.80002069e-02, -3.48501861e-01,\n",
              "          -4.18556482e-01,  2.39207089e-01,  2.76375525e-02,\n",
              "           2.59594053e-01,  1.70304075e-01,  5.59407659e-02,\n",
              "          -3.23815122e-02, -3.74431312e-01,  2.41009608e-01,\n",
              "           8.45952183e-02,  3.06305200e-01, -3.42292994e-01,\n",
              "          -6.70177788e-02, -9.03333426e-02,  2.46326387e-01,\n",
              "           2.80610412e-01, -3.10166389e-01, -3.47327799e-01,\n",
              "          -1.70034721e-01, -1.60475865e-01],\n",
              "         [-8.31702352e-02,  3.88308875e-02,  1.74597353e-02,\n",
              "           3.46503168e-01, -9.44126993e-02, -2.13699669e-01,\n",
              "           3.31307769e-01,  1.83284193e-01,  1.18568435e-01,\n",
              "           2.29293138e-01,  3.09727192e-01, -2.42154226e-01,\n",
              "           2.57120840e-02, -1.95362329e-01,  3.61547530e-01,\n",
              "           2.01269254e-01,  2.99125403e-01, -2.60821491e-01,\n",
              "           2.06188798e-01, -1.02760263e-01, -3.64921182e-01,\n",
              "           3.91380727e-01,  2.04545796e-01,  7.22931102e-02,\n",
              "          -1.60759360e-01, -8.75983201e-03,  1.57322496e-01,\n",
              "           3.46237123e-01, -3.16626757e-01, -1.02141224e-01,\n",
              "           2.47888337e-03, -1.12836130e-01]], dtype=float32),\n",
              "  array([-0.03649182, -0.03681315, -0.03602927, -0.0346902 , -0.0360187 ,\n",
              "          0.03815073,  0.03697658,  0.03715455,  0.03644593,  0.03652699,\n",
              "          0.03673327, -0.03647949,  0.03650888, -0.03651046,  0.03635436,\n",
              "         -0.03593337, -0.03602342,  0.0372524 ,  0.03644316,  0.03685154,\n",
              "          0.0380324 ,  0.03658115, -0.03633357, -0.03617994, -0.03688094,\n",
              "          0.03631536, -0.03657939,  0.03597313,  0.03035956, -0.03619004,\n",
              "          0.03750896, -0.03656664], dtype=float32),\n",
              "  array([[-0.04116464],\n",
              "         [-0.23358959],\n",
              "         [-0.18552792],\n",
              "         [-0.2570723 ],\n",
              "         [-0.10249788],\n",
              "         [ 0.04024564],\n",
              "         [ 0.42037323],\n",
              "         [ 0.26675814],\n",
              "         [ 0.32546526],\n",
              "         [ 0.44747347],\n",
              "         [ 0.35524496],\n",
              "         [-0.33631763],\n",
              "         [ 0.29036918],\n",
              "         [-0.16899754],\n",
              "         [ 0.28143525],\n",
              "         [-0.3741748 ],\n",
              "         [-0.3569356 ],\n",
              "         [ 0.12439805],\n",
              "         [ 0.44133878],\n",
              "         [ 0.22240788],\n",
              "         [ 0.05632005],\n",
              "         [ 0.13552012],\n",
              "         [-0.2373949 ],\n",
              "         [-0.13296779],\n",
              "         [-0.17558944],\n",
              "         [ 0.43724757],\n",
              "         [-0.22180943],\n",
              "         [ 0.40175903],\n",
              "         [ 0.03134226],\n",
              "         [-0.19690096],\n",
              "         [ 0.0529855 ],\n",
              "         [-0.12119126]], dtype=float32),\n",
              "  array([0.03766242], dtype=float32)],\n",
              " [array([[ 3.2813129e-01,  3.8578832e-01,  1.2383788e-01, -3.4528199e-01,\n",
              "          -2.8934531e-02,  8.2869366e-02, -2.9326580e-02, -2.1355452e-02,\n",
              "          -3.0656296e-01, -2.5663021e-01, -2.6336625e-01,  1.3617259e-01,\n",
              "          -3.6023805e-01,  7.8869872e-02, -3.6069250e-01, -9.8875187e-02,\n",
              "           5.9934165e-02,  3.5008910e-01, -1.0880095e-02,  1.8005979e-01,\n",
              "          -3.1964052e-01, -1.0397047e-01,  3.8348275e-01, -2.0284632e-01,\n",
              "           2.7366522e-01, -2.7371800e-01,  2.1668832e-01,  3.6559531e-01,\n",
              "          -4.0767182e-02, -2.8927076e-01, -7.4485652e-02, -1.8077394e-01],\n",
              "         [ 8.8567927e-02,  3.2777812e-02, -3.0057555e-01,  1.9925256e-01,\n",
              "          -3.3268845e-01, -3.2214740e-01,  4.2968354e-04,  1.5115431e-01,\n",
              "          -2.8731975e-01, -2.7575308e-01,  1.7032033e-01, -1.7227715e-01,\n",
              "           1.9954531e-01, -6.2781677e-02, -2.0773084e-01, -3.2908863e-01,\n",
              "           4.6812952e-02, -3.8922407e-02, -1.3908677e-01,  1.1955841e-01,\n",
              "          -3.2316217e-01,  1.3964416e-01, -3.9188749e-01, -2.0379274e-01,\n",
              "           3.5512665e-01,  1.5125635e-01,  1.2832321e-01,  2.6982692e-01,\n",
              "          -1.7069842e-01,  3.1081635e-01,  3.6213834e-02, -3.0397159e-01],\n",
              "         [ 6.4301930e-02,  3.6073837e-01,  4.4155914e-02, -3.0348885e-01,\n",
              "          -2.2857530e-01, -3.3990222e-01,  5.6519140e-02, -3.0626178e-01,\n",
              "          -3.8712007e-01,  2.5342917e-01,  3.0127959e-02, -3.2180676e-01,\n",
              "           8.8340277e-03,  9.1570504e-02,  3.2075951e-01, -5.2511968e-02,\n",
              "           2.9536164e-01,  3.7659075e-02, -3.0468634e-01,  6.9755703e-02,\n",
              "           3.1442019e-01,  3.6272553e-01, -1.3855128e-01,  9.5707528e-02,\n",
              "           2.0088612e-03, -2.7008840e-01, -3.3348743e-03,  3.4988815e-01,\n",
              "          -1.9229080e-01,  1.5831092e-01, -2.5065383e-01, -2.8930789e-01],\n",
              "         [ 2.0247416e-01,  2.5140390e-01,  6.3624240e-05, -2.1426679e-01,\n",
              "          -7.3863477e-02,  2.3037344e-01, -1.0352579e-01,  2.2188084e-01,\n",
              "          -1.2768020e-01,  1.4605591e-01, -5.3448040e-02,  4.0297648e-03,\n",
              "           2.8050530e-01,  3.7309319e-02,  2.5601599e-01, -2.0962557e-01,\n",
              "           1.1511839e-01, -1.7088218e-01, -9.7353600e-02,  8.7865375e-02,\n",
              "           9.4276540e-02, -6.3906886e-02, -8.9968309e-02, -2.7850112e-01,\n",
              "           5.8212234e-03, -2.1403638e-01, -1.7042586e-01, -2.0939767e-01,\n",
              "           1.6984582e-01, -1.6157743e-01, -9.2924982e-02, -3.6485568e-01],\n",
              "         [ 8.7351993e-02,  1.2292220e-01,  2.9868162e-01, -3.7069902e-02,\n",
              "           4.7803979e-02, -1.9827035e-01, -1.1468471e-01, -2.4314933e-01,\n",
              "          -4.7594931e-02, -1.8010397e-01,  2.0585185e-01,  3.7017503e-01,\n",
              "          -2.6128706e-01,  1.3946971e-01,  1.1412483e-01, -6.6973910e-02,\n",
              "          -1.8779965e-02, -1.9398466e-01,  3.3959103e-01,  6.0069576e-02,\n",
              "           1.7739156e-01, -2.9163402e-01, -1.3620429e-01,  4.0087610e-01,\n",
              "          -3.4753183e-01,  2.1383090e-01, -3.1649563e-02, -1.6596608e-01,\n",
              "           3.1914619e-01,  5.5384743e-03, -2.7242929e-01, -3.0084035e-01],\n",
              "         [-1.3743846e-01,  1.4916234e-01, -1.5628034e-01, -1.4256494e-01,\n",
              "           3.9295688e-02, -1.3871822e-01, -2.4424365e-01,  4.7020145e-02,\n",
              "          -2.9359682e-02, -1.1452573e-01, -1.3116284e-03,  8.5061520e-02,\n",
              "          -2.5616434e-01,  7.6853503e-03,  2.6505012e-03,  3.5355636e-01,\n",
              "           3.1019044e-01,  3.0438298e-01, -3.7226111e-01, -2.5517782e-02,\n",
              "          -1.3682057e-01, -4.0953174e-02, -2.2056085e-01, -3.1862533e-01,\n",
              "          -1.1207277e-02,  2.8444350e-01,  9.6071713e-02, -3.6143133e-01,\n",
              "          -8.5569032e-02, -7.7185780e-02,  1.5757555e-01,  9.9108338e-02],\n",
              "         [ 2.5360944e-02,  3.6632806e-01, -5.4978589e-03, -3.8066655e-01,\n",
              "          -2.1727139e-01,  1.6948007e-01,  1.8551652e-01, -2.6631135e-01,\n",
              "           1.9695534e-01,  7.0660159e-02,  3.5235223e-01, -1.7843333e-01,\n",
              "          -2.8388104e-01,  2.2013338e-01,  7.1789257e-02, -2.4941158e-01,\n",
              "          -2.2600284e-01, -2.6360008e-01, -5.1154882e-02,  4.1593888e-01,\n",
              "          -1.9218816e-01, -1.8670313e-01,  1.6514878e-01, -1.9390145e-01,\n",
              "          -2.0574154e-01, -8.8450052e-02,  3.4435481e-01, -1.3726653e-01,\n",
              "           1.9158354e-01,  1.8520193e-01, -1.9463640e-01, -7.1482576e-02],\n",
              "         [-1.3694683e-01, -2.9293808e-01,  4.0512040e-01, -2.8658250e-02,\n",
              "           3.2585323e-01,  4.5286166e-03,  1.0953603e-01, -2.5038841e-01,\n",
              "           1.4529721e-01, -7.0291914e-02,  1.4007684e-02, -9.5063157e-02,\n",
              "          -2.3302281e-01, -1.2377179e-01, -1.5784337e-01,  2.6758441e-01,\n",
              "          -3.3262312e-01, -1.8958136e-01, -1.6747548e-01, -2.5195503e-01,\n",
              "          -1.5340407e-01,  3.8093817e-01,  2.5613686e-01, -1.2319547e-01,\n",
              "           3.4882644e-01,  2.9537156e-01, -4.0480974e-01,  8.9933835e-02,\n",
              "          -2.0804574e-01, -1.8096177e-01, -2.1162736e-01, -7.6431207e-02]],\n",
              "        dtype=float32),\n",
              "  array([ 0.11762961,  0.11809657,  0.15746142,  0.00939981, -0.0360285 ,\n",
              "         -0.03772504, -0.05300661,  0.08091957,  0.08342663, -0.03286866,\n",
              "          0.0104744 , -0.01419182, -0.05636496, -0.0657148 , -0.03186774,\n",
              "         -0.0885051 , -0.08662306,  0.21721464, -0.03147492,  0.06051052,\n",
              "         -0.07628913,  0.00159585,  0.02035305,  0.06816845,  0.21212098,\n",
              "         -0.09839657,  0.03530865,  0.01712763,  0.05930437, -0.06825855,\n",
              "         -0.00380077,  0.05113098], dtype=float32),\n",
              "  array([[ 4.0222180e-01],\n",
              "         [ 5.6026572e-01],\n",
              "         [ 5.1814950e-01],\n",
              "         [ 1.2006413e-01],\n",
              "         [-3.0907747e-04],\n",
              "         [-1.5042630e-01],\n",
              "         [-3.1722933e-01],\n",
              "         [ 2.5178969e-01],\n",
              "         [ 3.6826983e-01],\n",
              "         [-1.8625315e-01],\n",
              "         [ 6.7859560e-02],\n",
              "         [-5.4231416e-03],\n",
              "         [-9.9803895e-02],\n",
              "         [-3.5093784e-01],\n",
              "         [-2.1608579e-01],\n",
              "         [-1.4354898e-01],\n",
              "         [-2.6436755e-01],\n",
              "         [ 5.6408358e-01],\n",
              "         [-1.0761985e-01],\n",
              "         [ 4.0207821e-01],\n",
              "         [-3.0989099e-01],\n",
              "         [ 1.0180809e-01],\n",
              "         [ 1.3381810e-01],\n",
              "         [ 2.9620790e-01],\n",
              "         [ 5.9469056e-01],\n",
              "         [-2.1401653e-01],\n",
              "         [ 2.6978302e-01],\n",
              "         [ 1.5450637e-01],\n",
              "         [ 2.8878295e-01],\n",
              "         [-2.7400634e-01],\n",
              "         [ 3.4373865e-02],\n",
              "         [ 2.1327247e-01]], dtype=float32),\n",
              "  array([0.6308396], dtype=float32)],\n",
              " [array([[-5.72489761e-02,  1.08294636e-01,  3.10806811e-01,\n",
              "          -2.65186876e-01,  3.64439756e-01, -1.05285682e-01,\n",
              "          -2.26527616e-01, -9.89651028e-03,  1.70551315e-01,\n",
              "           2.17050448e-01, -1.04454875e-01,  1.14136077e-01,\n",
              "          -3.69864047e-01, -2.28631958e-01,  2.14751258e-01,\n",
              "           3.68665695e-01, -5.20951562e-02,  4.33079451e-01,\n",
              "           2.62324721e-01,  2.28873596e-01, -1.30228415e-01,\n",
              "          -2.51782089e-01,  4.33388174e-01,  5.53752221e-02,\n",
              "           7.92398453e-02,  2.08559811e-01,  6.40623644e-02,\n",
              "          -3.48475069e-01,  3.10798228e-01, -2.72861391e-01,\n",
              "           5.89934923e-02,  1.72478646e-01],\n",
              "         [ 2.44440407e-01,  8.15405771e-02,  3.88887167e-01,\n",
              "          -1.18279934e-01,  1.54578239e-01, -3.78913701e-01,\n",
              "          -1.19965740e-01, -1.42590865e-01,  3.02842975e-01,\n",
              "          -1.63142309e-01,  2.89914608e-01, -3.23179603e-01,\n",
              "           2.08756462e-01,  2.38108754e-01, -2.21102666e-02,\n",
              "          -1.02996305e-01,  3.61924976e-01, -3.33516389e-01,\n",
              "          -3.99333276e-02,  1.53049618e-01, -3.31645280e-01,\n",
              "          -1.85515419e-01, -3.42680722e-01,  2.59786129e-01,\n",
              "          -1.70908496e-02,  3.33222926e-01, -3.61979693e-01,\n",
              "           1.78623617e-01,  5.41446395e-02,  2.21673384e-01,\n",
              "           8.34949911e-02, -4.20346595e-02],\n",
              "         [-2.20366642e-01, -1.47837758e-01, -1.24992132e-01,\n",
              "          -3.90356869e-01,  4.89421375e-02,  2.52882093e-01,\n",
              "           1.34387344e-01,  1.92833737e-01, -9.00821835e-02,\n",
              "           1.17746711e-01,  2.53433049e-01,  3.50954324e-01,\n",
              "           2.99756467e-01, -1.43267438e-02, -3.37464333e-01,\n",
              "          -3.76943856e-01, -3.91474098e-01,  2.63547182e-01,\n",
              "          -2.08290815e-01, -2.71678865e-01,  3.28127086e-01,\n",
              "          -1.30191863e-01,  6.00184612e-02, -2.05426663e-01,\n",
              "          -2.82473803e-01, -3.04846227e-01,  3.41180444e-01,\n",
              "          -3.57029945e-01,  4.84898724e-02,  1.97419554e-01,\n",
              "          -1.76191956e-01,  2.14555606e-01],\n",
              "         [ 3.32733303e-01,  2.03970745e-01,  1.96209207e-01,\n",
              "          -1.56037822e-01, -3.01698387e-01, -2.18823880e-01,\n",
              "           3.66407990e-01, -5.83744459e-02, -3.08567196e-01,\n",
              "          -1.11767054e-01, -2.34183129e-02,  2.01934308e-04,\n",
              "           1.78747788e-01, -3.72573465e-01,  1.41395852e-01,\n",
              "           1.83212399e-01,  3.39450300e-01, -1.02876067e-01,\n",
              "           4.78857532e-02,  2.59723008e-01,  1.05566539e-01,\n",
              "           1.00333549e-01, -3.10286909e-01, -3.11035275e-01,\n",
              "          -4.26256061e-01,  3.64665121e-01,  1.12797827e-01,\n",
              "          -5.44883683e-02,  6.58351034e-02, -2.47620977e-02,\n",
              "           2.30604038e-01,  3.22995484e-01],\n",
              "         [-1.93061844e-01,  3.42016041e-01, -3.56160969e-01,\n",
              "          -1.46634936e-01,  2.51432896e-01,  2.95427769e-01,\n",
              "          -3.36599320e-01,  1.23415358e-01,  2.22102568e-01,\n",
              "          -3.13451856e-01,  7.14383572e-02,  1.39234021e-01,\n",
              "          -1.25264404e-02,  3.55494469e-01, -3.15920892e-03,\n",
              "          -3.80653977e-01,  5.88526800e-02, -2.64075454e-02,\n",
              "          -1.15192741e-01,  3.22490185e-01,  1.17153667e-01,\n",
              "          -2.49146596e-01,  3.00369501e-01,  1.46053761e-01,\n",
              "           2.55072773e-01, -1.13589227e-01, -1.94307223e-01,\n",
              "           2.95017600e-01, -1.61409393e-01, -5.56737706e-02,\n",
              "           2.53493756e-01,  9.79364812e-02],\n",
              "         [ 9.71300825e-02,  3.15680742e-01,  9.31476057e-02,\n",
              "           9.26619098e-02,  3.15695733e-01,  3.82589959e-02,\n",
              "           2.63328522e-01, -4.01135147e-01,  2.04486530e-02,\n",
              "          -2.32464075e-01, -7.99805764e-03, -3.47574770e-01,\n",
              "           1.40353455e-03, -4.12785828e-01,  3.03080529e-01,\n",
              "          -3.40339363e-01,  3.74889284e-01,  2.26118147e-01,\n",
              "           2.64400870e-01,  3.70798916e-01,  4.41183755e-03,\n",
              "           1.98296890e-01, -2.16691464e-01,  1.03373863e-01,\n",
              "           1.34714350e-01,  1.38191298e-01, -2.39290491e-01,\n",
              "           1.97160363e-01, -1.60374641e-01, -2.60739863e-01,\n",
              "          -1.15575969e-01, -1.43036768e-01],\n",
              "         [ 1.79438621e-01, -2.19966948e-01, -4.21700448e-01,\n",
              "          -1.05213732e-01,  4.14375812e-01,  2.36110866e-01,\n",
              "           3.93030755e-02, -1.58838019e-01,  3.96588504e-01,\n",
              "          -2.94593796e-02, -3.38086039e-01, -2.27665350e-01,\n",
              "          -1.37056738e-01, -3.32962155e-01, -2.34390691e-01,\n",
              "           2.52479073e-02,  1.96600601e-01,  5.11364378e-02,\n",
              "          -2.44642198e-02, -1.04827899e-02, -2.38548756e-01,\n",
              "          -3.53129119e-01, -2.79811230e-02,  1.57148823e-01,\n",
              "          -1.15344383e-01,  2.40683824e-01,  1.31755367e-01,\n",
              "           1.04666807e-01, -2.59363383e-01, -6.27233041e-03,\n",
              "          -8.49221870e-02,  9.29735973e-02],\n",
              "         [ 3.44584078e-01, -2.48236001e-01,  3.37026894e-01,\n",
              "           7.89744481e-02,  1.09420054e-01, -1.38428941e-01,\n",
              "          -1.93399668e-01,  2.57270455e-01, -1.00405067e-02,\n",
              "          -4.12777007e-01, -9.77000669e-02,  2.91482925e-01,\n",
              "          -2.51584928e-02, -3.41643929e-01, -3.97528380e-01,\n",
              "           1.60691559e-01,  6.86510280e-02,  1.86419621e-01,\n",
              "          -2.15029761e-01,  3.27253640e-01, -7.21059516e-02,\n",
              "           2.36174747e-01,  3.30510646e-01,  2.01268509e-01,\n",
              "           4.43879247e-01, -5.41493706e-02,  2.56368577e-01,\n",
              "           4.09196824e-01, -2.07223505e-01, -4.02371705e-01,\n",
              "          -2.11679917e-02,  4.57735024e-02]], dtype=float32),\n",
              "  array([-0.06524173, -0.02957833,  0.06815637,  0.0649998 ,  0.06725103,\n",
              "          0.05850252, -0.05717786,  0.00800021,  0.05318734,  0.06983756,\n",
              "          0.0385358 ,  0.06375761,  0.03042216,  0.06329563,  0.0063111 ,\n",
              "          0.00591231,  0.06633437,  0.06355349, -0.02566862,  0.06352335,\n",
              "          0.04637654,  0.06053516,  0.06264418, -0.04121407,  0.06435768,\n",
              "          0.01460266,  0.06182454,  0.02666286,  0.07052157,  0.06157435,\n",
              "         -0.06115944, -0.05131267], dtype=float32),\n",
              "  array([[ 0.01584973,  0.07125384, -0.1372954 , ..., -0.1912992 ,\n",
              "          -0.1934049 , -0.12881263],\n",
              "         [-0.1629185 , -0.02414343,  0.07126261, ...,  0.18323264,\n",
              "          -0.27908602, -0.13248882],\n",
              "         [ 0.10777723,  0.32852405,  0.3393375 , ..., -0.14660269,\n",
              "          -0.06280758,  0.31255132],\n",
              "         ...,\n",
              "         [-0.13068998, -0.23082314,  0.3558647 , ..., -0.06029122,\n",
              "          -0.01290902, -0.01317535],\n",
              "         [-0.09465131, -0.17140703, -0.19611032, ..., -0.1695188 ,\n",
              "           0.00729687,  0.1194267 ],\n",
              "         [ 0.11522659, -0.2566211 ,  0.00188417, ...,  0.19407684,\n",
              "           0.3067551 ,  0.23902464]], dtype=float32),\n",
              "  array([-0.04401602,  0.06911719,  0.06920452,  0.06981742,  0.07076595,\n",
              "          0.06887168,  0.05292461,  0.02909978,  0.07107945, -0.05843548,\n",
              "          0.0692255 ,  0.06506819,  0.06762224,  0.05454547,  0.06946137,\n",
              "         -0.0645928 ,  0.06586774, -0.02803404, -0.06155248, -0.04174969,\n",
              "         -0.03546794,  0.05617933,  0.06967749, -0.03645668, -0.03206605,\n",
              "         -0.05498991, -0.06096563,  0.06967261, -0.0494736 , -0.03667621,\n",
              "          0.07050382,  0.06965946], dtype=float32),\n",
              "  array([[-0.00345856],\n",
              "         [ 0.42285186],\n",
              "         [ 0.45751038],\n",
              "         [ 0.40113047],\n",
              "         [ 0.08026852],\n",
              "         [ 0.06317899],\n",
              "         [ 0.10294113],\n",
              "         [ 0.0044827 ],\n",
              "         [ 0.10164809],\n",
              "         [-0.05697958],\n",
              "         [ 0.09859182],\n",
              "         [ 0.42939106],\n",
              "         [ 0.28561345],\n",
              "         [ 0.09322624],\n",
              "         [ 0.08282262],\n",
              "         [-0.02237269],\n",
              "         [ 0.12422187],\n",
              "         [-0.19420822],\n",
              "         [-0.03633665],\n",
              "         [-0.27448484],\n",
              "         [-0.01937428],\n",
              "         [ 0.44674128],\n",
              "         [ 0.33210847],\n",
              "         [-0.3312047 ],\n",
              "         [-0.18765034],\n",
              "         [-0.40527508],\n",
              "         [-0.26971352],\n",
              "         [ 0.12125345],\n",
              "         [-0.34252045],\n",
              "         [-0.1674276 ],\n",
              "         [ 0.26444218],\n",
              "         [ 0.14288655]], dtype=float32),\n",
              "  array([0.06878497], dtype=float32)],\n",
              " [array([[ 0.10485155,  0.30637115,  0.15070526,  0.00449118,  0.01766267,\n",
              "           0.2921551 , -0.41950834,  0.15862413,  0.3645884 ,  0.3033856 ,\n",
              "          -0.13809755, -0.12487958,  0.40928203, -0.23418108,  0.08482286,\n",
              "          -0.3205953 , -0.24835823, -0.12652987,  0.14823173,  0.34243774,\n",
              "          -0.32832345, -0.21870609, -0.40350133, -0.07968063, -0.18540923,\n",
              "          -0.42982423,  0.252495  ,  0.12148999,  0.03046483,  0.3937686 ,\n",
              "           0.3304913 , -0.4078259 ],\n",
              "         [-0.11074041,  0.26814973, -0.01435398,  0.26531708, -0.20761533,\n",
              "           0.13514803, -0.4075668 ,  0.03704982, -0.01780789,  0.23733674,\n",
              "           0.13438927,  0.01148145,  0.08378419, -0.05327921,  0.21236485,\n",
              "          -0.07576928,  0.06975451, -0.26306805, -0.13484378,  0.22753012,\n",
              "          -0.28173816,  0.19128934, -0.04683229,  0.28357074, -0.09286704,\n",
              "           0.25710955, -0.02232638, -0.12966634,  0.19351894,  0.05060922,\n",
              "          -0.18704848,  0.2720019 ],\n",
              "         [-0.03231724,  0.01395908, -0.03978375, -0.11379688,  0.16606572,\n",
              "           0.3658834 , -0.13670905, -0.22677629,  0.23703606, -0.3771184 ,\n",
              "          -0.18031289, -0.13014494, -0.00050672,  0.04338794, -0.10091252,\n",
              "           0.16212004, -0.21717507,  0.00685283, -0.12003452, -0.2888906 ,\n",
              "           0.16226594,  0.17585449,  0.28067493, -0.1887785 ,  0.2793259 ,\n",
              "          -0.40684038,  0.09408682, -0.3289235 ,  0.2831981 , -0.03090411,\n",
              "          -0.12112676, -0.3955947 ],\n",
              "         [ 0.07402711, -0.38507178,  0.01622777, -0.22008955,  0.1473339 ,\n",
              "           0.00648898,  0.08743669,  0.07270183, -0.10030761, -0.32580155,\n",
              "          -0.17835584,  0.304595  , -0.38430807, -0.3726235 , -0.23564474,\n",
              "          -0.03098787,  0.00045412, -0.12042249,  0.16027096,  0.17882404,\n",
              "           0.08807868,  0.2820489 , -0.10074249,  0.0913572 , -0.08381528,\n",
              "           0.08646952,  0.32291135, -0.3066698 ,  0.34145665, -0.31292716,\n",
              "          -0.07017176,  0.11829277],\n",
              "         [-0.16377379,  0.09819133, -0.20868915, -0.1022617 , -0.24551763,\n",
              "           0.24973808, -0.22318465,  0.19621918, -0.07605916, -0.20313686,\n",
              "           0.02969909,  0.1257416 ,  0.31586036,  0.21567905, -0.30019757,\n",
              "          -0.2033134 ,  0.18152101, -0.21299279,  0.00655282,  0.24308312,\n",
              "          -0.14472131,  0.39790547, -0.03218876, -0.13454011, -0.23495284,\n",
              "          -0.3796121 , -0.23017292,  0.3156731 ,  0.33372808,  0.2839595 ,\n",
              "          -0.12575573,  0.28524634],\n",
              "         [-0.3554798 , -0.31136283, -0.15056166, -0.2343662 , -0.3725616 ,\n",
              "          -0.13989179,  0.01357831,  0.17215025,  0.14951913, -0.11324514,\n",
              "          -0.01895624,  0.22049023,  0.3006784 ,  0.08019576,  0.2935289 ,\n",
              "          -0.03251   , -0.4192092 , -0.2631607 ,  0.20717885, -0.18834475,\n",
              "           0.35335144, -0.15034997, -0.32618245, -0.03375221,  0.00352322,\n",
              "           0.19872871,  0.14274682, -0.40310723,  0.32728478,  0.09095863,\n",
              "          -0.24214   ,  0.28338674],\n",
              "         [-0.1827046 , -0.24850723,  0.19319664, -0.34626257,  0.3054648 ,\n",
              "           0.03906547,  0.2963101 , -0.06076793,  0.25266424,  0.1067877 ,\n",
              "           0.0531575 ,  0.3038254 , -0.30824587,  0.00430763,  0.10613489,\n",
              "           0.37143642, -0.21563749,  0.23377109,  0.22093305,  0.34269792,\n",
              "          -0.25567275,  0.23241371,  0.00656902, -0.05857278, -0.19852251,\n",
              "           0.16408077,  0.01976013, -0.06175606,  0.28246793,  0.2056084 ,\n",
              "           0.11845869, -0.2741499 ],\n",
              "         [-0.39173502,  0.07739607,  0.18388982, -0.02889417,  0.2847233 ,\n",
              "          -0.13293588, -0.04426057,  0.18126288,  0.17342307,  0.17933616,\n",
              "          -0.3377779 , -0.26880017, -0.01845425,  0.13399366,  0.23572068,\n",
              "           0.1629366 , -0.06839988, -0.12441814, -0.36301082, -0.31036803,\n",
              "          -0.2913256 ,  0.1477662 , -0.14427437,  0.16271469,  0.19838884,\n",
              "           0.4218399 ,  0.31096616, -0.30044612, -0.34253928,  0.2840626 ,\n",
              "           0.09705959, -0.16819577]], dtype=float32),\n",
              "  array([ 0.07576784,  0.07568283, -0.07518736,  0.07751925,  0.07646566,\n",
              "          0.07555033,  0.0755324 , -0.07556642,  0.07610506,  0.07663872,\n",
              "         -0.07518991,  0.0754942 ,  0.07564507,  0.07565274,  0.07764718,\n",
              "         -0.07528029,  0.07720409, -0.07520769,  0.07550672,  0.07541329,\n",
              "         -0.07500084, -0.07542261,  0.07592056, -0.07521359,  0.07611685,\n",
              "          0.07553794, -0.07451834,  0.07590909,  0.07565736,  0.0756316 ,\n",
              "         -0.07540867,  0.07543258], dtype=float32),\n",
              "  array([[ 0.46607998],\n",
              "         [ 0.47083163],\n",
              "         [-0.0562459 ],\n",
              "         [ 0.08790901],\n",
              "         [ 0.12289893],\n",
              "         [ 0.3832488 ],\n",
              "         [ 0.46092343],\n",
              "         [-0.1537075 ],\n",
              "         [ 0.15244852],\n",
              "         [ 0.13138717],\n",
              "         [-0.1544081 ],\n",
              "         [ 0.16882397],\n",
              "         [ 0.4172654 ],\n",
              "         [ 0.24812992],\n",
              "         [ 0.08360118],\n",
              "         [-0.372899  ],\n",
              "         [ 0.06217004],\n",
              "         [-0.20312412],\n",
              "         [ 0.2896832 ],\n",
              "         [ 0.27904087],\n",
              "         [-0.02481536],\n",
              "         [-0.32757983],\n",
              "         [ 0.19416039],\n",
              "         [-0.33208722],\n",
              "         [ 0.21450253],\n",
              "         [ 0.4627204 ],\n",
              "         [-0.03929901],\n",
              "         [ 0.11069022],\n",
              "         [ 0.05448274],\n",
              "         [ 0.35924304],\n",
              "         [-0.38168478],\n",
              "         [ 0.3735017 ]], dtype=float32),\n",
              "  array([0.07548916], dtype=float32)],\n",
              " [array([[ 0.18430369, -0.21312852,  0.0205369 , -0.20610617,  0.03300546,\n",
              "          -0.09729877,  0.2069559 ,  0.2195617 ,  0.18562461, -0.01535008,\n",
              "           0.06590037,  0.21801937,  0.15618287, -0.21948573,  0.05254496,\n",
              "           0.15389396, -0.05880333,  0.05609967,  0.20489334, -0.09700301,\n",
              "          -0.04427481, -0.11203042, -0.09906153,  0.12414259,  0.09732042,\n",
              "          -0.25932363, -0.29999292,  0.30077443,  0.1576651 ,  0.10023364,\n",
              "          -0.25561202, -0.270157  , -0.0539331 , -0.13450705, -0.26747045,\n",
              "           0.2995011 , -0.21299058,  0.1520878 , -0.10635792,  0.2595683 ,\n",
              "          -0.1452714 , -0.05734409, -0.05011392, -0.13049404,  0.03315936,\n",
              "          -0.22923061, -0.17105074, -0.11674108, -0.14182632, -0.19293597,\n",
              "           0.12348344,  0.11256026,  0.07401551, -0.05376403, -0.12453459,\n",
              "          -0.04962724, -0.21993327,  0.03262588,  0.08147274,  0.09852148,\n",
              "           0.08089779,  0.21668209, -0.22545001,  0.22383596],\n",
              "         [ 0.11856236, -0.16234969, -0.19951363, -0.20643817,  0.12522535,\n",
              "           0.08194915,  0.05790482,  0.00610889, -0.06028358, -0.25561222,\n",
              "          -0.09653906, -0.24637644,  0.11973616, -0.26900196, -0.2665353 ,\n",
              "          -0.08370958,  0.13627239, -0.20339741,  0.2868752 ,  0.1853402 ,\n",
              "           0.08502933,  0.25962168,  0.23425402, -0.17410013, -0.2581143 ,\n",
              "           0.01997838, -0.06076314, -0.16465399, -0.2461129 , -0.2168303 ,\n",
              "           0.10758866, -0.22345236,  0.04096887,  0.2759626 , -0.22431126,\n",
              "          -0.2403662 ,  0.17666079, -0.13252828, -0.23839405, -0.24378264,\n",
              "           0.13036923, -0.16218989,  0.0869412 ,  0.16147108, -0.12119346,\n",
              "           0.23063774,  0.22044893,  0.2566663 ,  0.2556186 ,  0.2079154 ,\n",
              "          -0.21031938,  0.19377232,  0.21145958,  0.30892715, -0.17460433,\n",
              "           0.1781839 , -0.03451588,  0.12883848,  0.14442864, -0.07323223,\n",
              "           0.13546926,  0.10142414,  0.21355271, -0.00484476],\n",
              "         [-0.09909917, -0.19443865,  0.12764567, -0.001775  , -0.04565802,\n",
              "           0.24245302,  0.0949644 ,  0.16953844,  0.2643406 , -0.160618  ,\n",
              "           0.270958  , -0.18616244, -0.2755275 ,  0.04529647,  0.15630442,\n",
              "           0.13262726, -0.24781014, -0.02097899, -0.22184175, -0.11574315,\n",
              "          -0.03075445,  0.13572226, -0.04155016,  0.2160522 , -0.12159216,\n",
              "          -0.06032557,  0.15446645, -0.13567704,  0.0418318 , -0.18308838,\n",
              "           0.07178698, -0.26272297,  0.190362  ,  0.2703297 , -0.06146259,\n",
              "           0.16658565,  0.12122957,  0.08938029,  0.02623713, -0.26628798,\n",
              "          -0.08621864,  0.21972428, -0.08245019,  0.11456899, -0.18098496,\n",
              "          -0.09844152,  0.01741842, -0.04435737,  0.11880631,  0.13748883,\n",
              "          -0.03773324, -0.28632718, -0.10121525,  0.06947348,  0.01091636,\n",
              "          -0.24182041, -0.12809394, -0.07282717, -0.06523509,  0.18453714,\n",
              "           0.2210366 ,  0.1191398 ,  0.20737256, -0.25175202],\n",
              "         [-0.11869267,  0.14965495, -0.08846524,  0.06940031,  0.06235096,\n",
              "           0.27079323, -0.11995991, -0.04975761,  0.10078406,  0.2095816 ,\n",
              "           0.28260365,  0.20187873,  0.04204326,  0.12882419,  0.1922825 ,\n",
              "          -0.19914407,  0.24343127, -0.25445166, -0.2345347 , -0.18878178,\n",
              "          -0.24927554,  0.02116408,  0.07380152,  0.20129326,  0.13486685,\n",
              "          -0.13235202,  0.12397081,  0.16995753,  0.1866334 ,  0.19454305,\n",
              "           0.00949972, -0.18817829,  0.2076866 , -0.06045915, -0.19254227,\n",
              "          -0.19948432,  0.17206138, -0.27169114,  0.18459669, -0.2074035 ,\n",
              "           0.28766406, -0.14752893,  0.26883096, -0.06962245,  0.26220298,\n",
              "           0.04718242, -0.07919754,  0.05903972,  0.13098   ,  0.2571265 ,\n",
              "          -0.16739374, -0.26349056,  0.05877125,  0.16140829,  0.00438962,\n",
              "          -0.16398537, -0.24849537, -0.00106287,  0.05217314,  0.03425243,\n",
              "           0.20979668,  0.07610276, -0.01620444, -0.15836936],\n",
              "         [-0.27902526, -0.03387156, -0.19196478,  0.27327764,  0.06934123,\n",
              "          -0.16727506, -0.09589415,  0.13801728, -0.02336729, -0.1320481 ,\n",
              "          -0.05043516,  0.09273728,  0.03498854,  0.187415  , -0.18606211,\n",
              "           0.17740417, -0.0339887 , -0.23999299,  0.2681639 , -0.03911055,\n",
              "          -0.00995398, -0.10289831,  0.2168158 ,  0.17342737,  0.20554978,\n",
              "           0.1314753 , -0.13440283,  0.04358938, -0.10745151,  0.19049455,\n",
              "          -0.26505947,  0.05138467, -0.08204139, -0.12057374,  0.21620789,\n",
              "          -0.10910942,  0.00221592, -0.16450615, -0.0886317 ,  0.28668314,\n",
              "           0.02605313, -0.27413994, -0.23335999,  0.23427193, -0.14376591,\n",
              "          -0.04543262,  0.15057696,  0.05120381, -0.20125607, -0.0246183 ,\n",
              "          -0.26048097,  0.19477352, -0.22159456,  0.2514398 , -0.22055753,\n",
              "           0.09076632,  0.25111663,  0.24347314, -0.22365083, -0.21387197,\n",
              "           0.12217246,  0.1295471 ,  0.04795181, -0.24740967],\n",
              "         [-0.27126744,  0.19449645,  0.18709356,  0.12564638,  0.08654552,\n",
              "          -0.18907963, -0.2832792 ,  0.04483699,  0.12880276, -0.17780863,\n",
              "           0.1753006 ,  0.21294042, -0.19352806,  0.02806914,  0.18833439,\n",
              "           0.05189582,  0.08052284,  0.01836941, -0.12124123,  0.05489436,\n",
              "           0.08039482,  0.2781587 , -0.0558387 , -0.27652773, -0.14916213,\n",
              "          -0.05968626, -0.03846462,  0.18870182, -0.13786182, -0.14781635,\n",
              "          -0.02032879, -0.13844717,  0.17085716, -0.2684245 ,  0.16142015,\n",
              "           0.10165501, -0.2215132 , -0.02656302, -0.23144025,  0.27934635,\n",
              "           0.08817216,  0.21453688,  0.17494145,  0.08609222,  0.21246357,\n",
              "          -0.01038399,  0.03592314,  0.1730558 , -0.22999871,  0.1687915 ,\n",
              "           0.14954367,  0.20590189, -0.04481088,  0.09275166, -0.01153165,\n",
              "           0.13421896,  0.26073098, -0.21904531, -0.07106324,  0.07380665,\n",
              "           0.04367859,  0.10734184, -0.21489686,  0.25745878],\n",
              "         [ 0.27726316,  0.15311581, -0.04932926,  0.09849257, -0.21963063,\n",
              "          -0.20506968, -0.24362186, -0.1965236 , -0.13296606, -0.22615588,\n",
              "           0.12910576,  0.06927308,  0.23066272,  0.11267968,  0.00949109,\n",
              "          -0.13699444,  0.0404162 ,  0.0694691 ,  0.28066632,  0.03723449,\n",
              "          -0.25106546,  0.11200099,  0.1090835 , -0.02240739, -0.03897882,\n",
              "           0.00098044,  0.02538062, -0.02994882, -0.2162246 , -0.10695457,\n",
              "           0.06011279,  0.05105682, -0.15570839, -0.00780926, -0.07021379,\n",
              "          -0.24372685, -0.12604436, -0.14444661,  0.18221247, -0.00276252,\n",
              "          -0.23227781,  0.04314924,  0.2709138 , -0.1835278 , -0.22170177,\n",
              "           0.11392404,  0.04327893, -0.11723892,  0.15010491,  0.02928465,\n",
              "          -0.07247397, -0.0877412 , -0.27210048, -0.01657754, -0.2503859 ,\n",
              "          -0.2602915 , -0.27289698,  0.03548227, -0.01079364,  0.18498777,\n",
              "          -0.15797573,  0.08371365,  0.13459831,  0.26065144],\n",
              "         [-0.06395821, -0.08267998,  0.19883588, -0.05147845, -0.25841573,\n",
              "           0.00698716,  0.06854393, -0.1164627 , -0.0078441 , -0.25185618,\n",
              "          -0.01890842, -0.06919101, -0.24760845, -0.07489767, -0.11727452,\n",
              "          -0.19545089,  0.26093668,  0.02123187,  0.10612921, -0.03855883,\n",
              "           0.23860754, -0.02444198,  0.05563146,  0.00748408,  0.19363096,\n",
              "          -0.2394461 ,  0.27469718,  0.16512355, -0.20007636,  0.09731501,\n",
              "          -0.19078098, -0.15489365,  0.06591077,  0.07718428,  0.29823327,\n",
              "           0.17177087,  0.26823226,  0.03188073,  0.27313626,  0.02406429,\n",
              "          -0.18049458, -0.02522277, -0.04277137,  0.16490364,  0.20049813,\n",
              "           0.14097574,  0.26464212, -0.0969783 ,  0.02939431, -0.12778859,\n",
              "           0.0485426 , -0.04923061, -0.06711611,  0.02216601, -0.0600151 ,\n",
              "          -0.08959282,  0.17661476,  0.18994187,  0.00427128,  0.15988846,\n",
              "          -0.09099487,  0.28241256,  0.04817615,  0.13176806]],\n",
              "        dtype=float32),\n",
              "  array([ 0.03750629, -0.03756231, -0.03737741, -0.0376009 ,  0.03770165,\n",
              "          0.0375513 ,  0.03779297, -0.03724055, -0.03729471, -0.03721772,\n",
              "          0.03747565,  0.03752208, -0.03724834, -0.03770519, -0.03734928,\n",
              "          0.03760092, -0.0380632 , -0.03735029,  0.03765584,  0.03753832,\n",
              "         -0.03734185, -0.03736185, -0.03806419,  0.03764585, -0.0374792 ,\n",
              "         -0.03789484, -0.03768257,  0.03787948, -0.03732321,  0.03751667,\n",
              "          0.03781174, -0.03757358, -0.037356  ,  0.03755307,  0.03747424,\n",
              "          0.03745523,  0.03752775, -0.03729676,  0.0375544 , -0.03707254,\n",
              "          0.03760796,  0.03755296, -0.03733961, -0.03751611, -0.0373802 ,\n",
              "         -0.03751541, -0.03761135,  0.03752036, -0.03734022, -0.0374043 ,\n",
              "         -0.03622213,  0.0375198 , -0.03726992,  0.03750388,  0.03761162,\n",
              "         -0.03741572,  0.03736417, -0.03766175,  0.03773113, -0.03735058,\n",
              "         -0.03731086, -0.03695139,  0.03759719,  0.03754581], dtype=float32),\n",
              "  array([[ 0.23303063],\n",
              "         [-0.3145354 ],\n",
              "         [-0.16135822],\n",
              "         [-0.27242765],\n",
              "         [ 0.12290214],\n",
              "         [ 0.11757534],\n",
              "         [ 0.08246367],\n",
              "         [-0.1311647 ],\n",
              "         [-0.16333221],\n",
              "         [-0.02480153],\n",
              "         [ 0.31375542],\n",
              "         [ 0.2943433 ],\n",
              "         [-0.1617741 ],\n",
              "         [-0.0639597 ],\n",
              "         [-0.22147778],\n",
              "         [ 0.15617263],\n",
              "         [-0.02261855],\n",
              "         [-0.17292118],\n",
              "         [ 0.10623697],\n",
              "         [ 0.24528316],\n",
              "         [-0.11262505],\n",
              "         [-0.23834392],\n",
              "         [-0.03090151],\n",
              "         [ 0.06871799],\n",
              "         [-0.06652394],\n",
              "         [-0.03527406],\n",
              "         [-0.0426202 ],\n",
              "         [ 0.05873108],\n",
              "         [-0.21831533],\n",
              "         [ 0.2141578 ],\n",
              "         [ 0.05436912],\n",
              "         [-0.25947908],\n",
              "         [-0.06780037],\n",
              "         [ 0.16789891],\n",
              "         [ 0.15000743],\n",
              "         [ 0.31994674],\n",
              "         [ 0.09590869],\n",
              "         [-0.17202483],\n",
              "         [ 0.27062508],\n",
              "         [-0.02168689],\n",
              "         [ 0.06465106],\n",
              "         [ 0.25327614],\n",
              "         [-0.20918448],\n",
              "         [-0.14332727],\n",
              "         [-0.21177484],\n",
              "         [-0.24184273],\n",
              "         [-0.14030308],\n",
              "         [ 0.3038307 ],\n",
              "         [-0.2282767 ],\n",
              "         [-0.13276231],\n",
              "         [-0.00738355],\n",
              "         [ 0.30916464],\n",
              "         [-0.21945709],\n",
              "         [ 0.25375894],\n",
              "         [ 0.09515899],\n",
              "         [-0.19721638],\n",
              "         [ 0.04315445],\n",
              "         [-0.06880174],\n",
              "         [ 0.11192728],\n",
              "         [-0.28733695],\n",
              "         [-0.11791373],\n",
              "         [-0.02361781],\n",
              "         [ 0.0331495 ],\n",
              "         [ 0.24908474]], dtype=float32),\n",
              "  array([0.03748212], dtype=float32)],\n",
              " [array([[ 0.00312249, -0.06727092, -0.05042467, -0.01876886, -0.08498878,\n",
              "          -0.10417923, -0.17696044, -0.1434766 ,  0.08364756,  0.35617217,\n",
              "          -0.15431364, -0.16557854, -0.342151  ,  0.28740418,  0.2215087 ,\n",
              "           0.10312011, -0.22747478,  0.3944634 ,  0.09714439,  0.31656274,\n",
              "          -0.21057045, -0.0548152 ,  0.23157701, -0.01446073, -0.19858319,\n",
              "           0.13689038, -0.2267967 ,  0.21709917, -0.01099973, -0.04266853,\n",
              "           0.1308372 , -0.3402962 ],\n",
              "         [ 0.0104707 , -0.02600241,  0.08765965, -0.20860311, -0.11411414,\n",
              "           0.04104419,  0.12201264,  0.05285176,  0.16304749, -0.361445  ,\n",
              "          -0.21182604, -0.08793189,  0.06021405,  0.31801134, -0.2704933 ,\n",
              "           0.01294167, -0.17384149,  0.13147755,  0.10179403,  0.02321598,\n",
              "          -0.37110758, -0.2507644 ,  0.31450263, -0.3598582 , -0.17180072,\n",
              "           0.32522708,  0.26193506,  0.1764755 ,  0.3016543 , -0.14649247,\n",
              "          -0.36200964, -0.21922928],\n",
              "         [ 0.00610592,  0.32330513,  0.30651608,  0.31466302,  0.07553204,\n",
              "           0.12422447,  0.23439397, -0.22755906, -0.16823195,  0.21368006,\n",
              "           0.07576255, -0.16745405,  0.34758624, -0.28092438,  0.11443729,\n",
              "          -0.0013524 , -0.35318685,  0.25263503, -0.08826968, -0.07425889,\n",
              "          -0.28595626,  0.08779529, -0.25924873, -0.25734228, -0.2555496 ,\n",
              "          -0.08061269, -0.03816077,  0.3298055 , -0.36549765, -0.03067422,\n",
              "           0.30356073, -0.37563726],\n",
              "         [-0.10398351, -0.23321241,  0.3550671 , -0.23330921, -0.19108361,\n",
              "           0.31708306, -0.27935594,  0.11084018, -0.19419983, -0.11426062,\n",
              "          -0.298484  ,  0.1944095 , -0.33870444, -0.15063389, -0.1473105 ,\n",
              "           0.06183428,  0.10066734, -0.11836799, -0.1827702 ,  0.21441399,\n",
              "           0.03591244,  0.14283407, -0.28850582, -0.20822755,  0.30796966,\n",
              "           0.11398377,  0.05923506,  0.03498606,  0.26829815,  0.25638494,\n",
              "           0.21510008,  0.31273708],\n",
              "         [-0.07812987,  0.20098795,  0.28051007, -0.3067527 , -0.08863974,\n",
              "           0.13810048, -0.30514464, -0.25255254,  0.07805137,  0.39053747,\n",
              "          -0.33432734,  0.06324118,  0.3354231 ,  0.2016971 ,  0.09581815,\n",
              "          -0.32680377, -0.3294481 ,  0.06802397,  0.3693877 , -0.15024693,\n",
              "          -0.156319  ,  0.18848933,  0.2985657 ,  0.3494598 ,  0.312775  ,\n",
              "           0.23553221,  0.36548585, -0.12901038,  0.2020663 , -0.10306996,\n",
              "          -0.07241382,  0.12311794],\n",
              "         [ 0.37867635, -0.12352739, -0.00941695,  0.2953919 , -0.1558218 ,\n",
              "          -0.17298508, -0.02435762, -0.10432777,  0.19691099, -0.08884441,\n",
              "          -0.32456997,  0.26394928, -0.03523949, -0.29679722, -0.26779473,\n",
              "          -0.09180031, -0.23609056, -0.3502221 ,  0.12663479,  0.09568497,\n",
              "           0.01064285,  0.12152123, -0.05168358,  0.22015424,  0.04801875,\n",
              "           0.02429623, -0.00459318, -0.13949382,  0.32134563,  0.3237773 ,\n",
              "           0.08556058,  0.19163829],\n",
              "         [-0.01512129, -0.03870116, -0.02418825, -0.14950608,  0.33781135,\n",
              "           0.18577324, -0.01529268, -0.23953469,  0.0970623 ,  0.34945235,\n",
              "          -0.27759302,  0.20871183,  0.22084713,  0.10651408, -0.03361789,\n",
              "          -0.35983673,  0.01658312, -0.3761599 ,  0.32099327, -0.17859523,\n",
              "           0.11332643,  0.08668239,  0.0830833 ,  0.10441732,  0.38532135,\n",
              "           0.3003755 ,  0.13411789,  0.31190303,  0.20132002,  0.30442706,\n",
              "          -0.2757885 ,  0.07973597],\n",
              "         [-0.13557242, -0.15662384,  0.25085205, -0.12773435,  0.18763818,\n",
              "          -0.04013362,  0.16792606, -0.12652007, -0.13191007,  0.09583611,\n",
              "           0.34051558, -0.11797221, -0.17029588,  0.38740164, -0.3115048 ,\n",
              "          -0.22084777,  0.37653345,  0.09268253, -0.07094914,  0.29854164,\n",
              "           0.22414337,  0.13892035,  0.04808813,  0.3450993 ,  0.17764322,\n",
              "           0.15949494,  0.09447958, -0.24280654, -0.2213455 ,  0.28763348,\n",
              "          -0.1595884 , -0.19811341]], dtype=float32),\n",
              "  array([-0.12291141,  0.02905244,  0.01399857, -0.02930456,  0.05133047,\n",
              "          0.05723789, -0.05006708, -0.08297299, -0.03276797,  0.00061773,\n",
              "          0.02169303, -0.07498502, -0.00411415, -0.06377412, -0.00396598,\n",
              "          0.14595158,  0.09484982,  0.08855009,  0.05235236, -0.00827769,\n",
              "         -0.01157086, -0.05315439, -0.14201544,  0.10255432,  0.01604922,\n",
              "          0.04730358,  0.03752303,  0.07193186, -0.04169203, -0.05455063,\n",
              "         -0.01380472, -0.04280932], dtype=float32),\n",
              "  array([[ 0.2095269 , -0.09680679,  0.24595754, ...,  0.19027232,\n",
              "          -0.26163277, -0.05306889],\n",
              "         [-0.27221057,  0.09910932, -0.13629481, ..., -0.2873186 ,\n",
              "          -0.10981324,  0.01618239],\n",
              "         [-0.23686396, -0.18173067, -0.03530337, ..., -0.24874465,\n",
              "           0.25363553,  0.11305205],\n",
              "         ...,\n",
              "         [ 0.06684534, -0.2950202 ,  0.14279927, ..., -0.05834935,\n",
              "          -0.1292361 , -0.3080588 ],\n",
              "         [ 0.29682466,  0.2017383 ,  0.20111984, ..., -0.22768629,\n",
              "          -0.08677258, -0.17093754],\n",
              "         [-0.19561677,  0.15684612,  0.05046197, ..., -0.22353576,\n",
              "          -0.01934013,  0.15871261]], dtype=float32),\n",
              "  array([ 0.10426577,  0.07469928, -0.09342747, -0.03762667,  0.01086855,\n",
              "          0.07266044,  0.08310408, -0.05183282, -0.03616625,  0.07230125,\n",
              "          0.10720345, -0.06198521,  0.02877436, -0.03956012, -0.07269394,\n",
              "         -0.07795159, -0.06231549,  0.04559745,  0.07721629,  0.09340781,\n",
              "          0.02521827, -0.08067482, -0.07220116, -0.09323992,  0.02862902,\n",
              "         -0.02761921,  0.03186396,  0.00525023,  0.08987434, -0.05565884,\n",
              "          0.01664388,  0.06820793], dtype=float32),\n",
              "  array([[ 0.4556447 ],\n",
              "         [ 0.32286644],\n",
              "         [-0.4580672 ],\n",
              "         [-0.14518373],\n",
              "         [ 0.05717159],\n",
              "         [ 0.3118085 ],\n",
              "         [ 0.33927992],\n",
              "         [-0.19091158],\n",
              "         [-0.15755111],\n",
              "         [ 0.32330063],\n",
              "         [ 0.43885323],\n",
              "         [-0.2914369 ],\n",
              "         [ 0.11607635],\n",
              "         [-0.19016474],\n",
              "         [-0.3176283 ],\n",
              "         [-0.34324512],\n",
              "         [-0.30849066],\n",
              "         [ 0.20110795],\n",
              "         [ 0.34048718],\n",
              "         [ 0.3804453 ],\n",
              "         [ 0.11642626],\n",
              "         [-0.32893965],\n",
              "         [-0.34070072],\n",
              "         [-0.41529578],\n",
              "         [ 0.11261266],\n",
              "         [-0.14107968],\n",
              "         [ 0.12618631],\n",
              "         [ 0.0256693 ],\n",
              "         [ 0.36901668],\n",
              "         [-0.23115692],\n",
              "         [ 0.09393532],\n",
              "         [ 0.30194312]], dtype=float32),\n",
              "  array([0.26985237], dtype=float32)],\n",
              " [array([[ 0.26127306,  0.00557072, -0.06137431,  0.11971931, -0.27801514,\n",
              "          -0.20832664, -0.17854497,  0.24190332,  0.10592867, -0.01336069,\n",
              "          -0.04100751,  0.07488036, -0.05700675,  0.18001069, -0.28471854,\n",
              "           0.14502999, -0.05810488, -0.12145543,  0.1939922 , -0.1441307 ,\n",
              "          -0.16804838, -0.1976389 , -0.15520412,  0.30186194, -0.16742209,\n",
              "          -0.01890456,  0.06171221,  0.27006823, -0.12876555, -0.25279346,\n",
              "          -0.09237612,  0.02696248, -0.21826307, -0.12560783, -0.12530622,\n",
              "          -0.04855107,  0.0967937 ,  0.07376751, -0.07431109,  0.22152875,\n",
              "          -0.178421  ,  0.04655421,  0.08959362, -0.19533491, -0.23483545,\n",
              "          -0.06952623, -0.20888607,  0.08457413, -0.28004682, -0.08883688,\n",
              "           0.13010794,  0.19740032,  0.23147756,  0.01049541,  0.14592227,\n",
              "          -0.04481221,  0.09247887,  0.18442696,  0.10882423,  0.21285374,\n",
              "           0.19472517,  0.08062956,  0.25235108,  0.16746797],\n",
              "         [ 0.03032351,  0.07803772,  0.20404592,  0.23043334, -0.18188716,\n",
              "           0.00933236,  0.1637675 ,  0.12511052, -0.22669572,  0.09445823,\n",
              "          -0.2634927 ,  0.12673998, -0.06071642,  0.0622446 ,  0.20092003,\n",
              "          -0.08358826,  0.278812  ,  0.08743826, -0.13096738, -0.02250171,\n",
              "          -0.20897135,  0.2541004 , -0.11389796, -0.01692872, -0.15659328,\n",
              "           0.08575808, -0.16987318,  0.26025334, -0.240277  ,  0.22589843,\n",
              "          -0.14446498, -0.05136478,  0.20991221,  0.26929256,  0.14373238,\n",
              "           0.21517567,  0.04857576, -0.18377502, -0.16211027,  0.06454759,\n",
              "          -0.08130856,  0.01640091, -0.21078774,  0.17892437,  0.28377852,\n",
              "           0.06914044,  0.10453684, -0.2029208 , -0.19857828,  0.24716866,\n",
              "          -0.2613606 ,  0.06212439, -0.10440843,  0.19628716, -0.1394663 ,\n",
              "          -0.11764842, -0.1387342 , -0.28752518,  0.13331279, -0.27911818,\n",
              "          -0.13909511, -0.2243144 , -0.23613533, -0.2552644 ],\n",
              "         [-0.15475228,  0.19885881, -0.08697293,  0.12683979, -0.0943551 ,\n",
              "           0.24628451,  0.11375327,  0.13043252, -0.06203824,  0.00945898,\n",
              "          -0.00134812,  0.0503088 , -0.05745732,  0.27281445,  0.06931708,\n",
              "           0.25471145, -0.15123002,  0.2684554 , -0.19706137, -0.07353176,\n",
              "          -0.26117525,  0.06639536,  0.15282214, -0.24306962,  0.2158773 ,\n",
              "          -0.25120652, -0.20597804,  0.15271483,  0.04268317,  0.23202729,\n",
              "          -0.17299277,  0.2674236 ,  0.17117527,  0.1885214 , -0.1059392 ,\n",
              "          -0.25056937,  0.1818464 , -0.21837452, -0.05495269, -0.21615522,\n",
              "          -0.0306389 ,  0.25738287,  0.12256896,  0.12254717, -0.09739193,\n",
              "          -0.11064655,  0.23747581,  0.22099692, -0.17323254,  0.12223624,\n",
              "           0.0229546 ,  0.10605663,  0.19614874,  0.1929382 , -0.21184352,\n",
              "           0.12722445, -0.26956683,  0.26329836, -0.11937731,  0.0127769 ,\n",
              "          -0.28093725,  0.14495027, -0.14330947, -0.1613083 ],\n",
              "         [-0.07201147, -0.2566661 ,  0.24854812, -0.26134643,  0.00674525,\n",
              "           0.15332209,  0.10630406,  0.09920695, -0.10720245,  0.13373588,\n",
              "           0.22870995,  0.20513259, -0.04879536,  0.2730274 ,  0.03701471,\n",
              "          -0.18122269, -0.21521881,  0.08866218,  0.00979697, -0.19044499,\n",
              "           0.00582986, -0.23803546, -0.2895139 ,  0.12111382,  0.2519241 ,\n",
              "          -0.24691318, -0.17077896, -0.23273055,  0.04882163, -0.18247221,\n",
              "          -0.12823163,  0.10218509,  0.24101053, -0.16327983, -0.2813602 ,\n",
              "           0.021959  ,  0.07532085,  0.01250418,  0.00386755, -0.04058061,\n",
              "          -0.02645171,  0.00422475,  0.14209649, -0.06587524,  0.1739224 ,\n",
              "           0.00871103,  0.25646424, -0.24279262,  0.0054875 , -0.1961798 ,\n",
              "           0.06807104,  0.13168776, -0.02628257, -0.03593615,  0.12301359,\n",
              "           0.12469563,  0.18813652, -0.19114603, -0.17478597, -0.10576702,\n",
              "          -0.08052466,  0.13208249, -0.21397129,  0.07818974],\n",
              "         [ 0.21778981, -0.01804416,  0.23535533,  0.06862547, -0.28807697,\n",
              "          -0.10078546,  0.25062555,  0.24794939,  0.22362362,  0.18888737,\n",
              "          -0.02303589,  0.03461275, -0.10228888,  0.19686821,  0.19624749,\n",
              "           0.03484381,  0.21777096,  0.25973958, -0.14567807, -0.2520751 ,\n",
              "          -0.27961096, -0.28789514,  0.144835  ,  0.17287412, -0.0372119 ,\n",
              "           0.17627873, -0.12429761, -0.12729688, -0.03960912,  0.20777063,\n",
              "           0.24557346,  0.25464484, -0.1268596 , -0.21111903,  0.16115737,\n",
              "           0.05892605, -0.16082405, -0.08517417, -0.18553   , -0.03100974,\n",
              "          -0.19440249, -0.04840541,  0.11774042, -0.2874609 , -0.11919197,\n",
              "           0.15105338, -0.19927536, -0.20370422,  0.0996318 ,  0.05071655,\n",
              "           0.05063566, -0.12195984, -0.18540868, -0.29179475,  0.1640067 ,\n",
              "          -0.20610045, -0.22920561,  0.00984327,  0.0052334 , -0.15640621,\n",
              "           0.22781953, -0.05133303,  0.2557647 ,  0.08327027],\n",
              "         [-0.09971327, -0.15682554,  0.11734073, -0.15570262, -0.15081334,\n",
              "           0.25380227,  0.2675839 , -0.06819288,  0.21112369,  0.16172192,\n",
              "           0.03841284, -0.25875   ,  0.10797101, -0.20944569,  0.18848354,\n",
              "           0.12678558, -0.09897412,  0.13196525, -0.00723303,  0.15381484,\n",
              "           0.05218865, -0.04929859, -0.11598389,  0.17736407,  0.25010225,\n",
              "          -0.12781279,  0.19081609, -0.07872475, -0.25664905,  0.22277696,\n",
              "           0.10487736, -0.18273239, -0.23408288, -0.01532218,  0.06950305,\n",
              "          -0.1253341 ,  0.26148272,  0.0355198 , -0.30469006, -0.08486532,\n",
              "          -0.17373028, -0.24822074,  0.04267887, -0.2465204 , -0.07064559,\n",
              "           0.11173366,  0.23179029,  0.2278183 ,  0.01124584, -0.13102455,\n",
              "           0.17409797, -0.04200288,  0.22990313,  0.21454604,  0.2843237 ,\n",
              "          -0.24799849, -0.21254978, -0.15093657,  0.14596723,  0.18569648,\n",
              "           0.20814922, -0.21988335,  0.23219758, -0.16237782],\n",
              "         [ 0.14722806,  0.18211237, -0.24236363, -0.02280978,  0.2751238 ,\n",
              "          -0.18384695, -0.14684644,  0.26739585,  0.13436034, -0.29870167,\n",
              "          -0.2391004 , -0.13824506, -0.03076055, -0.17917538, -0.2717532 ,\n",
              "           0.0841397 ,  0.28049263,  0.19687684,  0.05077725, -0.28246784,\n",
              "           0.22946225, -0.0844804 ,  0.02420156, -0.14896788,  0.28286952,\n",
              "           0.18050937, -0.19165339,  0.2687409 , -0.21328829, -0.10491978,\n",
              "          -0.02290933,  0.11691907, -0.22266644, -0.03802294, -0.12321503,\n",
              "           0.15728997,  0.19040167, -0.06875485, -0.10729256, -0.21771856,\n",
              "           0.27551156, -0.07791369,  0.13730861,  0.18436527, -0.15525784,\n",
              "          -0.22106363,  0.14388831,  0.210726  ,  0.28751904, -0.1416719 ,\n",
              "           0.00717069,  0.27456123,  0.11275099,  0.20298144,  0.12403368,\n",
              "          -0.1190673 , -0.15958352,  0.20962577,  0.25944066, -0.00127773,\n",
              "          -0.10772557, -0.05668694,  0.17975459,  0.16893524],\n",
              "         [-0.02927931, -0.15015958,  0.21816671, -0.14239442, -0.30242437,\n",
              "          -0.2635217 ,  0.26668307, -0.07090461, -0.18025923,  0.08625077,\n",
              "           0.07005347, -0.09776958, -0.10395131,  0.28462014,  0.09258123,\n",
              "           0.28522816,  0.10816204, -0.23318715,  0.01955474,  0.17720495,\n",
              "          -0.2843881 ,  0.05771829, -0.20582727,  0.16825058,  0.0858485 ,\n",
              "           0.11003746,  0.05856355,  0.10914644, -0.13558216, -0.28092977,\n",
              "          -0.1395649 , -0.05390007,  0.2379164 , -0.20060788,  0.00235032,\n",
              "           0.11507206,  0.06860401, -0.11692522,  0.05860458,  0.10737166,\n",
              "           0.10314313, -0.11489467, -0.09655507, -0.2350439 ,  0.21816029,\n",
              "          -0.0474763 , -0.13376278, -0.14601658,  0.04785442,  0.09170398,\n",
              "           0.07996856,  0.14115374, -0.04566476,  0.08156212, -0.0797497 ,\n",
              "           0.2522247 , -0.2352986 ,  0.2733761 , -0.30261806, -0.1836334 ,\n",
              "          -0.06092031, -0.10128453, -0.00330887, -0.07708891]],\n",
              "        dtype=float32),\n",
              "  array([ 0.03049528, -0.0307015 ,  0.0302384 ,  0.03039779,  0.03131486,\n",
              "         -0.00126859, -0.03018254, -0.01414548, -0.02982854,  0.00329397,\n",
              "          0.01658507,  0.03025511, -0.02920288,  0.03111557,  0.02981627,\n",
              "          0.02901031,  0.03008983, -0.01885087,  0.03038563,  0.03105358,\n",
              "          0.03242586,  0.02101847,  0.03207519,  0.03127468, -0.02987024,\n",
              "         -0.02887535,  0.02344691,  0.03031495, -0.02958011,  0.03173291,\n",
              "         -0.02223957, -0.03057361, -0.03139457,  0.03047499,  0.03111129,\n",
              "         -0.03049146,  0.02875421,  0.01833915,  0.02753639,  0.03107278,\n",
              "         -0.03059479,  0.02972769, -0.03005685,  0.02889815, -0.03122075,\n",
              "         -0.02750739,  0.02933569, -0.029109  ,  0.01094129, -0.02699132,\n",
              "          0.02567447,  0.02849327,  0.02940642,  0.03009311, -0.03007498,\n",
              "          0.03095836,  0.02088088,  0.02914806, -0.02347719,  0.0321404 ,\n",
              "          0.03168468, -0.02993973,  0.03115232, -0.02700437], dtype=float32),\n",
              "  array([[ 0.13251446,  0.09185331, -0.20493157, ...,  0.19857095,\n",
              "          -0.1338637 , -0.11288174],\n",
              "         [ 0.20682341, -0.07180255, -0.18901896, ..., -0.14085278,\n",
              "           0.14627664, -0.1137869 ],\n",
              "         [-0.1490109 ,  0.14383282, -0.03589861, ...,  0.05778164,\n",
              "           0.02608473, -0.15560938],\n",
              "         ...,\n",
              "         [-0.12118837, -0.17010918, -0.01742822, ...,  0.21111788,\n",
              "           0.16566266,  0.02549407],\n",
              "         [ 0.15250513, -0.11274686, -0.16761918, ..., -0.17483361,\n",
              "           0.00377748, -0.02995978],\n",
              "         [ 0.2081914 ,  0.2078389 , -0.03175533, ...,  0.15175113,\n",
              "          -0.22769883, -0.02771189]], dtype=float32),\n",
              "  array([ 0.02888514,  0.03222665, -0.02992537, -0.02949781,  0.03191369,\n",
              "         -0.03116683,  0.03162224,  0.02948662,  0.03244744, -0.02973179,\n",
              "          0.02586539,  0.03059649, -0.03068805,  0.02996113, -0.02968503,\n",
              "          0.03025809,  0.02967992,  0.02978846, -0.03016853, -0.03132638,\n",
              "          0.02935144,  0.03049851,  0.02996344,  0.00304449, -0.03004422,\n",
              "          0.00156618, -0.0295323 ,  0.02952895, -0.0322587 , -0.03030439,\n",
              "         -0.03017561,  0.02977617,  0.02991513,  0.03019795, -0.03091153,\n",
              "          0.02997569, -0.03076948, -0.02914334, -0.02895597,  0.03024129,\n",
              "          0.03010458,  0.0297425 , -0.0305227 , -0.02919356,  0.028597  ,\n",
              "          0.02189397,  0.03201636,  0.02994641,  0.03143461,  0.0301445 ,\n",
              "          0.0313913 , -0.02983668,  0.0298604 ,  0.02963692, -0.02993937,\n",
              "         -0.02858061,  0.02899773,  0.03000356,  0.03041767,  0.01073054,\n",
              "          0.02946148, -0.01755921, -0.02898638,  0.02991525], dtype=float32),\n",
              "  array([[ 0.0340804 , -0.06147519, -0.06311768, ..., -0.09890457,\n",
              "           0.06552881,  0.09652381],\n",
              "         [ 0.00380041, -0.06053582,  0.06121497, ..., -0.06813943,\n",
              "           0.08097037, -0.11948296],\n",
              "         [ 0.02084654, -0.06769007, -0.09001057, ..., -0.1025344 ,\n",
              "          -0.20637694, -0.18110904],\n",
              "         ...,\n",
              "         [-0.03070746, -0.04793793,  0.1016079 , ...,  0.0753651 ,\n",
              "           0.05890483, -0.00440797],\n",
              "         [ 0.19723451, -0.0664338 , -0.11124885, ..., -0.13392827,\n",
              "           0.17429781, -0.03630995],\n",
              "         [ 0.00695403,  0.21269938,  0.02662162, ..., -0.12181107,\n",
              "           0.1657195 ,  0.10370532]], dtype=float32),\n",
              "  array([ 0.02942959,  0.02934797, -0.02971916, -0.02959348,  0.02920787,\n",
              "         -0.02927217,  0.02925348,  0.02942626, -0.02948057,  0.02998735,\n",
              "         -0.02981459,  0.02960276,  0.02957162,  0.0295057 ,  0.02946202,\n",
              "         -0.02886935,  0.02924773, -0.02934588,  0.02948844,  0.02925071,\n",
              "          0.02910553,  0.02944701, -0.029146  , -0.02946948, -0.02910925,\n",
              "         -0.02897169, -0.03013436,  0.0288655 , -0.02914641, -0.02888071,\n",
              "         -0.02925467,  0.02910071,  0.02939057,  0.02955433, -0.0296409 ,\n",
              "         -0.03163741,  0.02965304, -0.02939597, -0.0296669 ,  0.02900651,\n",
              "          0.02925986, -0.02958944, -0.02947688,  0.02930067, -0.02913199,\n",
              "         -0.02926015, -0.02943445, -0.02912448, -0.02943696,  0.02938099,\n",
              "         -0.02927618,  0.02956899, -0.02891514,  0.02972503,  0.02924741,\n",
              "          0.02868934,  0.02953696,  0.02936447,  0.02942969,  0.02979361,\n",
              "          0.02905009,  0.02941317,  0.01723565,  0.02929309], dtype=float32),\n",
              "  array([[ 0.13844807],\n",
              "         [ 0.17118338],\n",
              "         [-0.23229462],\n",
              "         [-0.06069069],\n",
              "         [ 0.1444242 ],\n",
              "         [-0.07214686],\n",
              "         [ 0.19028455],\n",
              "         [ 0.2578997 ],\n",
              "         [-0.23105301],\n",
              "         [ 0.02238278],\n",
              "         [-0.24120885],\n",
              "         [ 0.1569489 ],\n",
              "         [ 0.31771815],\n",
              "         [ 0.2004547 ],\n",
              "         [ 0.20488133],\n",
              "         [-0.1896658 ],\n",
              "         [ 0.27370575],\n",
              "         [-0.15660764],\n",
              "         [ 0.14291078],\n",
              "         [ 0.25425577],\n",
              "         [ 0.20246376],\n",
              "         [ 0.3036432 ],\n",
              "         [-0.09808417],\n",
              "         [-0.13432138],\n",
              "         [-0.18100896],\n",
              "         [-0.30195546],\n",
              "         [-0.31302214],\n",
              "         [ 0.13633195],\n",
              "         [-0.06509893],\n",
              "         [-0.22960655],\n",
              "         [-0.2369526 ],\n",
              "         [ 0.19555424],\n",
              "         [ 0.1731198 ],\n",
              "         [ 0.20995739],\n",
              "         [-0.19749776],\n",
              "         [-0.01663086],\n",
              "         [ 0.22850434],\n",
              "         [-0.20324573],\n",
              "         [-0.23052676],\n",
              "         [ 0.22531377],\n",
              "         [ 0.2280817 ],\n",
              "         [-0.08310866],\n",
              "         [-0.16542847],\n",
              "         [ 0.0528125 ],\n",
              "         [-0.22205801],\n",
              "         [-0.07436135],\n",
              "         [-0.22428168],\n",
              "         [-0.08862858],\n",
              "         [-0.21105734],\n",
              "         [ 0.27377942],\n",
              "         [-0.2347968 ],\n",
              "         [ 0.1672391 ],\n",
              "         [-0.15096173],\n",
              "         [ 0.22606133],\n",
              "         [ 0.32042903],\n",
              "         [ 0.28875726],\n",
              "         [ 0.12994386],\n",
              "         [ 0.3149384 ],\n",
              "         [ 0.21677601],\n",
              "         [ 0.03938821],\n",
              "         [ 0.2597841 ],\n",
              "         [ 0.18930657],\n",
              "         [ 0.01445657],\n",
              "         [ 0.10883359]], dtype=float32),\n",
              "  array([0.02916643], dtype=float32)],\n",
              " [array([[-0.27098697, -0.02135372, -0.16595505,  0.28220996, -0.03405692,\n",
              "           0.2200248 ,  0.24974646,  0.03298233, -0.00458999,  0.16160938,\n",
              "           0.04528642, -0.14477181, -0.05713924, -0.21740173,  0.25726256,\n",
              "           0.20052287, -0.18411627, -0.07200637, -0.10445233, -0.14231196,\n",
              "          -0.10682627,  0.22942834,  0.0478829 ,  0.19469024,  0.24831629,\n",
              "           0.17851803, -0.0586774 , -0.15785785, -0.22805275, -0.24982871,\n",
              "           0.06980857,  0.29721215, -0.0150334 ,  0.10988148, -0.2616986 ,\n",
              "           0.09010027,  0.09051308, -0.14152025, -0.25585634,  0.30747652,\n",
              "           0.12163728, -0.10234336,  0.05747079,  0.13647492, -0.23443595,\n",
              "          -0.027691  , -0.06085227,  0.1675218 , -0.05909467,  0.04509069,\n",
              "           0.18367836, -0.21632482, -0.25369486, -0.19916461, -0.00446021,\n",
              "          -0.07786956, -0.299509  , -0.08687972, -0.08047085,  0.10910997,\n",
              "           0.08867379,  0.20812787, -0.0849178 , -0.03735697],\n",
              "         [-0.27565387, -0.16736212,  0.00525285, -0.26584706,  0.15271148,\n",
              "          -0.21517837,  0.2562413 , -0.22021773,  0.0614768 ,  0.0013051 ,\n",
              "          -0.00638118, -0.07109837,  0.17085479, -0.00068845,  0.2774187 ,\n",
              "           0.16916199, -0.18323763, -0.18275884,  0.26111826, -0.28515103,\n",
              "           0.25910944, -0.269727  ,  0.21007113, -0.28439644,  0.15071078,\n",
              "           0.03038715,  0.10245175,  0.13348134, -0.15087652, -0.1662226 ,\n",
              "          -0.0354344 , -0.09822395,  0.1267366 , -0.05665971,  0.25690457,\n",
              "           0.18601099, -0.06630972,  0.0674018 ,  0.08531488,  0.17167667,\n",
              "          -0.3101399 , -0.09007934,  0.25370005,  0.14585003,  0.11146328,\n",
              "          -0.02099842, -0.2445821 , -0.04095678, -0.18152316, -0.08433385,\n",
              "           0.24076511,  0.18523787, -0.2616489 , -0.03898491, -0.2725389 ,\n",
              "          -0.04860592, -0.17049539,  0.26923004,  0.25110376, -0.06563095,\n",
              "           0.2599838 ,  0.26270023,  0.07061263,  0.12912521],\n",
              "         [-0.173115  ,  0.19646066,  0.25006154, -0.01135376, -0.20332555,\n",
              "          -0.0284036 ,  0.25388926,  0.02795305,  0.19967115, -0.16390105,\n",
              "           0.24087338, -0.09517891, -0.1349379 , -0.28518072, -0.2338121 ,\n",
              "          -0.20930694, -0.18954375, -0.26805738,  0.15834546, -0.27466837,\n",
              "          -0.11997651, -0.24389607,  0.13654631,  0.18220219,  0.07160447,\n",
              "          -0.01398496, -0.18965052, -0.29108447, -0.03450641,  0.10274216,\n",
              "           0.17911562, -0.13315262,  0.10090996,  0.23285776, -0.14438505,\n",
              "          -0.09128158,  0.10010314,  0.27055496, -0.00504227, -0.1516659 ,\n",
              "          -0.11611363,  0.02332685, -0.08538684,  0.00252324,  0.07276709,\n",
              "          -0.20011534, -0.08941176,  0.18977816,  0.15703267,  0.25943407,\n",
              "           0.23417439, -0.1742459 , -0.23224756,  0.20285131, -0.17505175,\n",
              "          -0.0865382 , -0.03022599, -0.04094397, -0.2261023 ,  0.18507099,\n",
              "           0.14474826,  0.23304884, -0.01222208, -0.26818332],\n",
              "         [-0.2231013 ,  0.02983519,  0.27900615, -0.2868299 , -0.14425834,\n",
              "          -0.10821467, -0.24960142,  0.060888  ,  0.15493025,  0.17690234,\n",
              "           0.10267977,  0.03240115,  0.02671346, -0.10053455, -0.13553947,\n",
              "          -0.01705473, -0.29182813, -0.29546607,  0.04792298, -0.2707526 ,\n",
              "          -0.06215309,  0.10344055, -0.17291771, -0.1610641 , -0.03724369,\n",
              "           0.27803168, -0.2133992 , -0.16008182, -0.24257651, -0.03028354,\n",
              "          -0.09387121,  0.12545337, -0.1979845 ,  0.2825269 , -0.04205659,\n",
              "          -0.19366838,  0.21683231, -0.0284479 , -0.08994648, -0.00068415,\n",
              "           0.04301655, -0.04099402,  0.07823028, -0.00755537, -0.26619625,\n",
              "          -0.05026263,  0.09936755, -0.17211552,  0.14987397,  0.2886116 ,\n",
              "          -0.06717416, -0.16639851, -0.20091823,  0.24180107, -0.23142615,\n",
              "           0.15338871,  0.17591816,  0.1764473 ,  0.20232573, -0.09687704,\n",
              "          -0.11154334,  0.22790667, -0.25971285, -0.09729213],\n",
              "         [-0.2084457 ,  0.05085857,  0.24051018, -0.2585328 ,  0.17125677,\n",
              "           0.15783621, -0.07616158,  0.02172946, -0.16674984, -0.1021219 ,\n",
              "          -0.25407496, -0.22304863, -0.06462417, -0.20299783,  0.18539406,\n",
              "          -0.31039432,  0.01719654, -0.23943588,  0.09092268, -0.02932731,\n",
              "           0.10331372,  0.2717581 ,  0.24808498,  0.09058791, -0.0665886 ,\n",
              "           0.05606087,  0.17904498, -0.10525098, -0.16132957, -0.10445792,\n",
              "          -0.2732504 ,  0.12166093,  0.22696272, -0.07724413,  0.00311986,\n",
              "          -0.18177457,  0.2147273 ,  0.03178456, -0.25239557, -0.24728669,\n",
              "           0.00241887,  0.04952267, -0.25951052,  0.23963004,  0.17622256,\n",
              "           0.13359389,  0.13506152, -0.2952443 ,  0.07023376,  0.22626707,\n",
              "           0.01568969,  0.1762076 ,  0.21606013,  0.13149713,  0.14541706,\n",
              "           0.05957753, -0.2338735 ,  0.28273052,  0.2551834 ,  0.2325273 ,\n",
              "           0.27440116,  0.20183985, -0.16177064, -0.1203244 ],\n",
              "         [-0.1618329 , -0.01985634,  0.21439007,  0.16920653, -0.12989445,\n",
              "          -0.18668033, -0.27109492,  0.12743361,  0.01227096,  0.18214215,\n",
              "          -0.04809754,  0.16752367, -0.30789012, -0.2832221 , -0.15232557,\n",
              "           0.25130436,  0.00254344, -0.3030816 ,  0.09331147,  0.26728195,\n",
              "          -0.09260485,  0.21466781, -0.1068283 ,  0.1308185 , -0.16136838,\n",
              "           0.16991372, -0.24474622,  0.23195657, -0.18342718, -0.30039072,\n",
              "          -0.06332561, -0.07618091, -0.23693801,  0.21655434, -0.14230631,\n",
              "           0.22826819,  0.17401034, -0.10962477, -0.22751077, -0.31473336,\n",
              "           0.13974868,  0.13247037, -0.18341231, -0.15426694, -0.22958335,\n",
              "           0.02186964, -0.21596532, -0.3022043 , -0.08798215,  0.21376568,\n",
              "           0.10914692,  0.1270201 ,  0.05533954, -0.2179737 , -0.13478847,\n",
              "          -0.05570101, -0.05522195,  0.04057295,  0.27721795, -0.29821557,\n",
              "           0.19988585, -0.31668872,  0.09874769,  0.18567657],\n",
              "         [ 0.02918108,  0.1210155 , -0.18324076,  0.06639872,  0.01680786,\n",
              "           0.21240026,  0.02444675,  0.03586632, -0.10803904,  0.18889742,\n",
              "          -0.25171927,  0.23796044, -0.20372394, -0.08049639, -0.26733887,\n",
              "           0.19300364,  0.1299081 ,  0.15436783,  0.1592926 ,  0.07874639,\n",
              "          -0.02180345,  0.22930017, -0.19393423,  0.08998871, -0.272732  ,\n",
              "          -0.29910246, -0.32138222,  0.08180971, -0.00266902, -0.04470694,\n",
              "          -0.21512952,  0.04350668,  0.17264488,  0.2431503 , -0.09002934,\n",
              "          -0.2838423 ,  0.17091605, -0.17227387, -0.06644601,  0.10299201,\n",
              "           0.13785529,  0.15632933,  0.278837  ,  0.1252901 ,  0.23830795,\n",
              "          -0.1975865 , -0.2151492 ,  0.1899328 ,  0.14890268, -0.10817345,\n",
              "           0.18632601,  0.04312176,  0.19007404, -0.23852739,  0.27244562,\n",
              "           0.2602883 ,  0.2742341 ,  0.28189096, -0.21533501,  0.01978021,\n",
              "           0.26116526, -0.1241053 , -0.12156563,  0.1224291 ],\n",
              "         [ 0.1582112 , -0.06829312, -0.18698859, -0.06882384, -0.16938716,\n",
              "           0.11175369,  0.0118866 , -0.18041515,  0.27721682, -0.08205146,\n",
              "          -0.21685615, -0.09455492,  0.04955805,  0.10197706, -0.24556848,\n",
              "           0.219167  ,  0.0320565 , -0.14782104, -0.00309739, -0.26035562,\n",
              "           0.14522743, -0.17021333,  0.2005628 ,  0.28087503,  0.27525175,\n",
              "           0.08025369,  0.28574362,  0.11124971,  0.05161943, -0.192357  ,\n",
              "           0.25371283,  0.17924978, -0.29836255, -0.11175475, -0.07010831,\n",
              "          -0.30294034,  0.27388713, -0.24034408, -0.02355436, -0.15868905,\n",
              "          -0.15094358, -0.2254211 , -0.24484731,  0.05834245, -0.01719871,\n",
              "           0.30555168,  0.11583865, -0.01347303,  0.17139332, -0.07668436,\n",
              "          -0.12568012,  0.2162153 ,  0.2631287 , -0.27245837, -0.16709782,\n",
              "           0.07337202,  0.0590217 ,  0.1765559 , -0.26442307, -0.10441101,\n",
              "          -0.22992049,  0.05928969, -0.27292782, -0.13131808]],\n",
              "        dtype=float32),\n",
              "  array([ 1.8634953e-05, -2.8536074e-02, -3.3822410e-02,  2.3114000e-02,\n",
              "         -2.2053139e-02,  4.8951220e-02,  2.3470975e-02,  3.0477682e-02,\n",
              "          4.6327110e-02,  3.8398918e-02,  4.7432296e-02,  3.0963542e-02,\n",
              "          2.9307283e-02,  3.1341460e-02,  5.3489264e-02,  3.8612414e-02,\n",
              "         -1.2120045e-02,  3.5968803e-02,  1.1193710e-02, -6.0596652e-03,\n",
              "          1.9795248e-02,  4.3804418e-02,  4.1447289e-02,  4.5833334e-02,\n",
              "          4.3594342e-02,  2.8270373e-02,  4.0753748e-02,  1.5656071e-02,\n",
              "         -1.9962477e-02,  2.8150806e-02,  4.6406105e-02,  5.2060984e-02,\n",
              "          2.3958879e-02,  4.2432941e-02, -2.3415107e-02,  4.5394551e-02,\n",
              "          2.6938790e-02,  2.1559468e-02, -4.2645666e-03,  5.3197294e-02,\n",
              "          4.4197366e-02,  2.4588214e-02,  3.7832230e-02,  1.9100422e-02,\n",
              "         -5.9242724e-03,  3.7575498e-02,  3.4828950e-02,  4.9175128e-02,\n",
              "         -2.2000818e-02, -3.8827628e-02,  9.2773829e-03, -2.0690029e-02,\n",
              "          4.6120035e-03, -8.4092245e-03,  3.0812167e-02,  2.0919733e-02,\n",
              "         -5.1542313e-04,  1.3448499e-03,  3.8286638e-02,  4.3715898e-02,\n",
              "          1.8806018e-02,  3.3557381e-02,  4.3593604e-02, -5.7632866e-04],\n",
              "        dtype=float32),\n",
              "  array([[-0.16417132,  0.10766391,  0.03798382, ..., -0.20416106,\n",
              "          -0.14141512, -0.04689198],\n",
              "         [ 0.19436827,  0.1005336 , -0.19194809, ..., -0.04312556,\n",
              "           0.13248205,  0.08056044],\n",
              "         [-0.06815738, -0.03778936,  0.1459902 , ...,  0.19240369,\n",
              "           0.08228114,  0.17216465],\n",
              "         ...,\n",
              "         [ 0.03208863, -0.04055948,  0.0754089 , ...,  0.16896932,\n",
              "          -0.08320086,  0.00029098],\n",
              "         [-0.14792842, -0.12030482, -0.13491744, ...,  0.12210106,\n",
              "          -0.19603936,  0.11073866],\n",
              "         [-0.00615343, -0.01511365, -0.17743047, ..., -0.11574403,\n",
              "           0.19599563, -0.03688943]], dtype=float32),\n",
              "  array([ 0.04938491, -0.02358577,  0.02902942,  0.02237714,  0.05183919,\n",
              "         -0.01865267, -0.01668253,  0.04021609,  0.0244686 ,  0.04575968,\n",
              "         -0.01509804,  0.03802427, -0.01002563,  0.0494707 ,  0.05151369,\n",
              "          0.04964823, -0.03058887, -0.01943439,  0.03685779,  0.04573317,\n",
              "          0.01572727, -0.02562834,  0.04374263,  0.00016703,  0.00784617,\n",
              "         -0.00396769, -0.00347127,  0.03680572,  0.0411684 ,  0.04709841,\n",
              "          0.0387109 ,  0.00107172,  0.03805222, -0.0351277 ,  0.04894257,\n",
              "         -0.00754825,  0.01438281, -0.00923273,  0.04308346, -0.02574938,\n",
              "          0.04779143,  0.05236494,  0.04735754,  0.0224411 ,  0.04893978,\n",
              "         -0.02931774,  0.00094479,  0.04014788, -0.00822372,  0.04891862,\n",
              "          0.0004815 , -0.00590677,  0.03683569, -0.03035651,  0.03769676,\n",
              "          0.04864589,  0.01221647, -0.01501033,  0.04657185,  0.04454807,\n",
              "          0.04816672, -0.01695006,  0.04162842, -0.00732806], dtype=float32),\n",
              "  array([[ 0.13140424,  0.10266386,  0.21779111, ..., -0.10952099,\n",
              "           0.15035447,  0.2228125 ],\n",
              "         [-0.14563122,  0.03625696, -0.0828132 , ...,  0.14120524,\n",
              "           0.07459114, -0.16343285],\n",
              "         [-0.17926167,  0.00175025, -0.06651103, ...,  0.2035312 ,\n",
              "          -0.15699798, -0.17447528],\n",
              "         ...,\n",
              "         [-0.08360655, -0.06833815, -0.11465073, ..., -0.09661881,\n",
              "           0.16439463, -0.06632371],\n",
              "         [ 0.07334086, -0.21365774,  0.22173823, ...,  0.19211797,\n",
              "           0.17308754,  0.1751764 ],\n",
              "         [-0.10294535, -0.21215828,  0.03529555, ..., -0.07509135,\n",
              "           0.18482196,  0.21283327]], dtype=float32),\n",
              "  array([ 0.04660238, -0.02595873,  0.04876675, -0.00189614, -0.00358375,\n",
              "          0.045869  ,  0.04650211,  0.04250629, -0.00224971,  0.04681075,\n",
              "         -0.03634706,  0.0239211 , -0.03247923, -0.02269473, -0.01668403,\n",
              "          0.04533238, -0.03355142,  0.04729839, -0.00073355, -0.0063    ,\n",
              "          0.04731088, -0.0144214 , -0.00826511, -0.01280475,  0.04672255,\n",
              "          0.04581785,  0.00192906, -0.01332797,  0.05005694,  0.03627884,\n",
              "         -0.02598048, -0.02478866, -0.02241567,  0.04860069,  0.04716916,\n",
              "         -0.01077325,  0.04982125, -0.01808821, -0.02039509, -0.00169548,\n",
              "         -0.0073469 , -0.00883946, -0.00326275,  0.01922533, -0.00520499,\n",
              "         -0.01453678, -0.0448069 ,  0.04609423, -0.0045102 ,  0.04615512,\n",
              "          0.04662929, -0.02760683, -0.02029747, -0.00178368, -0.01456381,\n",
              "          0.04564765,  0.04569339,  0.04651696,  0.04678875,  0.04562671,\n",
              "          0.05033671, -0.0184758 , -0.03566878,  0.04559649], dtype=float32),\n",
              "  array([[ 0.28618324],\n",
              "         [-0.3032561 ],\n",
              "         [ 0.01913995],\n",
              "         [-0.21836838],\n",
              "         [-0.10031045],\n",
              "         [ 0.23258796],\n",
              "         [ 0.10818952],\n",
              "         [ 0.02107697],\n",
              "         [-0.00866542],\n",
              "         [ 0.2676923 ],\n",
              "         [-0.04972288],\n",
              "         [ 0.2571611 ],\n",
              "         [-0.10346013],\n",
              "         [-0.00604645],\n",
              "         [-0.04596222],\n",
              "         [ 0.2785555 ],\n",
              "         [-0.16391507],\n",
              "         [ 0.05791854],\n",
              "         [-0.18425651],\n",
              "         [-0.23960866],\n",
              "         [ 0.05442396],\n",
              "         [-0.2748493 ],\n",
              "         [-0.13793987],\n",
              "         [-0.27931625],\n",
              "         [ 0.07701782],\n",
              "         [ 0.13459784],\n",
              "         [ 0.14913338],\n",
              "         [-0.24044992],\n",
              "         [ 0.16084887],\n",
              "         [ 0.21109466],\n",
              "         [-0.1933411 ],\n",
              "         [-0.14881521],\n",
              "         [-0.16259187],\n",
              "         [ 0.30030438],\n",
              "         [ 0.09846226],\n",
              "         [-0.00508451],\n",
              "         [ 0.13730465],\n",
              "         [-0.22880013],\n",
              "         [-0.0249087 ],\n",
              "         [-0.0366743 ],\n",
              "         [-0.02177854],\n",
              "         [-0.01878596],\n",
              "         [-0.23765953],\n",
              "         [ 0.11018622],\n",
              "         [-0.1237735 ],\n",
              "         [-0.1489105 ],\n",
              "         [-0.08821511],\n",
              "         [ 0.12404618],\n",
              "         [-0.22874294],\n",
              "         [ 0.0900239 ],\n",
              "         [ 0.21618214],\n",
              "         [-0.0502758 ],\n",
              "         [-0.14311855],\n",
              "         [-0.20404541],\n",
              "         [-0.28838262],\n",
              "         [ 0.28345296],\n",
              "         [ 0.10682713],\n",
              "         [ 0.02827116],\n",
              "         [ 0.06192498],\n",
              "         [ 0.1628283 ],\n",
              "         [ 0.32482013],\n",
              "         [-0.15138182],\n",
              "         [-0.22550155],\n",
              "         [ 0.30263323]], dtype=float32),\n",
              "  array([0.04513079], dtype=float32)],\n",
              " [array([[ 0.13477743,  0.01927296,  0.30103526, -0.17613317,  0.19723003,\n",
              "          -0.05834021,  0.12017734,  0.1582341 ,  0.10950894, -0.25545362,\n",
              "           0.2077407 , -0.2054591 , -0.09754833,  0.23168114,  0.00384299,\n",
              "          -0.12204178,  0.17895655,  0.20369682,  0.24819624,  0.22164993,\n",
              "           0.02176621,  0.03482598,  0.12482941,  0.05477362, -0.16180335,\n",
              "           0.0127171 , -0.01760166, -0.24183665, -0.03695179,  0.01139771,\n",
              "          -0.2353429 , -0.13998593,  0.25265574, -0.2559407 , -0.26377246,\n",
              "          -0.22947115, -0.05914643, -0.1865519 ,  0.2523695 ,  0.20759025,\n",
              "          -0.01632236, -0.04345165,  0.11598262,  0.27566475, -0.22536571,\n",
              "           0.2509547 ,  0.0145317 , -0.01132212,  0.24806158,  0.00650346,\n",
              "           0.16608343, -0.01139794, -0.07643613,  0.06716616, -0.22074969,\n",
              "           0.12108651, -0.08539978, -0.1403619 ,  0.11692479,  0.25425324,\n",
              "          -0.14645623,  0.0403696 ,  0.20797895,  0.05576045],\n",
              "         [-0.08037411,  0.06099986,  0.06078337, -0.05477229,  0.23654564,\n",
              "          -0.04851034,  0.26987508,  0.10567886, -0.29937917, -0.02474713,\n",
              "          -0.05454271, -0.00901447, -0.19311538, -0.10775694, -0.20616738,\n",
              "          -0.06251971,  0.06176256, -0.17126733,  0.06538754, -0.00548518,\n",
              "           0.05703862, -0.23594512, -0.298339  , -0.10153373,  0.07485137,\n",
              "           0.09021648,  0.25073618,  0.1296106 , -0.2575328 ,  0.12221339,\n",
              "          -0.29671046,  0.14151505,  0.0533028 , -0.15352954,  0.20807534,\n",
              "           0.25755292,  0.04059325, -0.11866689, -0.27842346,  0.15363711,\n",
              "           0.29440758,  0.10347721,  0.20221801,  0.1534283 ,  0.03108327,\n",
              "           0.17045632,  0.1362613 ,  0.03226766, -0.07059897, -0.25278726,\n",
              "           0.08518812, -0.13398564,  0.26967052, -0.1115679 ,  0.06642497,\n",
              "           0.03717666, -0.09557053, -0.2430807 ,  0.17911513,  0.05716119,\n",
              "           0.25492093,  0.23337285,  0.00986834, -0.04807474],\n",
              "         [ 0.23305687,  0.13537575, -0.24877487,  0.19932957, -0.18073405,\n",
              "          -0.2740849 ,  0.21241353, -0.08264083,  0.07601912, -0.19629706,\n",
              "          -0.18371455,  0.197416  ,  0.09589848, -0.27048188,  0.21521899,\n",
              "           0.07072047,  0.14577095, -0.27129516,  0.21817388,  0.1167529 ,\n",
              "          -0.16852891,  0.27880067,  0.25248307,  0.02767155,  0.25090694,\n",
              "          -0.05641469, -0.28201813, -0.1375741 , -0.0312783 , -0.14770521,\n",
              "          -0.16349764,  0.21367685, -0.25654316, -0.16497636, -0.02196257,\n",
              "          -0.00897481,  0.18977934,  0.06422842,  0.02425396,  0.01959861,\n",
              "          -0.10871091,  0.21125478, -0.25675556, -0.1403536 , -0.21531408,\n",
              "          -0.17969324,  0.01867231, -0.00864663, -0.10423698,  0.14882982,\n",
              "          -0.06362787,  0.16033556,  0.14483844, -0.19612262,  0.20173484,\n",
              "           0.07612017, -0.14489938,  0.05898526,  0.13112646,  0.26731026,\n",
              "          -0.22062328, -0.28153598,  0.22516468,  0.1309027 ],\n",
              "         [-0.12691006, -0.18229017, -0.28793514, -0.03811168, -0.0993931 ,\n",
              "           0.07858251, -0.18432735,  0.17267974,  0.21483405,  0.14824538,\n",
              "          -0.24454486, -0.2928316 ,  0.13104749,  0.25809294, -0.14742395,\n",
              "          -0.10301353,  0.05313297,  0.07211593, -0.25990456, -0.09721714,\n",
              "          -0.21767545, -0.18053734, -0.16550432,  0.22478339,  0.2070582 ,\n",
              "           0.25403026, -0.04013694,  0.19177963,  0.2740298 , -0.19102107,\n",
              "           0.0932865 ,  0.23331179, -0.24605364,  0.20071293,  0.22728623,\n",
              "           0.10880809, -0.22095601,  0.10378339,  0.1658447 , -0.1869503 ,\n",
              "          -0.08466125,  0.00633586,  0.10089715, -0.09474967,  0.15659042,\n",
              "           0.02350584,  0.16649482, -0.24425046,  0.23482178, -0.01913911,\n",
              "          -0.1433352 ,  0.24408644, -0.24538954,  0.2152104 , -0.28577638,\n",
              "           0.17164457, -0.24310394, -0.18834855, -0.25797126,  0.07190805,\n",
              "          -0.09722333, -0.06651579, -0.1383652 ,  0.06505597],\n",
              "         [-0.05129467, -0.23553926, -0.12401477, -0.21638778,  0.16332516,\n",
              "           0.14203744, -0.10192067, -0.27725053, -0.09546097,  0.07068303,\n",
              "          -0.01661874,  0.08033442, -0.06274857,  0.25477627, -0.07866727,\n",
              "          -0.24613714,  0.04328592,  0.07189724,  0.04930288,  0.05685035,\n",
              "          -0.12926793, -0.1874276 ,  0.17412452, -0.16161478,  0.001462  ,\n",
              "          -0.06299478,  0.23880766, -0.25659224, -0.02426622, -0.30347365,\n",
              "          -0.18439679, -0.26817387,  0.0817165 , -0.06835332,  0.01388232,\n",
              "           0.16898383,  0.05191222, -0.12827696,  0.2684133 , -0.19212194,\n",
              "          -0.29523268,  0.04999308, -0.02358686, -0.01021052,  0.14854868,\n",
              "          -0.10811181, -0.16266237, -0.02762336,  0.22641474,  0.03368529,\n",
              "          -0.18266137,  0.21123666, -0.04951221,  0.08942714, -0.21883702,\n",
              "           0.04218254,  0.09385196, -0.19380735,  0.015547  ,  0.20356172,\n",
              "           0.12258406, -0.08621672, -0.17312442, -0.02209472],\n",
              "         [ 0.00462341, -0.2370585 , -0.07596243, -0.28305995, -0.23377427,\n",
              "          -0.13339935,  0.22916186,  0.06400775, -0.20888922,  0.15717286,\n",
              "          -0.13727619, -0.21374059, -0.26128244, -0.2865044 ,  0.17589444,\n",
              "           0.25061935, -0.1526547 , -0.24087971,  0.13382892, -0.12393283,\n",
              "           0.21623701, -0.13417333, -0.2179557 , -0.05985267, -0.08175051,\n",
              "          -0.10890021,  0.15373404, -0.19819447,  0.02185676,  0.07893504,\n",
              "          -0.23474549,  0.2418861 ,  0.11994999, -0.16758077,  0.06944674,\n",
              "           0.0382697 , -0.12382744,  0.03853316, -0.29385918, -0.03464898,\n",
              "          -0.03209778,  0.15230955,  0.27373207, -0.18256387, -0.27441332,\n",
              "           0.27245292,  0.17606272, -0.1478741 ,  0.0346972 , -0.23107503,\n",
              "          -0.20529701,  0.16960214, -0.09846441, -0.15606444, -0.16410854,\n",
              "           0.06978884, -0.27147695,  0.15320016, -0.26612017,  0.04665013,\n",
              "           0.08885942,  0.19760528,  0.24094202, -0.17069386],\n",
              "         [ 0.01504372, -0.07335474, -0.14957592, -0.0633302 ,  0.25573656,\n",
              "           0.11245739, -0.11860147, -0.22408445, -0.15485556,  0.27895483,\n",
              "          -0.02793109,  0.14186612, -0.19308214,  0.27218714, -0.19458091,\n",
              "          -0.07992425, -0.13229077, -0.18093388,  0.26078412,  0.17150344,\n",
              "          -0.04582422,  0.09652359, -0.19305378,  0.27721626,  0.04878773,\n",
              "           0.16098294, -0.13669328, -0.28621072,  0.04640153, -0.02725471,\n",
              "           0.08874305, -0.18135762, -0.3034408 , -0.17308831,  0.15389492,\n",
              "          -0.00640515, -0.22589336, -0.26926604,  0.03569161,  0.2084226 ,\n",
              "          -0.185093  , -0.28517723, -0.29234484, -0.20138371, -0.06480278,\n",
              "           0.23938198, -0.13014852, -0.22642499, -0.3158189 ,  0.13868737,\n",
              "          -0.2226207 ,  0.12460686, -0.09304708,  0.03854367,  0.08023262,\n",
              "           0.07417995, -0.26078933, -0.0452918 , -0.25649688, -0.2750261 ,\n",
              "           0.19748151,  0.24691667,  0.05630534,  0.21254511],\n",
              "         [ 0.11080245,  0.14336476, -0.20984417, -0.21870996,  0.18074752,\n",
              "           0.03365314,  0.04263329, -0.3002727 ,  0.04842418,  0.25212854,\n",
              "           0.30768773, -0.00800792,  0.00814455,  0.06877965, -0.07751557,\n",
              "          -0.01385562, -0.16443133, -0.067956  ,  0.27038962,  0.06305066,\n",
              "           0.293669  , -0.10948412, -0.21905757,  0.09929935,  0.2217015 ,\n",
              "           0.16706099, -0.02330099,  0.0339357 , -0.16683866, -0.27242807,\n",
              "           0.12245858, -0.06591696,  0.31264997,  0.00441343,  0.20335901,\n",
              "           0.1453192 ,  0.02278182, -0.22871321, -0.14576511,  0.19934125,\n",
              "          -0.18641672,  0.13639395,  0.1420881 , -0.02638541, -0.18600139,\n",
              "           0.20961837, -0.1407433 ,  0.26243454, -0.21357073, -0.06837419,\n",
              "           0.11657016, -0.25353912, -0.18513007,  0.26853016,  0.0893217 ,\n",
              "          -0.16160427, -0.231861  ,  0.18480873, -0.16924608, -0.11558498,\n",
              "           0.17708923,  0.07890689, -0.1755961 ,  0.17460524]],\n",
              "        dtype=float32),\n",
              "  array([-2.6053416e-02, -4.4443468e-03,  3.4386158e-02,  2.5024278e-02,\n",
              "          3.2565631e-02, -2.3081107e-02,  3.2981738e-02,  3.3102579e-02,\n",
              "          3.2867610e-02, -1.7878732e-02,  3.2278322e-02,  2.6177106e-02,\n",
              "         -2.1447256e-02,  2.9654346e-02, -2.9369215e-02, -8.1347618e-03,\n",
              "          3.4293238e-02,  3.4951720e-02,  3.2036070e-02, -3.2282960e-02,\n",
              "          2.6740370e-02,  3.3152752e-02,  3.3777162e-02, -9.1946060e-03,\n",
              "         -8.1075579e-03, -2.2541976e-02,  3.0237209e-02,  2.6801309e-02,\n",
              "          3.1306601e-05,  3.3499509e-02,  2.8562035e-02,  3.0933611e-02,\n",
              "          2.7093938e-02,  5.3394651e-03, -1.9320169e-02,  7.7377521e-03,\n",
              "          3.1977061e-02, -5.1593618e-03,  3.0750876e-02,  3.0061383e-02,\n",
              "          3.2266606e-02,  1.8718751e-02,  2.9273741e-02,  3.4075160e-02,\n",
              "          2.8757494e-02, -2.9328734e-02,  3.2594670e-02,  3.1012589e-02,\n",
              "          2.5354400e-02, -3.1217908e-02,  3.3076394e-02,  3.2798249e-02,\n",
              "          2.9960560e-02,  3.1026864e-02,  3.0303340e-02,  2.7986236e-02,\n",
              "          3.1597376e-02, -2.8865276e-02,  3.8723641e-03,  3.3100478e-02,\n",
              "          2.6910968e-02,  2.6119204e-02,  3.3243660e-02, -3.1219276e-02],\n",
              "        dtype=float32),\n",
              "  array([[-0.19249281,  0.02163024, -0.15354034, ...,  0.1303817 ,\n",
              "           0.19386286,  0.15910718],\n",
              "         [-0.19391532, -0.02345605,  0.12594056, ...,  0.18071598,\n",
              "          -0.07911245,  0.1634358 ],\n",
              "         [-0.18269907, -0.0530268 , -0.03078357, ...,  0.04727123,\n",
              "           0.11314096,  0.00763473],\n",
              "         ...,\n",
              "         [-0.1578775 , -0.08418326, -0.17970674, ..., -0.015871  ,\n",
              "          -0.1759304 ,  0.08929365],\n",
              "         [ 0.08162375,  0.12482604, -0.17948152, ..., -0.013179  ,\n",
              "           0.06037272, -0.15808672],\n",
              "         [-0.02482092, -0.13456708,  0.18169115, ...,  0.06908923,\n",
              "           0.01648882,  0.21394579]], dtype=float32),\n",
              "  array([ 0.03038094,  0.03426934,  0.01454396, -0.02250315,  0.03379673,\n",
              "         -0.0291588 , -0.0174099 , -0.02472721,  0.03359169,  0.03386692,\n",
              "         -0.0238014 ,  0.03351057, -0.02454623,  0.02663023, -0.02521191,\n",
              "         -0.02905038,  0.03377519,  0.03480129, -0.00999507, -0.00791697,\n",
              "         -0.01626589,  0.03101667,  0.03411647,  0.03307446,  0.0333652 ,\n",
              "          0.02669564,  0.03047662, -0.02981451,  0.03359349, -0.01967961,\n",
              "          0.0286276 , -0.02681418, -0.0245399 , -0.02778504,  0.03436557,\n",
              "         -0.02957601,  0.03384957, -0.02726379,  0.03437051, -0.02530908,\n",
              "          0.00853315,  0.03288358, -0.02864121, -0.02308573,  0.03392598,\n",
              "          0.0336237 , -0.01497989,  0.03160534, -0.00122653,  0.03351449,\n",
              "          0.03229039,  0.03467096, -0.02649148, -0.03002086,  0.03397598,\n",
              "          0.033797  ,  0.03334365,  0.00163271,  0.03399217,  0.03387009,\n",
              "          0.03360184, -0.03020428,  0.0340815 ,  0.03372326], dtype=float32),\n",
              "  array([[ 0.19351098],\n",
              "         [ 0.2811161 ],\n",
              "         [ 0.12551817],\n",
              "         [-0.21733873],\n",
              "         [ 0.3066875 ],\n",
              "         [-0.2456258 ],\n",
              "         [-0.03515196],\n",
              "         [-0.2524351 ],\n",
              "         [ 0.27078077],\n",
              "         [ 0.2871764 ],\n",
              "         [-0.29284117],\n",
              "         [ 0.0863632 ],\n",
              "         [-0.2696683 ],\n",
              "         [ 0.24981189],\n",
              "         [-0.16869518],\n",
              "         [-0.10204806],\n",
              "         [ 0.2801141 ],\n",
              "         [ 0.04131586],\n",
              "         [-0.20566201],\n",
              "         [-0.00734675],\n",
              "         [-0.03633419],\n",
              "         [ 0.0721434 ],\n",
              "         [ 0.24781649],\n",
              "         [ 0.15197745],\n",
              "         [ 0.20770353],\n",
              "         [ 0.3253681 ],\n",
              "         [ 0.12533711],\n",
              "         [-0.18891521],\n",
              "         [ 0.22890635],\n",
              "         [-0.03856698],\n",
              "         [ 0.090063  ],\n",
              "         [-0.05099697],\n",
              "         [-0.109291  ],\n",
              "         [-0.18466558],\n",
              "         [ 0.06777371],\n",
              "         [-0.04546506],\n",
              "         [ 0.28338516],\n",
              "         [-0.02347459],\n",
              "         [ 0.14332807],\n",
              "         [-0.09893715],\n",
              "         [ 0.00147532],\n",
              "         [ 0.12657453],\n",
              "         [-0.17308822],\n",
              "         [-0.16845016],\n",
              "         [ 0.29147753],\n",
              "         [ 0.17323467],\n",
              "         [-0.12539905],\n",
              "         [ 0.15497246],\n",
              "         [ 0.01168096],\n",
              "         [ 0.30550343],\n",
              "         [ 0.16739057],\n",
              "         [ 0.01512158],\n",
              "         [-0.14179046],\n",
              "         [-0.01708374],\n",
              "         [ 0.07036731],\n",
              "         [ 0.06408574],\n",
              "         [ 0.257635  ],\n",
              "         [ 0.26216975],\n",
              "         [ 0.06268568],\n",
              "         [ 0.06851675],\n",
              "         [ 0.28944007],\n",
              "         [-0.252865  ],\n",
              "         [ 0.10246518],\n",
              "         [ 0.14517656]], dtype=float32),\n",
              "  array([0.03344583], dtype=float32)],\n",
              " [array([[ 1.39992656e-02,  1.70755610e-01, -3.02087903e-01,\n",
              "          -1.63402230e-01, -1.16663024e-01,  6.30754754e-02,\n",
              "          -4.85954136e-02,  1.48771077e-01,  2.66311795e-01,\n",
              "          -2.09166408e-01,  7.60730356e-02,  7.17321113e-02,\n",
              "           2.90706754e-01,  1.10574424e-01, -6.55508488e-02,\n",
              "           1.59258880e-02, -1.80431426e-01, -6.02141134e-02,\n",
              "          -3.04927111e-01, -1.25252247e-01, -2.04093605e-01,\n",
              "           2.29924276e-01,  2.57368416e-01,  1.20596737e-01,\n",
              "          -8.19010735e-02,  1.29502565e-01, -3.13506909e-02,\n",
              "          -1.98486805e-01,  3.06553803e-02, -6.62712902e-02,\n",
              "           2.43055359e-01,  1.96161166e-01,  8.69130492e-02,\n",
              "           1.17467500e-01, -1.43060207e-01, -1.69572651e-01,\n",
              "           1.72491163e-01, -2.52368867e-01,  3.16151567e-02,\n",
              "          -2.20080927e-01, -5.31184599e-02,  2.27547474e-02,\n",
              "           2.44466200e-01, -2.63304919e-01,  8.37453231e-02,\n",
              "           2.16002718e-01,  2.89817397e-02, -2.50532568e-01,\n",
              "          -1.73996851e-01, -2.10733905e-01,  2.73804694e-01,\n",
              "           1.88687339e-01,  8.71790200e-02,  8.64215493e-02,\n",
              "          -8.10092017e-02,  4.15600054e-02,  2.06962511e-01,\n",
              "           3.99375446e-02, -1.61745340e-01, -1.87380642e-01,\n",
              "           2.77813345e-01,  1.54738903e-01,  1.12221278e-01,\n",
              "           5.45748957e-02],\n",
              "         [ 7.03167021e-02, -1.97558805e-01, -1.02546751e-01,\n",
              "           1.99291319e-01,  2.33782977e-01,  1.50255011e-02,\n",
              "           1.20425532e-02,  2.20674053e-01, -1.46252453e-01,\n",
              "           3.06487251e-02, -2.81000644e-01, -5.26475385e-02,\n",
              "          -7.12502375e-02,  3.61384079e-02, -1.90807834e-01,\n",
              "           1.13585979e-01,  2.25011513e-01,  1.30353838e-01,\n",
              "          -1.58392608e-01,  1.91347746e-04, -8.09262842e-02,\n",
              "           1.28384486e-01, -2.70006537e-01, -1.73113614e-01,\n",
              "           1.08803660e-01,  2.55660921e-01,  2.17312127e-02,\n",
              "           1.03675641e-01,  5.46462983e-02,  1.23740755e-01,\n",
              "           2.62261391e-01, -1.21116087e-01,  2.30843514e-01,\n",
              "          -1.59519091e-01, -1.90957204e-01, -6.11758865e-02,\n",
              "           4.32656109e-02,  2.24349245e-01,  2.57683128e-01,\n",
              "           6.98299110e-02,  5.63685708e-02,  9.83680859e-02,\n",
              "           2.37227790e-02, -4.66990545e-02, -1.15980960e-01,\n",
              "           2.32926831e-01, -3.73734608e-02, -2.22076342e-01,\n",
              "           1.86301216e-01, -1.94977790e-01,  8.42074305e-02,\n",
              "           2.90413082e-01,  2.53455251e-01, -2.92705327e-01,\n",
              "          -2.79892594e-01, -6.23049447e-03, -2.37257138e-01,\n",
              "          -4.88743670e-02, -1.90514829e-02,  2.10482463e-01,\n",
              "          -1.23663001e-01, -2.97034830e-02,  2.46514887e-01,\n",
              "           1.74034953e-01],\n",
              "         [ 1.72895804e-01, -1.64173126e-01, -1.67471230e-01,\n",
              "          -2.50351667e-01,  7.40910247e-02,  5.79826944e-02,\n",
              "          -2.45905802e-01,  1.09164312e-01, -2.56397277e-01,\n",
              "           2.25908428e-01,  1.39439449e-01,  1.58806100e-01,\n",
              "           7.18120411e-02,  5.11657298e-02,  2.33093336e-01,\n",
              "           2.62692511e-01,  1.26704350e-01,  2.13624209e-01,\n",
              "          -1.46502182e-01, -1.22218966e-01, -1.16689846e-01,\n",
              "           2.62357712e-01,  1.26974493e-01,  9.87929404e-02,\n",
              "           9.75605845e-02, -2.49083310e-01, -1.37961432e-01,\n",
              "           2.76550680e-01, -4.35719974e-02, -9.80462655e-02,\n",
              "          -2.51174778e-01, -1.53283641e-01, -1.45237356e-01,\n",
              "          -2.49825656e-01,  1.17200516e-01, -2.44092494e-01,\n",
              "           2.15005636e-01,  6.68311492e-02,  2.73667723e-01,\n",
              "           2.22684383e-01,  1.76209137e-01,  1.12053469e-01,\n",
              "           2.48146076e-02, -1.31953834e-02,  5.88433743e-02,\n",
              "          -2.07835093e-01,  1.00869894e-01,  1.49294108e-01,\n",
              "           1.22449175e-01, -1.57072902e-01,  3.36344019e-02,\n",
              "           2.08258405e-01, -2.47556224e-01,  5.82654551e-02,\n",
              "           2.74347097e-01, -3.04900687e-02, -2.70645380e-01,\n",
              "          -1.02682374e-01,  7.41877928e-02, -1.19312905e-01,\n",
              "           9.35872123e-02, -1.56566292e-01,  1.65749982e-01,\n",
              "          -1.72416806e-01],\n",
              "         [-1.39416996e-02, -6.66391617e-03, -6.04651533e-02,\n",
              "          -5.25499741e-03,  6.66080043e-02, -2.56524056e-01,\n",
              "           1.50196910e-01,  2.35618606e-01, -2.93821841e-01,\n",
              "           7.05642402e-02, -9.97123122e-02,  9.04068723e-02,\n",
              "           2.67901182e-01,  1.10341512e-01, -5.19765578e-02,\n",
              "          -2.72760749e-01, -2.71781266e-01,  2.78620034e-01,\n",
              "          -1.07046574e-01,  1.41321020e-02, -8.77077729e-02,\n",
              "          -1.22735858e-01, -1.08675115e-01,  9.52410772e-02,\n",
              "          -1.38710871e-01,  2.83949196e-01, -1.48480281e-01,\n",
              "          -1.74663469e-01,  7.73632303e-02, -1.14941522e-01,\n",
              "          -2.13821918e-01, -1.04472719e-01,  2.45752297e-02,\n",
              "          -2.10389063e-01, -1.77105427e-01, -1.44858928e-02,\n",
              "           2.45858252e-01, -1.00446649e-01,  1.17275469e-01,\n",
              "          -5.23437373e-02, -1.25836208e-01,  8.44765231e-02,\n",
              "           2.89897829e-01,  2.42184401e-02,  1.96969599e-01,\n",
              "          -1.28973559e-01, -2.64647156e-01,  6.31896704e-02,\n",
              "          -7.06245080e-02,  1.90202385e-01,  2.04384029e-01,\n",
              "           6.64534420e-02, -2.06913203e-01,  2.71416426e-01,\n",
              "           1.89562857e-01, -2.50738353e-01, -1.97241321e-01,\n",
              "           1.69668514e-02, -2.36464649e-01,  1.88718885e-01,\n",
              "           2.38319576e-01,  6.17558323e-02, -3.93689750e-03,\n",
              "          -2.02563196e-01],\n",
              "         [-1.09035820e-01, -7.41266012e-02,  2.90372103e-01,\n",
              "          -2.18283653e-01, -1.68945253e-01, -1.72949493e-01,\n",
              "          -2.98465472e-02, -1.01355603e-03, -9.88908857e-02,\n",
              "          -2.00847656e-01,  6.05036914e-02, -1.48576543e-01,\n",
              "           2.73789823e-01,  1.71220794e-01,  3.36985216e-02,\n",
              "          -5.95105924e-02, -7.29554817e-02, -4.78177965e-02,\n",
              "          -7.02115744e-02,  1.89418972e-01,  1.74276739e-01,\n",
              "          -2.17055231e-01,  2.47736603e-01,  1.64628938e-01,\n",
              "          -3.13662589e-01, -3.09897568e-02,  1.61034867e-01,\n",
              "          -1.49526834e-01, -8.86706810e-04, -1.54064953e-01,\n",
              "          -2.35513091e-01,  2.37081647e-01,  2.60981649e-01,\n",
              "           3.52944247e-02, -1.58560146e-02,  1.85680494e-01,\n",
              "          -8.97115096e-02,  1.91425458e-01,  7.61516467e-02,\n",
              "          -2.33895406e-01, -4.68578711e-02, -1.58381492e-01,\n",
              "           3.15428138e-01,  1.48406446e-01,  2.06202436e-02,\n",
              "           2.64703602e-01, -1.02009416e-01, -1.66534632e-01,\n",
              "           9.31899473e-02,  1.44006744e-01, -2.14210004e-01,\n",
              "           2.56782100e-02, -2.86421537e-01, -1.71324745e-01,\n",
              "           1.17032163e-01,  1.04656011e-01, -7.51477033e-02,\n",
              "           1.30245388e-01,  1.44378111e-01,  1.09530576e-01,\n",
              "          -1.62083238e-01,  3.37327570e-02,  2.56204844e-01,\n",
              "           2.28537500e-01],\n",
              "         [-2.83348292e-01, -2.11441740e-02,  2.06349954e-01,\n",
              "           1.28986686e-01, -8.75715613e-02, -2.23124221e-01,\n",
              "          -3.02708447e-01,  7.70568699e-02, -2.28223607e-01,\n",
              "          -1.11394279e-01,  3.18158381e-02, -1.12215959e-01,\n",
              "          -1.36152506e-01, -8.19942579e-02, -2.33307779e-01,\n",
              "          -3.23804244e-02, -2.57736668e-02,  1.15222424e-01,\n",
              "          -3.26478124e-01,  1.00310035e-01, -6.20748438e-02,\n",
              "          -2.44338319e-01,  8.40213895e-02, -6.22990541e-03,\n",
              "           1.35022059e-01,  1.46469787e-01,  1.62167087e-01,\n",
              "          -2.57575601e-01,  1.25110224e-01,  1.70807824e-01,\n",
              "          -6.96759671e-02, -1.02839163e-02,  7.42447823e-02,\n",
              "          -1.20582618e-01,  1.53788492e-01,  8.24809745e-02,\n",
              "          -5.60693108e-02,  4.02537733e-02, -6.92375079e-02,\n",
              "           2.88484880e-04,  6.00057058e-02,  1.39086202e-01,\n",
              "           5.95935397e-02, -4.19989340e-02,  1.36517704e-01,\n",
              "          -3.08833003e-01, -1.28444061e-01,  3.14240868e-04,\n",
              "           1.03227329e-02, -4.61353548e-02,  8.37177876e-03,\n",
              "          -3.79480980e-02, -7.89337531e-02,  1.69274420e-01,\n",
              "           7.11515471e-02,  1.93789154e-01, -1.42761856e-01,\n",
              "           1.43791780e-01,  3.05714607e-01, -1.82542056e-01,\n",
              "          -2.87358947e-02, -2.34984219e-01, -1.37946740e-01,\n",
              "          -1.39622897e-01],\n",
              "         [ 2.76430070e-01,  1.69157118e-01, -7.55615393e-03,\n",
              "          -4.95180546e-04,  1.76474765e-01, -1.41661331e-01,\n",
              "          -2.66159505e-01,  7.28776529e-02, -1.20099619e-01,\n",
              "          -1.67230800e-01, -3.55018079e-02, -2.14980513e-01,\n",
              "           1.52239770e-01,  2.05036029e-01,  2.43945777e-01,\n",
              "           3.65209430e-02,  1.69063568e-01, -3.85460909e-03,\n",
              "          -7.34094977e-02, -1.46701038e-01, -1.26380354e-01,\n",
              "           2.16450468e-01,  3.29848230e-02,  4.74857837e-02,\n",
              "           1.12654731e-01, -2.76755244e-01,  2.66663998e-01,\n",
              "          -5.09425290e-02,  1.39813572e-01,  6.24493621e-02,\n",
              "          -1.36485741e-01,  1.67413950e-01,  2.40786105e-01,\n",
              "           1.24016739e-01,  3.75877460e-03,  1.94528863e-01,\n",
              "           2.87673652e-01, -1.63860127e-01, -7.33556077e-02,\n",
              "          -2.49229237e-01, -1.87865973e-01,  3.34223956e-02,\n",
              "          -4.18513045e-02, -2.54912116e-02, -9.25603136e-02,\n",
              "          -1.20920070e-01,  1.29560679e-01, -2.33537614e-01,\n",
              "          -2.72197932e-01,  1.28230944e-01, -1.47749200e-01,\n",
              "           2.32692182e-01,  1.84383467e-01,  9.12245363e-02,\n",
              "           2.09822506e-01, -5.21331131e-02,  7.75880814e-02,\n",
              "           1.38655886e-01,  7.83338696e-02,  6.28764033e-02,\n",
              "          -8.13687444e-02, -4.48089913e-02, -2.42949381e-01,\n",
              "          -2.06784695e-01],\n",
              "         [-4.83730175e-02, -1.34640351e-01, -1.41832605e-01,\n",
              "           8.25061798e-02,  3.63259465e-02, -9.50152650e-02,\n",
              "          -1.13722079e-01,  2.12112933e-01,  2.95875538e-02,\n",
              "          -1.51062876e-01, -1.19234480e-01, -2.40474254e-01,\n",
              "          -1.58724129e-01,  1.98112965e-01, -2.03160211e-01,\n",
              "          -1.93576425e-01,  2.13840231e-02, -3.95494848e-02,\n",
              "           2.28075683e-01,  6.18822053e-02,  3.77145782e-02,\n",
              "           9.96031705e-03, -2.31295273e-01,  1.86268389e-01,\n",
              "          -2.83206385e-02, -2.17319224e-02, -2.93176752e-02,\n",
              "          -2.69366175e-01,  1.28397897e-01,  2.81186223e-01,\n",
              "           3.04440051e-01,  1.25776483e-02,  5.71293868e-02,\n",
              "          -5.36519513e-02,  1.49872094e-01,  2.87198603e-01,\n",
              "           2.07501009e-01,  2.74833590e-01,  2.30874538e-01,\n",
              "          -1.61801368e-01, -1.40347192e-02,  2.75997847e-01,\n",
              "           2.26299495e-01, -5.81974685e-02,  9.47914347e-02,\n",
              "           1.52556062e-01,  1.23477899e-01,  1.06305238e-02,\n",
              "           3.12223911e-01, -1.93365246e-01,  1.91932440e-01,\n",
              "          -2.79348195e-01,  3.44512984e-02, -2.40501106e-01,\n",
              "           2.13744819e-01, -2.56779134e-01, -2.23811157e-02,\n",
              "          -2.38381356e-01, -5.23144752e-02,  8.36993083e-02,\n",
              "           1.06549144e-01, -2.80610889e-01, -5.40216602e-02,\n",
              "          -2.50003356e-02]], dtype=float32),\n",
              "  array([ 0.05568855, -0.04720656,  0.05551764,  0.05577308, -0.05487794,\n",
              "         -0.05271612,  0.03854816, -0.05485396,  0.05892848, -0.05728126,\n",
              "          0.05748373, -0.05676459, -0.01225582, -0.05570622, -0.05003144,\n",
              "         -0.05467927,  0.04869385,  0.05184381,  0.05593931,  0.05679859,\n",
              "         -0.05478276,  0.05166448,  0.05723565, -0.05359999,  0.0575923 ,\n",
              "          0.05977613,  0.05475668,  0.05727216, -0.05727366,  0.0565162 ,\n",
              "          0.05723999,  0.05780707, -0.05593276,  0.05963289, -0.05305357,\n",
              "         -0.05296287,  0.05465427,  0.05458041,  0.05570789, -0.05459623,\n",
              "         -0.05544184, -0.05468095, -0.03298209, -0.05199523, -0.05550123,\n",
              "          0.05658807, -0.05472196,  0.00037102,  0.05674631, -0.05455814,\n",
              "          0.05718252,  0.05672627,  0.05476901,  0.05775763,  0.05575369,\n",
              "          0.05627223, -0.05446435, -0.05633118, -0.05484519, -0.05560423,\n",
              "          0.05747766,  0.05911371, -0.02886432, -0.05287759], dtype=float32),\n",
              "  array([[-0.09383707,  0.12446725, -0.09406442, ...,  0.11241489,\n",
              "          -0.12406915,  0.05729943],\n",
              "         [ 0.13598362, -0.07437053, -0.12362625, ...,  0.13631552,\n",
              "           0.0048225 ,  0.20626198],\n",
              "         [-0.02556301, -0.17554928, -0.00508953, ...,  0.12233987,\n",
              "          -0.23814847,  0.02369536],\n",
              "         ...,\n",
              "         [ 0.24758157,  0.12318617,  0.04487206, ..., -0.23628725,\n",
              "           0.01294338,  0.06492224],\n",
              "         [-0.01349841, -0.17455211, -0.07845577, ..., -0.16866146,\n",
              "          -0.14767349,  0.015961  ],\n",
              "         [ 0.08192509,  0.180108  ,  0.19966881, ..., -0.15414903,\n",
              "           0.14590262, -0.11747898]], dtype=float32),\n",
              "  array([ 0.0543751 , -0.05252928, -0.05401313, -0.05458541,  0.05570261,\n",
              "         -0.05511297,  0.05447679, -0.05307399,  0.05463289,  0.05427279,\n",
              "         -0.05377118, -0.05457113,  0.05377841,  0.05446082,  0.05671074,\n",
              "          0.05551209, -0.05189642, -0.05330856, -0.05329701,  0.05477184,\n",
              "         -0.05400932, -0.05500573,  0.05588279, -0.05629337,  0.05397371,\n",
              "          0.05549841,  0.05515005,  0.05465274, -0.05383651,  0.05404417,\n",
              "          0.05425062,  0.05560591,  0.05565382,  0.05533072, -0.0543182 ,\n",
              "          0.05520453, -0.05442531,  0.05494694, -0.05476617,  0.05597081,\n",
              "         -0.05494318, -0.05594698,  0.05453934, -0.0546369 , -0.05351641,\n",
              "          0.05494757, -0.05570721, -0.05471715,  0.05459149,  0.05518395,\n",
              "          0.0549268 ,  0.05499727, -0.05368152, -0.05491771, -0.05458914,\n",
              "          0.05345925,  0.05470406, -0.05533991,  0.0558362 ,  0.05558885,\n",
              "          0.05457616, -0.05512047, -0.05285972,  0.05580822], dtype=float32),\n",
              "  array([[ 0.19585451],\n",
              "         [-0.04768882],\n",
              "         [-0.15039986],\n",
              "         [-0.1852753 ],\n",
              "         [ 0.06001725],\n",
              "         [-0.19590813],\n",
              "         [ 0.28968164],\n",
              "         [-0.10520507],\n",
              "         [ 0.31878838],\n",
              "         [ 0.10918311],\n",
              "         [-0.06327061],\n",
              "         [-0.04230002],\n",
              "         [ 0.21113928],\n",
              "         [ 0.07328822],\n",
              "         [ 0.05078505],\n",
              "         [ 0.20521238],\n",
              "         [-0.18545622],\n",
              "         [-0.12791395],\n",
              "         [-0.03761658],\n",
              "         [ 0.21897389],\n",
              "         [-0.32982406],\n",
              "         [-0.22046179],\n",
              "         [ 0.05477744],\n",
              "         [-0.04559667],\n",
              "         [ 0.25689188],\n",
              "         [ 0.29056844],\n",
              "         [ 0.18818259],\n",
              "         [ 0.1124671 ],\n",
              "         [-0.08343892],\n",
              "         [ 0.28872147],\n",
              "         [ 0.18795137],\n",
              "         [ 0.17101946],\n",
              "         [ 0.16682063],\n",
              "         [ 0.11324316],\n",
              "         [-0.05640825],\n",
              "         [ 0.0927102 ],\n",
              "         [-0.17698862],\n",
              "         [ 0.33913326],\n",
              "         [-0.18367779],\n",
              "         [ 0.1659778 ],\n",
              "         [-0.26937518],\n",
              "         [-0.16775477],\n",
              "         [ 0.3357867 ],\n",
              "         [-0.29403844],\n",
              "         [-0.24606729],\n",
              "         [ 0.32251683],\n",
              "         [-0.05053043],\n",
              "         [-0.09135272],\n",
              "         [ 0.21481754],\n",
              "         [ 0.09478555],\n",
              "         [ 0.31984943],\n",
              "         [ 0.14785701],\n",
              "         [-0.11488909],\n",
              "         [-0.08861437],\n",
              "         [-0.21481541],\n",
              "         [ 0.27939513],\n",
              "         [ 0.19384263],\n",
              "         [-0.10402234],\n",
              "         [ 0.18744601],\n",
              "         [ 0.24928795],\n",
              "         [ 0.17036791],\n",
              "         [-0.15685877],\n",
              "         [-0.2362234 ],\n",
              "         [ 0.28916067]], dtype=float32),\n",
              "  array([0.05414947], dtype=float32)],\n",
              " [array([[ 0.02731738, -0.29391584,  0.31745574,  0.18389906,  0.25549573,\n",
              "           0.18245287,  0.24136534, -0.16936566, -0.2950346 ,  0.14948359,\n",
              "          -0.39335957, -0.28249115, -0.1337084 ,  0.1162661 ,  0.01742799,\n",
              "           0.21765661, -0.16902621, -0.25150204, -0.3846142 , -0.15522216,\n",
              "          -0.15763734,  0.39917725,  0.151584  , -0.04808031,  0.35690072,\n",
              "           0.209429  ,  0.20106116,  0.26917908,  0.11742007, -0.09223432,\n",
              "          -0.1774732 , -0.02521285],\n",
              "         [ 0.36046502,  0.3601428 ,  0.2926992 , -0.09737293, -0.04655708,\n",
              "          -0.2139771 , -0.24837871, -0.24070883, -0.09609826, -0.27013898,\n",
              "          -0.44459376, -0.24523045, -0.26089352,  0.32063743,  0.02839617,\n",
              "           0.22646554, -0.1422538 , -0.21937045,  0.25356874,  0.16068175,\n",
              "           0.43534324, -0.3497415 , -0.33283815,  0.22708352, -0.37774387,\n",
              "          -0.2078162 ,  0.29674715,  0.08581097, -0.08279072,  0.3309594 ,\n",
              "          -0.13410676,  0.03747393],\n",
              "         [-0.43466553,  0.2891152 ,  0.21067892,  0.36370534,  0.09382768,\n",
              "           0.24557827,  0.10295706,  0.25154406, -0.09902047, -0.1955061 ,\n",
              "          -0.15530421, -0.02262403, -0.14939442,  0.29021084,  0.01401834,\n",
              "           0.266281  ,  0.01130279, -0.22341703, -0.19712111, -0.27135962,\n",
              "          -0.09903976, -0.2689484 ,  0.2579924 ,  0.29642352,  0.10685366,\n",
              "           0.0576872 ,  0.29202992,  0.02343344, -0.16690598, -0.28025198,\n",
              "           0.3254207 , -0.0532408 ],\n",
              "         [ 0.15101917,  0.00438432,  0.31868917, -0.07967791,  0.09709718,\n",
              "          -0.28037077,  0.08245894,  0.26035637, -0.12411876, -0.30165505,\n",
              "          -0.3786599 , -0.04419964, -0.07110466, -0.36930105, -0.25243443,\n",
              "          -0.12965243, -0.17523117,  0.03456433,  0.12346508,  0.1987822 ,\n",
              "           0.22923478,  0.35883418, -0.4267396 , -0.12010963,  0.17170002,\n",
              "          -0.32452267,  0.07329176, -0.35080963,  0.05908614, -0.1377536 ,\n",
              "           0.01249956, -0.04311825],\n",
              "         [-0.06108151, -0.19131345,  0.11765225,  0.11697176,  0.22970465,\n",
              "           0.24593127,  0.1794593 ,  0.0623267 ,  0.01002869,  0.27436113,\n",
              "          -0.06797779,  0.3463452 ,  0.16996   ,  0.10848819,  0.03326077,\n",
              "           0.3312184 ,  0.03376848, -0.19248635,  0.38091648,  0.25258812,\n",
              "           0.33624253, -0.323261  , -0.3112995 ,  0.06063747, -0.24600816,\n",
              "          -0.278438  ,  0.30200547,  0.26344222, -0.18930489, -0.44107562,\n",
              "           0.21269174,  0.3009706 ],\n",
              "         [ 0.37282807,  0.2851306 ,  0.03538508,  0.17724043,  0.10454562,\n",
              "          -0.18334043,  0.16964513,  0.0546984 , -0.3044082 ,  0.34073478,\n",
              "           0.34771466,  0.12233757,  0.00073116,  0.1997343 , -0.3227991 ,\n",
              "           0.11007341,  0.1652878 ,  0.07796087, -0.40928265, -0.19064398,\n",
              "          -0.19512527,  0.07268665, -0.23868412, -0.17803496,  0.32044116,\n",
              "           0.2310098 , -0.02464118, -0.3068964 ,  0.34809285,  0.2742565 ,\n",
              "          -0.27571216,  0.3762457 ],\n",
              "         [-0.00497661,  0.11767423, -0.36138278,  0.14078487,  0.35952905,\n",
              "           0.26009706, -0.02754061,  0.29794434,  0.22928032, -0.02565104,\n",
              "          -0.0667308 , -0.30898386,  0.32727647, -0.02434072, -0.11884461,\n",
              "           0.19819355, -0.08377247, -0.2619098 ,  0.17729335, -0.29486236,\n",
              "          -0.25282177,  0.04842148, -0.44418323,  0.32862583, -0.26914555,\n",
              "           0.17426357, -0.03804301, -0.40794358,  0.23526146,  0.18184136,\n",
              "           0.23031902, -0.2343738 ],\n",
              "         [ 0.11903945,  0.43168345, -0.13390201,  0.36168537, -0.3091053 ,\n",
              "           0.24839386,  0.27112788, -0.11216019,  0.0106052 , -0.10229708,\n",
              "          -0.09439757, -0.3457873 , -0.03996676,  0.26837498, -0.13987902,\n",
              "          -0.05297276, -0.30373013,  0.12593499, -0.2707886 , -0.0435436 ,\n",
              "          -0.40446955, -0.25025174,  0.4338278 , -0.37054262, -0.3401456 ,\n",
              "           0.1117277 ,  0.32586646,  0.00732637,  0.3218677 ,  0.04429973,\n",
              "          -0.36555243,  0.23370428]], dtype=float32),\n",
              "  array([ 0.07325145,  0.07251624, -0.0180673 ,  0.00276619,  0.06265197,\n",
              "          0.07160391, -0.0703567 , -0.06753329, -0.06842485, -0.06393605,\n",
              "          0.07197322,  0.07152261, -0.06816884, -0.00508461, -0.06978583,\n",
              "         -0.06929242, -0.03782532, -0.07070172,  0.07115947,  0.00571329,\n",
              "          0.07165328,  0.07143133,  0.0727941 ,  0.06914536,  0.07182755,\n",
              "         -0.07006301,  0.07258213,  0.0745309 , -0.0644702 ,  0.07258195,\n",
              "          0.06952795, -0.07160456], dtype=float32),\n",
              "  array([[ 0.1226359 ],\n",
              "         [ 0.13469905],\n",
              "         [ 0.01385185],\n",
              "         [ 0.02493186],\n",
              "         [ 0.05727416],\n",
              "         [ 0.3353879 ],\n",
              "         [-0.18986166],\n",
              "         [-0.29883438],\n",
              "         [-0.16692124],\n",
              "         [-0.00098131],\n",
              "         [ 0.2630357 ],\n",
              "         [ 0.26632312],\n",
              "         [-0.16849957],\n",
              "         [ 0.02704338],\n",
              "         [-0.12467949],\n",
              "         [-0.2138733 ],\n",
              "         [ 0.00907882],\n",
              "         [-0.12058285],\n",
              "         [ 0.22572733],\n",
              "         [ 0.032731  ],\n",
              "         [ 0.282273  ],\n",
              "         [ 0.41404176],\n",
              "         [ 0.36691192],\n",
              "         [ 0.25762382],\n",
              "         [ 0.3015569 ],\n",
              "         [-0.17472279],\n",
              "         [ 0.21967198],\n",
              "         [ 0.1088679 ],\n",
              "         [-0.00203828],\n",
              "         [ 0.17853518],\n",
              "         [ 0.4706929 ],\n",
              "         [-0.24418727]], dtype=float32),\n",
              "  array([0.07364798], dtype=float32)]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 390
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frpagCE6NecP",
        "colab_type": "text"
      },
      "source": [
        "### Using Reporting()\n",
        "In the Scan process, the results are stored round-by-round in the corresponding experiment log which is a .csv file stored in the present working directory. The Reporting() accepts as its source either a file name, or the Scan object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7KrtTZuOZO9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# use Scan object as input\n",
        "analyze_object = talos.Analyze(scan_object)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_atTXmSDOd9P",
        "colab_type": "code",
        "outputId": "17980c4f-ed54-4bd7-e302-7074a0f1fe22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# access the dataframe with the results\n",
        "analyze_object.data"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>start</th>\n",
              "      <th>end</th>\n",
              "      <th>duration</th>\n",
              "      <th>round_epochs</th>\n",
              "      <th>loss</th>\n",
              "      <th>mse</th>\n",
              "      <th>val_loss</th>\n",
              "      <th>val_mse</th>\n",
              "      <th>activation</th>\n",
              "      <th>batch_size</th>\n",
              "      <th>dropout</th>\n",
              "      <th>epochs</th>\n",
              "      <th>first_neuron</th>\n",
              "      <th>hidden_layers</th>\n",
              "      <th>losses</th>\n",
              "      <th>lr</th>\n",
              "      <th>optimizer</th>\n",
              "      <th>shapes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>03/29/20-225448</td>\n",
              "      <td>03/29/20-225602</td>\n",
              "      <td>74.184704</td>\n",
              "      <td>100</td>\n",
              "      <td>0.701878</td>\n",
              "      <td>1.005612</td>\n",
              "      <td>0.707667</td>\n",
              "      <td>1.003745</td>\n",
              "      <td>elu</td>\n",
              "      <td>20</td>\n",
              "      <td>0</td>\n",
              "      <td>100</td>\n",
              "      <td>32</td>\n",
              "      <td>3</td>\n",
              "      <td>mae</td>\n",
              "      <td>0.001</td>\n",
              "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
              "      <td>brick</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>03/29/20-225602</td>\n",
              "      <td>03/29/20-225706</td>\n",
              "      <td>63.696198</td>\n",
              "      <td>100</td>\n",
              "      <td>0.729934</td>\n",
              "      <td>0.730945</td>\n",
              "      <td>0.743067</td>\n",
              "      <td>0.744447</td>\n",
              "      <td>elu</td>\n",
              "      <td>20</td>\n",
              "      <td>0</td>\n",
              "      <td>100</td>\n",
              "      <td>64</td>\n",
              "      <td>1</td>\n",
              "      <td>mse</td>\n",
              "      <td>0.001</td>\n",
              "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.g...</td>\n",
              "      <td>brick</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>03/29/20-225706</td>\n",
              "      <td>03/29/20-225825</td>\n",
              "      <td>78.316898</td>\n",
              "      <td>100</td>\n",
              "      <td>1.192151</td>\n",
              "      <td>1.194020</td>\n",
              "      <td>1.257920</td>\n",
              "      <td>1.259226</td>\n",
              "      <td>relu</td>\n",
              "      <td>20</td>\n",
              "      <td>0</td>\n",
              "      <td>100</td>\n",
              "      <td>64</td>\n",
              "      <td>3</td>\n",
              "      <td>mse</td>\n",
              "      <td>0.001</td>\n",
              "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
              "      <td>brick</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>03/29/20-225825</td>\n",
              "      <td>03/29/20-225931</td>\n",
              "      <td>66.415177</td>\n",
              "      <td>100</td>\n",
              "      <td>0.951792</td>\n",
              "      <td>0.953028</td>\n",
              "      <td>0.983768</td>\n",
              "      <td>0.984797</td>\n",
              "      <td>relu</td>\n",
              "      <td>20</td>\n",
              "      <td>0</td>\n",
              "      <td>100</td>\n",
              "      <td>32</td>\n",
              "      <td>2</td>\n",
              "      <td>mse</td>\n",
              "      <td>0.001</td>\n",
              "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.g...</td>\n",
              "      <td>brick</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>03/29/20-225932</td>\n",
              "      <td>03/29/20-230039</td>\n",
              "      <td>67.255532</td>\n",
              "      <td>100</td>\n",
              "      <td>1.685242</td>\n",
              "      <td>4.186103</td>\n",
              "      <td>1.722976</td>\n",
              "      <td>4.352967</td>\n",
              "      <td>relu</td>\n",
              "      <td>20</td>\n",
              "      <td>0</td>\n",
              "      <td>100</td>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "      <td>mae</td>\n",
              "      <td>0.001</td>\n",
              "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.n...</td>\n",
              "      <td>brick</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>03/29/20-230039</td>\n",
              "      <td>03/29/20-230143</td>\n",
              "      <td>63.983262</td>\n",
              "      <td>100</td>\n",
              "      <td>2.993923</td>\n",
              "      <td>2.997629</td>\n",
              "      <td>3.319123</td>\n",
              "      <td>3.320611</td>\n",
              "      <td>relu</td>\n",
              "      <td>20</td>\n",
              "      <td>0</td>\n",
              "      <td>100</td>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "      <td>mse</td>\n",
              "      <td>0.001</td>\n",
              "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
              "      <td>brick</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>03/29/20-230143</td>\n",
              "      <td>03/29/20-230258</td>\n",
              "      <td>74.553571</td>\n",
              "      <td>100</td>\n",
              "      <td>1.778293</td>\n",
              "      <td>1.780283</td>\n",
              "      <td>1.760582</td>\n",
              "      <td>1.762643</td>\n",
              "      <td>elu</td>\n",
              "      <td>20</td>\n",
              "      <td>0</td>\n",
              "      <td>100</td>\n",
              "      <td>32</td>\n",
              "      <td>3</td>\n",
              "      <td>mse</td>\n",
              "      <td>0.001</td>\n",
              "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
              "      <td>brick</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>03/29/20-230258</td>\n",
              "      <td>03/29/20-230403</td>\n",
              "      <td>65.255099</td>\n",
              "      <td>100</td>\n",
              "      <td>0.650136</td>\n",
              "      <td>0.650901</td>\n",
              "      <td>0.694784</td>\n",
              "      <td>0.696294</td>\n",
              "      <td>elu</td>\n",
              "      <td>20</td>\n",
              "      <td>0</td>\n",
              "      <td>100</td>\n",
              "      <td>32</td>\n",
              "      <td>2</td>\n",
              "      <td>mse</td>\n",
              "      <td>0.001</td>\n",
              "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.g...</td>\n",
              "      <td>brick</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>03/29/20-230403</td>\n",
              "      <td>03/29/20-230517</td>\n",
              "      <td>74.075158</td>\n",
              "      <td>100</td>\n",
              "      <td>0.810759</td>\n",
              "      <td>0.811942</td>\n",
              "      <td>0.903872</td>\n",
              "      <td>0.905537</td>\n",
              "      <td>relu</td>\n",
              "      <td>20</td>\n",
              "      <td>0</td>\n",
              "      <td>100</td>\n",
              "      <td>64</td>\n",
              "      <td>2</td>\n",
              "      <td>mse</td>\n",
              "      <td>0.001</td>\n",
              "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.n...</td>\n",
              "      <td>brick</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>03/29/20-230518</td>\n",
              "      <td>03/29/20-230625</td>\n",
              "      <td>66.990948</td>\n",
              "      <td>100</td>\n",
              "      <td>1.133847</td>\n",
              "      <td>2.307838</td>\n",
              "      <td>1.161780</td>\n",
              "      <td>2.417095</td>\n",
              "      <td>relu</td>\n",
              "      <td>20</td>\n",
              "      <td>0</td>\n",
              "      <td>100</td>\n",
              "      <td>64</td>\n",
              "      <td>2</td>\n",
              "      <td>mae</td>\n",
              "      <td>0.001</td>\n",
              "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.g...</td>\n",
              "      <td>brick</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>03/29/20-230625</td>\n",
              "      <td>03/29/20-230729</td>\n",
              "      <td>64.115000</td>\n",
              "      <td>100</td>\n",
              "      <td>1.656450</td>\n",
              "      <td>4.340416</td>\n",
              "      <td>1.695170</td>\n",
              "      <td>4.620042</td>\n",
              "      <td>relu</td>\n",
              "      <td>20</td>\n",
              "      <td>0</td>\n",
              "      <td>100</td>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "      <td>mae</td>\n",
              "      <td>0.001</td>\n",
              "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
              "      <td>brick</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>03/29/20-230729</td>\n",
              "      <td>03/29/20-230833</td>\n",
              "      <td>63.598839</td>\n",
              "      <td>100</td>\n",
              "      <td>0.820775</td>\n",
              "      <td>0.821799</td>\n",
              "      <td>0.830885</td>\n",
              "      <td>0.831727</td>\n",
              "      <td>relu</td>\n",
              "      <td>20</td>\n",
              "      <td>0</td>\n",
              "      <td>100</td>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "      <td>mse</td>\n",
              "      <td>0.001</td>\n",
              "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.g...</td>\n",
              "      <td>brick</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>03/29/20-230833</td>\n",
              "      <td>03/29/20-230945</td>\n",
              "      <td>71.902190</td>\n",
              "      <td>100</td>\n",
              "      <td>1.216365</td>\n",
              "      <td>1.218418</td>\n",
              "      <td>1.320451</td>\n",
              "      <td>1.321589</td>\n",
              "      <td>relu</td>\n",
              "      <td>20</td>\n",
              "      <td>0</td>\n",
              "      <td>100</td>\n",
              "      <td>32</td>\n",
              "      <td>2</td>\n",
              "      <td>mse</td>\n",
              "      <td>0.001</td>\n",
              "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.n...</td>\n",
              "      <td>brick</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>03/29/20-230945</td>\n",
              "      <td>03/29/20-231053</td>\n",
              "      <td>68.093326</td>\n",
              "      <td>100</td>\n",
              "      <td>1.239378</td>\n",
              "      <td>2.520447</td>\n",
              "      <td>1.253259</td>\n",
              "      <td>2.684544</td>\n",
              "      <td>elu</td>\n",
              "      <td>20</td>\n",
              "      <td>0</td>\n",
              "      <td>100</td>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "      <td>mae</td>\n",
              "      <td>0.001</td>\n",
              "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.n...</td>\n",
              "      <td>brick</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>03/29/20-231053</td>\n",
              "      <td>03/29/20-231201</td>\n",
              "      <td>67.728189</td>\n",
              "      <td>100</td>\n",
              "      <td>3.849401</td>\n",
              "      <td>3.854039</td>\n",
              "      <td>3.922625</td>\n",
              "      <td>3.924561</td>\n",
              "      <td>elu</td>\n",
              "      <td>20</td>\n",
              "      <td>0</td>\n",
              "      <td>100</td>\n",
              "      <td>64</td>\n",
              "      <td>1</td>\n",
              "      <td>mse</td>\n",
              "      <td>0.001</td>\n",
              "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
              "      <td>brick</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>03/29/20-231201</td>\n",
              "      <td>03/29/20-231308</td>\n",
              "      <td>66.549669</td>\n",
              "      <td>100</td>\n",
              "      <td>0.858888</td>\n",
              "      <td>1.481252</td>\n",
              "      <td>0.897093</td>\n",
              "      <td>1.595400</td>\n",
              "      <td>elu</td>\n",
              "      <td>20</td>\n",
              "      <td>0</td>\n",
              "      <td>100</td>\n",
              "      <td>32</td>\n",
              "      <td>2</td>\n",
              "      <td>mae</td>\n",
              "      <td>0.001</td>\n",
              "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.g...</td>\n",
              "      <td>brick</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>03/29/20-231308</td>\n",
              "      <td>03/29/20-231428</td>\n",
              "      <td>80.117752</td>\n",
              "      <td>100</td>\n",
              "      <td>0.594064</td>\n",
              "      <td>0.715449</td>\n",
              "      <td>0.601183</td>\n",
              "      <td>0.760146</td>\n",
              "      <td>elu</td>\n",
              "      <td>20</td>\n",
              "      <td>0</td>\n",
              "      <td>100</td>\n",
              "      <td>64</td>\n",
              "      <td>3</td>\n",
              "      <td>mae</td>\n",
              "      <td>0.001</td>\n",
              "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
              "      <td>brick</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>03/29/20-231428</td>\n",
              "      <td>03/29/20-231547</td>\n",
              "      <td>78.452853</td>\n",
              "      <td>100</td>\n",
              "      <td>0.718989</td>\n",
              "      <td>0.719961</td>\n",
              "      <td>0.826072</td>\n",
              "      <td>0.827515</td>\n",
              "      <td>relu</td>\n",
              "      <td>20</td>\n",
              "      <td>0</td>\n",
              "      <td>100</td>\n",
              "      <td>64</td>\n",
              "      <td>3</td>\n",
              "      <td>mse</td>\n",
              "      <td>0.001</td>\n",
              "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.n...</td>\n",
              "      <td>brick</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>03/29/20-231547</td>\n",
              "      <td>03/29/20-231659</td>\n",
              "      <td>72.048239</td>\n",
              "      <td>100</td>\n",
              "      <td>0.957634</td>\n",
              "      <td>0.959247</td>\n",
              "      <td>1.030138</td>\n",
              "      <td>1.031671</td>\n",
              "      <td>relu</td>\n",
              "      <td>20</td>\n",
              "      <td>0</td>\n",
              "      <td>100</td>\n",
              "      <td>64</td>\n",
              "      <td>2</td>\n",
              "      <td>mse</td>\n",
              "      <td>0.001</td>\n",
              "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
              "      <td>brick</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>03/29/20-231659</td>\n",
              "      <td>03/29/20-231815</td>\n",
              "      <td>75.594268</td>\n",
              "      <td>100</td>\n",
              "      <td>0.549227</td>\n",
              "      <td>0.643474</td>\n",
              "      <td>0.550153</td>\n",
              "      <td>0.638903</td>\n",
              "      <td>elu</td>\n",
              "      <td>20</td>\n",
              "      <td>0</td>\n",
              "      <td>100</td>\n",
              "      <td>64</td>\n",
              "      <td>2</td>\n",
              "      <td>mae</td>\n",
              "      <td>0.001</td>\n",
              "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.n...</td>\n",
              "      <td>brick</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>03/29/20-231815</td>\n",
              "      <td>03/29/20-231923</td>\n",
              "      <td>67.924126</td>\n",
              "      <td>100</td>\n",
              "      <td>2.321051</td>\n",
              "      <td>2.324641</td>\n",
              "      <td>2.413426</td>\n",
              "      <td>2.416014</td>\n",
              "      <td>relu</td>\n",
              "      <td>20</td>\n",
              "      <td>0</td>\n",
              "      <td>100</td>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "      <td>mse</td>\n",
              "      <td>0.001</td>\n",
              "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.n...</td>\n",
              "      <td>brick</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              start  ... shapes\n",
              "0   03/29/20-225448  ...  brick\n",
              "1   03/29/20-225602  ...  brick\n",
              "2   03/29/20-225706  ...  brick\n",
              "3   03/29/20-225825  ...  brick\n",
              "4   03/29/20-225932  ...  brick\n",
              "5   03/29/20-230039  ...  brick\n",
              "6   03/29/20-230143  ...  brick\n",
              "7   03/29/20-230258  ...  brick\n",
              "8   03/29/20-230403  ...  brick\n",
              "9   03/29/20-230518  ...  brick\n",
              "10  03/29/20-230625  ...  brick\n",
              "11  03/29/20-230729  ...  brick\n",
              "12  03/29/20-230833  ...  brick\n",
              "13  03/29/20-230945  ...  brick\n",
              "14  03/29/20-231053  ...  brick\n",
              "15  03/29/20-231201  ...  brick\n",
              "16  03/29/20-231308  ...  brick\n",
              "17  03/29/20-231428  ...  brick\n",
              "18  03/29/20-231547  ...  brick\n",
              "19  03/29/20-231659  ...  brick\n",
              "20  03/29/20-231815  ...  brick\n",
              "\n",
              "[21 rows x 18 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 392
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsB4ZtbSOeEb",
        "colab_type": "code",
        "outputId": "2ee62512-f2fa-49a1-903c-ea3d265611e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# get the number of rounds in the Scan\n",
        "analyze_object.rounds()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "21"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 393
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDk4-GmkOeJt",
        "colab_type": "code",
        "outputId": "74737f98-8f10-420d-e054-dabcd26098e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# get the highest result for any metric\n",
        "analyze_object.high('mse')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.340416431427002"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 394
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBI0sdVhOeNL",
        "colab_type": "code",
        "outputId": "f77542fe-6e99-4624-d3ed-f86657d01b72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# get the lowest result for any metric\n",
        "analyze_object.low('mse')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6434736251831055"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 395
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFB-T3b6OeH0",
        "colab_type": "code",
        "outputId": "45a9abca-f193-4ee3-80c4-f039c3f05535",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# get the round with the best result\n",
        "analyze_object.rounds2high('val_mse')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 396
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBmFXpjKO3mJ",
        "colab_type": "code",
        "outputId": "b9a6f022-bbab-4f67-b63b-d7b1cb40ba85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        }
      },
      "source": [
        "# get the best paramaters\n",
        "analyze_object.best_params(metric='mse', exclude=['loss'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 64.11499953269958, 1.6951704025268555, 0, 100, 0.001, 'brick',\n",
              "        '03/29/20-230729', 20, 'relu', 100, 'mae',\n",
              "        <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>,\n",
              "        4.620042324066162, 32, '03/29/20-230625', 0],\n",
              "       [1, 67.25553154945374, 1.7229764461517334, 0, 100, 0.001, 'brick',\n",
              "        '03/29/20-230039', 20, 'relu', 100, 'mae',\n",
              "        <class 'tensorflow.python.keras.optimizer_v2.nadam.Nadam'>,\n",
              "        4.352967262268066, 32, '03/29/20-225932', 1],\n",
              "       [1, 67.72818922996521, 3.9226253032684326, 0, 100, 0.001, 'brick',\n",
              "        '03/29/20-231201', 20, 'elu', 100, 'mse',\n",
              "        <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>,\n",
              "        3.924560785293579, 64, '03/29/20-231053', 2],\n",
              "       [1, 63.98326230049133, 3.319122791290283, 0, 100, 0.001, 'brick',\n",
              "        '03/29/20-230143', 20, 'relu', 100, 'mse',\n",
              "        <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>,\n",
              "        3.320610761642456, 32, '03/29/20-230039', 3],\n",
              "       [1, 68.09332609176636, 1.2532588243484497, 0, 100, 0.001, 'brick',\n",
              "        '03/29/20-231053', 20, 'elu', 100, 'mae',\n",
              "        <class 'tensorflow.python.keras.optimizer_v2.nadam.Nadam'>,\n",
              "        2.684544086456299, 32, '03/29/20-230945', 4],\n",
              "       [1, 67.92412567138672, 2.413426399230957, 0, 100, 0.001, 'brick',\n",
              "        '03/29/20-231923', 20, 'relu', 100, 'mse',\n",
              "        <class 'tensorflow.python.keras.optimizer_v2.nadam.Nadam'>,\n",
              "        2.416013717651367, 32, '03/29/20-231815', 5],\n",
              "       [2, 66.99094843864441, 1.1617803573608398, 0, 100, 0.001, 'brick',\n",
              "        '03/29/20-230625', 20, 'relu', 100, 'mae',\n",
              "        <class 'tensorflow.python.keras.optimizer_v2.gradient_descent.SGD'>,\n",
              "        2.4170949459075928, 64, '03/29/20-230518', 6],\n",
              "       [3, 74.55357098579407, 1.760582447052002, 0, 100, 0.001, 'brick',\n",
              "        '03/29/20-230258', 20, 'elu', 100, 'mse',\n",
              "        <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>,\n",
              "        1.7626430988311768, 32, '03/29/20-230143', 7],\n",
              "       [2, 66.54966855049133, 0.8970925807952881, 0, 100, 0.001, 'brick',\n",
              "        '03/29/20-231308', 20, 'elu', 100, 'mae',\n",
              "        <class 'tensorflow.python.keras.optimizer_v2.gradient_descent.SGD'>,\n",
              "        1.5953998565673828, 32, '03/29/20-231201', 8],\n",
              "       [2, 71.90219044685364, 1.3204511404037476, 0, 100, 0.001, 'brick',\n",
              "        '03/29/20-230945', 20, 'relu', 100, 'mse',\n",
              "        <class 'tensorflow.python.keras.optimizer_v2.nadam.Nadam'>,\n",
              "        1.321589469909668, 32, '03/29/20-230833', 9]], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 397
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PB-pS-RnQB4g",
        "colab_type": "code",
        "outputId": "9d2b6523-5020-4460-911b-a0408563437b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "# get correlation for hyperparameters against a metric\n",
        "analyze_object.correlate('val_mse', ['epochs', 'round_epochs', 'batch_size', 'dropout'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "duration        -4.414739e-01\n",
              "loss             7.394133e-01\n",
              "mse              9.987491e-01\n",
              "val_loss         7.284959e-01\n",
              "first_neuron    -2.943610e-01\n",
              "hidden_layers   -5.836093e-01\n",
              "lr              -1.016288e-16\n",
              "Name: val_mse, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 398
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZx1nNHYQSA4",
        "colab_type": "text"
      },
      "source": [
        "### Visualizing Reporting()\n",
        "\n",
        "In addition to the key obsevations, several useful plots are available for analysis of the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUJ4qCcBQaMS",
        "colab_type": "code",
        "outputId": "84b814e5-3666-4951-e4ab-e0183bcfb3d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        }
      },
      "source": [
        "# a regression plot for two dimensions \n",
        "analyze_object.plot_regs('val_mse', 'val_loss')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsQAAAHPCAYAAABUeszdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
            "text/plain": [
              "<Figure size 720x475.2 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRTZ-lTPQkls",
        "colab_type": "code",
        "outputId": "c329064b-68d3-4faf-9a37-6ebf7274fdb9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        }
      },
      "source": [
        "# line plot\n",
        "analyze_object.plot_line('val_mse')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1wAAAHPCAYAAABHiNsAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAACEhMAFAAAAACEhcAEAAABASL4AHPNX8gc5OLwAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 864x475.2 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvcmDgjzQp3a",
        "colab_type": "code",
        "outputId": "da8b1377-7e81-49d6-adf8-4cfebb566d0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        }
      },
      "source": [
        "# up to two dimensional kernel density estimator\n",
        "analyze_object.plot_kde('val_mse')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsQAAAHPCAYAAABUeszdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRESymhJiEREREclqSohFREREJKv9P1rIPd3IkmsbAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 720x475.2 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rz4HAUx-Qw0E",
        "colab_type": "code",
        "outputId": "63f2fdfe-2042-4666-9d41-e8a3e8e2b72e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        }
      },
      "source": [
        "# a simple histogram\n",
        "analyze_object.plot_hist('val_mse', bins=50)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsQAAAHPCAYAAABUeszdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTRADAJA0QQwAQNL+P0a91KBq615DAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 720x475.2 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_X3NUnLjQ1W4",
        "colab_type": "code",
        "outputId": "153c3065-1b39-4774-88be-f012e9f4d481",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 643
        }
      },
      "source": [
        "# heatmap correlation\n",
        "analyze_object.plot_corr(metric='val_mse', \n",
        "                         exclude=['epochs', 'round_epochs', 'batch_size', 'dropout'],\n",
        "                         color_grades=5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnkAAAJyCAYAAAC4ztZiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 684x684 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBlIUqVfQ9E0",
        "colab_type": "code",
        "outputId": "c29b321d-28a8-4769-cfec-3b59bcf473ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# a four dimensional bar grid\n",
        "analyze_object.plot_bars('batch_size', 'val_mse', 'first_neuron', 'lr')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABM8AAAEUCAYAAAA81hGFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUjrzAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 1235.75x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3z8gW2kRj7q",
        "colab_type": "text"
      },
      "source": [
        "### Evaluating data with Evaluate()\n",
        "Models can be evaluated with Evaluate() against a k-fold cross-validation. Ideally at least 50% of the data, or more if possible, is kept completely out of the Scan process and only exposed into Evaluate once one or more candidate models have been identified."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZuI-JnWRp2B",
        "colab_type": "code",
        "outputId": "721913eb-c087-4559-dca5-49a5caeaea89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "evaluate_object = talos.Evaluate(scan_object)\n",
        "evaluate_object.evaluate(X_test_scaled, y_test, folds=10, metric='mse', task='continuous')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.6583969244789047,\n",
              " 1.6919705299453403,\n",
              " 1.6981397160601406,\n",
              " 1.644793880876976,\n",
              " 1.631251654854381,\n",
              " 1.7441988274552942,\n",
              " 1.6591423796499123,\n",
              " 1.6709766174638097,\n",
              " 1.6689021419105154,\n",
              " 1.6032133408836764]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 405
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQ-2ZJPPWRF8",
        "colab_type": "text"
      },
      "source": [
        "### Deploying models with Deploy()\n",
        "\n",
        "Once the right model or models have been found, you can create a deployment package with Deploy() which is then easy to transfer to a production or other environment, send via email, or upload to shared remote location. Best model is automatically chosen based on a given metric ('val_acc' by default).\n",
        "\n",
        "The Deploy package is a zip file that consist of:\n",
        "\n",
        "* details of the scan\n",
        "* model weights\n",
        "* model json\n",
        "* results of the experiment\n",
        "* sample of x data\n",
        "* sample of y data\n",
        "\n",
        "The Deploy package can be easily restored with Restore() which is covered in the next section."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1zAd-PCWgYV",
        "colab_type": "code",
        "outputId": "93bce1b2-64ce-4f2c-ded2-82ace0a7232b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "talos.Deploy(scan_object=scan_object, model_name='houses_deploy3', metric='val_mse');\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Deploy package houses_deploy3 have been saved.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSIk5sCyWl6f",
        "colab_type": "text"
      },
      "source": [
        "### Restoring models with Restore()\n",
        "\n",
        "The Restore object now consists of the assets from the Scan object originally associated with the experiment, together with the model that had been picked as 'best'. The model can be immediately used for making prediction, or use in any other other way Keras model objects can be used.\n",
        "\n",
        "In addition, for book keeping purpose, and for simplicity of sharing models with team members and other stakeholders, various attributes are included in the Restore object:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTz4CpdBWudJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "houses_talos = talos.Restore('houses_deploy3.zip')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijBPp4NMW06h",
        "colab_type": "code",
        "outputId": "6bc50400-4499-4451-a37d-d12148a76a24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        }
      },
      "source": [
        "# get the meta-data for the experiment\n",
        "houses_talos.details"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>experiment_name</td>\n",
              "      <td>my_exp</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>random_method</td>\n",
              "      <td>uniform_mersenne</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>reduction_method</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>reduction_interval</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>reduction_window</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>reduction_threshold</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>reduction_metric</td>\n",
              "      <td>val_mse</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>complete_time</td>\n",
              "      <td>03/29/20/23:19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>x_shape</td>\n",
              "      <td>(15480, 8)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>y_shape</td>\n",
              "      <td>(15480,)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                     0                 1\n",
              "0      experiment_name            my_exp\n",
              "1        random_method  uniform_mersenne\n",
              "2     reduction_method               NaN\n",
              "3   reduction_interval                50\n",
              "4     reduction_window                20\n",
              "5  reduction_threshold               0.2\n",
              "6     reduction_metric           val_mse\n",
              "7        complete_time    03/29/20/23:19\n",
              "8              x_shape        (15480, 8)\n",
              "9              y_shape          (15480,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 408
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohv06rAsW1ZE",
        "colab_type": "code",
        "outputId": "bd8b01cd-6d23-42d7-bcff-35b6a8575747",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "# get the hyperparameter space boundary\n",
        "houses_talos.params"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'activation': ['relu', 'elu'],\n",
              " 'batch_size': [20],\n",
              " 'dropout': [0],\n",
              " 'epochs': [100],\n",
              " 'first_neuron': [32, 64],\n",
              " 'hidden_layers': [1, 2, 3],\n",
              " 'losses': ['mse', 'mae'],\n",
              " 'lr': (0.001, 0.01, 0.1, 10, 10),\n",
              " 'optimizer': [tensorflow.python.keras.optimizer_v2.adam.Adam,\n",
              "  tensorflow.python.keras.optimizer_v2.nadam.Nadam,\n",
              "  tensorflow.python.keras.optimizer_v2.gradient_descent.SGD],\n",
              " 'shapes': ['brick']}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 409
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCvj0rw3W4SJ",
        "colab_type": "code",
        "outputId": "9b285e21-105b-461e-d42f-fab16a7dd0e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "# sample of x and y data\n",
        "houses_talos.x\n",
        "houses_talos.y"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.406</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.912</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.279</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.612</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.805</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>1.079</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>1.172</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>1.716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>0.608</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>0.866</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        0\n",
              "0   1.406\n",
              "1   0.912\n",
              "2   1.279\n",
              "3   1.612\n",
              "4   1.805\n",
              "..    ...\n",
              "95  1.079\n",
              "96  1.172\n",
              "97  1.716\n",
              "98  0.608\n",
              "99  0.866\n",
              "\n",
              "[100 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 410
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_UrmauHW2iC",
        "colab_type": "code",
        "outputId": "a86ead81-5c54-427a-8991-a13b5e47e34b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# the results dataframe\n",
        "houses_talos.results"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>start</th>\n",
              "      <th>end</th>\n",
              "      <th>duration</th>\n",
              "      <th>round_epochs</th>\n",
              "      <th>loss</th>\n",
              "      <th>mse</th>\n",
              "      <th>val_loss</th>\n",
              "      <th>val_mse</th>\n",
              "      <th>activation</th>\n",
              "      <th>batch_size</th>\n",
              "      <th>dropout</th>\n",
              "      <th>epochs</th>\n",
              "      <th>first_neuron</th>\n",
              "      <th>hidden_layers</th>\n",
              "      <th>losses</th>\n",
              "      <th>lr</th>\n",
              "      <th>optimizer</th>\n",
              "      <th>shapes</th>\n",
              "      <th>Unnamed: 19</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>03/29/20-225448</td>\n",
              "      <td>03/29/20-225602</td>\n",
              "      <td>74.184704</td>\n",
              "      <td>100</td>\n",
              "      <td>0.701878</td>\n",
              "      <td>1.005612</td>\n",
              "      <td>0.707667</td>\n",
              "      <td>1.003745</td>\n",
              "      <td>elu</td>\n",
              "      <td>20</td>\n",
              "      <td>0</td>\n",
              "      <td>100</td>\n",
              "      <td>32</td>\n",
              "      <td>3</td>\n",
              "      <td>mae</td>\n",
              "      <td>0.001</td>\n",
              "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
              "      <td>brick</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>03/29/20-225602</td>\n",
              "      <td>03/29/20-225706</td>\n",
              "      <td>63.696198</td>\n",
              "      <td>100</td>\n",
              "      <td>0.729934</td>\n",
              "      <td>0.730945</td>\n",
              "      <td>0.743067</td>\n",
              "      <td>0.744447</td>\n",
              "      <td>elu</td>\n",
              "      <td>20</td>\n",
              "      <td>0</td>\n",
              "      <td>100</td>\n",
              "      <td>64</td>\n",
              "      <td>1</td>\n",
              "      <td>mse</td>\n",
              "      <td>0.001</td>\n",
              "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.g...</td>\n",
              "      <td>brick</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>03/29/20-225706</td>\n",
              "      <td>03/29/20-225825</td>\n",
              "      <td>78.316898</td>\n",
              "      <td>100</td>\n",
              "      <td>1.192151</td>\n",
              "      <td>1.194020</td>\n",
              "      <td>1.257920</td>\n",
              "      <td>1.259226</td>\n",
              "      <td>relu</td>\n",
              "      <td>20</td>\n",
              "      <td>0</td>\n",
              "      <td>100</td>\n",
              "      <td>64</td>\n",
              "      <td>3</td>\n",
              "      <td>mse</td>\n",
              "      <td>0.001</td>\n",
              "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
              "      <td>brick</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>03/29/20-225825</td>\n",
              "      <td>03/29/20-225931</td>\n",
              "      <td>66.415177</td>\n",
              "      <td>100</td>\n",
              "      <td>0.951792</td>\n",
              "      <td>0.953028</td>\n",
              "      <td>0.983768</td>\n",
              "      <td>0.984797</td>\n",
              "      <td>relu</td>\n",
              "      <td>20</td>\n",
              "      <td>0</td>\n",
              "      <td>100</td>\n",
              "      <td>32</td>\n",
              "      <td>2</td>\n",
              "      <td>mse</td>\n",
              "      <td>0.001</td>\n",
              "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.g...</td>\n",
              "      <td>brick</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>03/29/20-225932</td>\n",
              "      <td>03/29/20-230039</td>\n",
              "      <td>67.255532</td>\n",
              "      <td>100</td>\n",
              "      <td>1.685242</td>\n",
              "      <td>4.186103</td>\n",
              "      <td>1.722976</td>\n",
              "      <td>4.352967</td>\n",
              "      <td>relu</td>\n",
              "      <td>20</td>\n",
              "      <td>0</td>\n",
              "      <td>100</td>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "      <td>mae</td>\n",
              "      <td>0.001</td>\n",
              "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.n...</td>\n",
              "      <td>brick</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>03/29/20-230039</td>\n",
              "      <td>03/29/20-230143</td>\n",
              "      <td>63.983262</td>\n",
              "      <td>100</td>\n",
              "      <td>2.993923</td>\n",
              "      <td>2.997629</td>\n",
              "      <td>3.319123</td>\n",
              "      <td>3.320611</td>\n",
              "      <td>relu</td>\n",
              "      <td>20</td>\n",
              "      <td>0</td>\n",
              "      <td>100</td>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "      <td>mse</td>\n",
              "      <td>0.001</td>\n",
              "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
              "      <td>brick</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>03/29/20-230143</td>\n",
              "      <td>03/29/20-230258</td>\n",
              "      <td>74.553571</td>\n",
              "      <td>100</td>\n",
              "      <td>1.778293</td>\n",
              "      <td>1.780283</td>\n",
              "      <td>1.760582</td>\n",
              "      <td>1.762643</td>\n",
              "      <td>elu</td>\n",
              "      <td>20</td>\n",
              "      <td>0</td>\n",
              "      <td>100</td>\n",
              "      <td>32</td>\n",
              "      <td>3</td>\n",
              "      <td>mse</td>\n",
              "      <td>0.001</td>\n",
              "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
              "      <td>brick</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>03/29/20-230258</td>\n",
              "      <td>03/29/20-230403</td>\n",
              "      <td>65.255099</td>\n",
              "      <td>100</td>\n",
              "      <td>0.650136</td>\n",
              "      <td>0.650901</td>\n",
              "      <td>0.694784</td>\n",
              "      <td>0.696294</td>\n",
              "      <td>elu</td>\n",
              "      <td>20</td>\n",
              "      <td>0</td>\n",
              "      <td>100</td>\n",
              "      <td>32</td>\n",
              "      <td>2</td>\n",
              "      <td>mse</td>\n",
              "      <td>0.001</td>\n",
              "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.g...</td>\n",
              "      <td>brick</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>03/29/20-230403</td>\n",
              "      <td>03/29/20-230517</td>\n",
              "      <td>74.075158</td>\n",
              "      <td>100</td>\n",
              "      <td>0.810759</td>\n",
              "      <td>0.811942</td>\n",
              "      <td>0.903872</td>\n",
              "      <td>0.905537</td>\n",
              "      <td>relu</td>\n",
              "      <td>20</td>\n",
              "      <td>0</td>\n",
              "      <td>100</td>\n",
              "      <td>64</td>\n",
              "      <td>2</td>\n",
              "      <td>mse</td>\n",
              "      <td>0.001</td>\n",
              "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.n...</td>\n",
              "      <td>brick</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>03/29/20-230518</td>\n",
              "      <td>03/29/20-230625</td>\n",
              "      <td>66.990948</td>\n",
              "      <td>100</td>\n",
              "      <td>1.133847</td>\n",
              "      <td>2.307838</td>\n",
              "      <td>1.161780</td>\n",
              "      <td>2.417095</td>\n",
              "      <td>relu</td>\n",
              "      <td>20</td>\n",
              "      <td>0</td>\n",
              "      <td>100</td>\n",
              "      <td>64</td>\n",
              "      <td>2</td>\n",
              "      <td>mae</td>\n",
              "      <td>0.001</td>\n",
              "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.g...</td>\n",
              "      <td>brick</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>03/29/20-230625</td>\n",
              "      <td>03/29/20-230729</td>\n",
              "      <td>64.115000</td>\n",
              "      <td>100</td>\n",
              "      <td>1.656450</td>\n",
              "      <td>4.340416</td>\n",
              "      <td>1.695170</td>\n",
              "      <td>4.620042</td>\n",
              "      <td>relu</td>\n",
              "      <td>20</td>\n",
              "      <td>0</td>\n",
              "      <td>100</td>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "      <td>mae</td>\n",
              "      <td>0.001</td>\n",
              "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
              "      <td>brick</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>03/29/20-230729</td>\n",
              "      <td>03/29/20-230833</td>\n",
              "      <td>63.598839</td>\n",
              "      <td>100</td>\n",
              "      <td>0.820775</td>\n",
              "      <td>0.821799</td>\n",
              "      <td>0.830885</td>\n",
              "      <td>0.831727</td>\n",
              "      <td>relu</td>\n",
              "      <td>20</td>\n",
              "      <td>0</td>\n",
              "      <td>100</td>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "      <td>mse</td>\n",
              "      <td>0.001</td>\n",
              "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.g...</td>\n",
              "      <td>brick</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>03/29/20-230833</td>\n",
              "      <td>03/29/20-230945</td>\n",
              "      <td>71.902190</td>\n",
              "      <td>100</td>\n",
              "      <td>1.216365</td>\n",
              "      <td>1.218418</td>\n",
              "      <td>1.320451</td>\n",
              "      <td>1.321589</td>\n",
              "      <td>relu</td>\n",
              "      <td>20</td>\n",
              "      <td>0</td>\n",
              "      <td>100</td>\n",
              "      <td>32</td>\n",
              "      <td>2</td>\n",
              "      <td>mse</td>\n",
              "      <td>0.001</td>\n",
              "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.n...</td>\n",
              "      <td>brick</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>03/29/20-230945</td>\n",
              "      <td>03/29/20-231053</td>\n",
              "      <td>68.093326</td>\n",
              "      <td>100</td>\n",
              "      <td>1.239378</td>\n",
              "      <td>2.520447</td>\n",
              "      <td>1.253259</td>\n",
              "      <td>2.684544</td>\n",
              "      <td>elu</td>\n",
              "      <td>20</td>\n",
              "      <td>0</td>\n",
              "      <td>100</td>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "      <td>mae</td>\n",
              "      <td>0.001</td>\n",
              "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.n...</td>\n",
              "      <td>brick</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>03/29/20-231053</td>\n",
              "      <td>03/29/20-231201</td>\n",
              "      <td>67.728189</td>\n",
              "      <td>100</td>\n",
              "      <td>3.849401</td>\n",
              "      <td>3.854039</td>\n",
              "      <td>3.922625</td>\n",
              "      <td>3.924561</td>\n",
              "      <td>elu</td>\n",
              "      <td>20</td>\n",
              "      <td>0</td>\n",
              "      <td>100</td>\n",
              "      <td>64</td>\n",
              "      <td>1</td>\n",
              "      <td>mse</td>\n",
              "      <td>0.001</td>\n",
              "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
              "      <td>brick</td>\n",
              "      <td>14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>03/29/20-231201</td>\n",
              "      <td>03/29/20-231308</td>\n",
              "      <td>66.549669</td>\n",
              "      <td>100</td>\n",
              "      <td>0.858888</td>\n",
              "      <td>1.481252</td>\n",
              "      <td>0.897093</td>\n",
              "      <td>1.595400</td>\n",
              "      <td>elu</td>\n",
              "      <td>20</td>\n",
              "      <td>0</td>\n",
              "      <td>100</td>\n",
              "      <td>32</td>\n",
              "      <td>2</td>\n",
              "      <td>mae</td>\n",
              "      <td>0.001</td>\n",
              "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.g...</td>\n",
              "      <td>brick</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>03/29/20-231308</td>\n",
              "      <td>03/29/20-231428</td>\n",
              "      <td>80.117752</td>\n",
              "      <td>100</td>\n",
              "      <td>0.594064</td>\n",
              "      <td>0.715449</td>\n",
              "      <td>0.601183</td>\n",
              "      <td>0.760146</td>\n",
              "      <td>elu</td>\n",
              "      <td>20</td>\n",
              "      <td>0</td>\n",
              "      <td>100</td>\n",
              "      <td>64</td>\n",
              "      <td>3</td>\n",
              "      <td>mae</td>\n",
              "      <td>0.001</td>\n",
              "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
              "      <td>brick</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>03/29/20-231428</td>\n",
              "      <td>03/29/20-231547</td>\n",
              "      <td>78.452853</td>\n",
              "      <td>100</td>\n",
              "      <td>0.718989</td>\n",
              "      <td>0.719961</td>\n",
              "      <td>0.826072</td>\n",
              "      <td>0.827515</td>\n",
              "      <td>relu</td>\n",
              "      <td>20</td>\n",
              "      <td>0</td>\n",
              "      <td>100</td>\n",
              "      <td>64</td>\n",
              "      <td>3</td>\n",
              "      <td>mse</td>\n",
              "      <td>0.001</td>\n",
              "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.n...</td>\n",
              "      <td>brick</td>\n",
              "      <td>17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>03/29/20-231547</td>\n",
              "      <td>03/29/20-231659</td>\n",
              "      <td>72.048239</td>\n",
              "      <td>100</td>\n",
              "      <td>0.957634</td>\n",
              "      <td>0.959247</td>\n",
              "      <td>1.030138</td>\n",
              "      <td>1.031671</td>\n",
              "      <td>relu</td>\n",
              "      <td>20</td>\n",
              "      <td>0</td>\n",
              "      <td>100</td>\n",
              "      <td>64</td>\n",
              "      <td>2</td>\n",
              "      <td>mse</td>\n",
              "      <td>0.001</td>\n",
              "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
              "      <td>brick</td>\n",
              "      <td>18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>03/29/20-231659</td>\n",
              "      <td>03/29/20-231815</td>\n",
              "      <td>75.594268</td>\n",
              "      <td>100</td>\n",
              "      <td>0.549227</td>\n",
              "      <td>0.643474</td>\n",
              "      <td>0.550153</td>\n",
              "      <td>0.638903</td>\n",
              "      <td>elu</td>\n",
              "      <td>20</td>\n",
              "      <td>0</td>\n",
              "      <td>100</td>\n",
              "      <td>64</td>\n",
              "      <td>2</td>\n",
              "      <td>mae</td>\n",
              "      <td>0.001</td>\n",
              "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.n...</td>\n",
              "      <td>brick</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>03/29/20-231815</td>\n",
              "      <td>03/29/20-231923</td>\n",
              "      <td>67.924126</td>\n",
              "      <td>100</td>\n",
              "      <td>2.321051</td>\n",
              "      <td>2.324641</td>\n",
              "      <td>2.413426</td>\n",
              "      <td>2.416014</td>\n",
              "      <td>relu</td>\n",
              "      <td>20</td>\n",
              "      <td>0</td>\n",
              "      <td>100</td>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "      <td>mse</td>\n",
              "      <td>0.001</td>\n",
              "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.n...</td>\n",
              "      <td>brick</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              start              end  ...  shapes  Unnamed: 19\n",
              "0   03/29/20-225448  03/29/20-225602  ...   brick            0\n",
              "1   03/29/20-225602  03/29/20-225706  ...   brick            1\n",
              "2   03/29/20-225706  03/29/20-225825  ...   brick            2\n",
              "3   03/29/20-225825  03/29/20-225931  ...   brick            3\n",
              "4   03/29/20-225932  03/29/20-230039  ...   brick            4\n",
              "5   03/29/20-230039  03/29/20-230143  ...   brick            5\n",
              "6   03/29/20-230143  03/29/20-230258  ...   brick            6\n",
              "7   03/29/20-230258  03/29/20-230403  ...   brick            7\n",
              "8   03/29/20-230403  03/29/20-230517  ...   brick            8\n",
              "9   03/29/20-230518  03/29/20-230625  ...   brick            9\n",
              "10  03/29/20-230625  03/29/20-230729  ...   brick           10\n",
              "11  03/29/20-230729  03/29/20-230833  ...   brick           11\n",
              "12  03/29/20-230833  03/29/20-230945  ...   brick           12\n",
              "13  03/29/20-230945  03/29/20-231053  ...   brick           13\n",
              "14  03/29/20-231053  03/29/20-231201  ...   brick           14\n",
              "15  03/29/20-231201  03/29/20-231308  ...   brick           15\n",
              "16  03/29/20-231308  03/29/20-231428  ...   brick           16\n",
              "17  03/29/20-231428  03/29/20-231547  ...   brick           17\n",
              "18  03/29/20-231547  03/29/20-231659  ...   brick           18\n",
              "19  03/29/20-231659  03/29/20-231815  ...   brick           19\n",
              "20  03/29/20-231815  03/29/20-231923  ...   brick           20\n",
              "\n",
              "[21 rows x 19 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 411
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WAueX6UW88i",
        "colab_type": "code",
        "outputId": "212fbf35-2bc2-4570-a47e-75f8014cf28c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# make predictions with the model\n",
        "houses_talos.model.predict(houses_talos.x)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.30563876],\n",
              "       [ 0.29604703],\n",
              "       [ 0.8784391 ],\n",
              "       [ 0.02225883],\n",
              "       [ 0.41520113],\n",
              "       [ 0.9926022 ],\n",
              "       [-0.0527037 ],\n",
              "       [ 0.35138342],\n",
              "       [ 0.23030892],\n",
              "       [ 0.6004959 ],\n",
              "       [ 0.47182325],\n",
              "       [ 0.56174946],\n",
              "       [-0.11169381],\n",
              "       [ 1.5043579 ],\n",
              "       [-0.18459125],\n",
              "       [ 1.0034773 ],\n",
              "       [-0.19626853],\n",
              "       [-0.22220203],\n",
              "       [ 0.76712906],\n",
              "       [ 0.5829014 ],\n",
              "       [ 0.8073291 ],\n",
              "       [ 0.5863644 ],\n",
              "       [ 1.3964181 ],\n",
              "       [ 0.4380223 ],\n",
              "       [ 0.64853615],\n",
              "       [ 0.2092759 ],\n",
              "       [ 0.74188346],\n",
              "       [ 0.32738543],\n",
              "       [ 0.77272815],\n",
              "       [-0.26831818],\n",
              "       [-0.30281   ],\n",
              "       [ 0.6465046 ],\n",
              "       [ 3.1052423 ],\n",
              "       [ 0.47534296],\n",
              "       [-0.03845235],\n",
              "       [-0.36917505],\n",
              "       [ 0.48842555],\n",
              "       [ 1.083488  ],\n",
              "       [-0.10115838],\n",
              "       [ 0.6692639 ],\n",
              "       [ 1.0117623 ],\n",
              "       [ 0.53758216],\n",
              "       [ 0.13057032],\n",
              "       [-0.30540386],\n",
              "       [-0.05971636],\n",
              "       [ 0.8768675 ],\n",
              "       [ 0.57984835],\n",
              "       [ 0.08075678],\n",
              "       [ 0.66651756],\n",
              "       [ 0.14272946],\n",
              "       [ 0.3585501 ],\n",
              "       [ 0.6501897 ],\n",
              "       [ 0.26653296],\n",
              "       [-0.26663175],\n",
              "       [ 0.1850028 ],\n",
              "       [-0.28480014],\n",
              "       [ 0.13709038],\n",
              "       [ 0.23313902],\n",
              "       [ 0.71529794],\n",
              "       [ 1.0974145 ],\n",
              "       [ 0.40387675],\n",
              "       [ 0.7229369 ],\n",
              "       [-0.12932464],\n",
              "       [ 0.18909071],\n",
              "       [ 0.60590154],\n",
              "       [-0.20814504],\n",
              "       [ 0.34274295],\n",
              "       [ 0.7638492 ],\n",
              "       [ 1.1346874 ],\n",
              "       [-0.05710026],\n",
              "       [ 1.4869176 ],\n",
              "       [ 0.43424645],\n",
              "       [ 0.25376958],\n",
              "       [ 0.6281095 ],\n",
              "       [ 0.40070486],\n",
              "       [ 0.11574638],\n",
              "       [-0.00998683],\n",
              "       [ 0.29836965],\n",
              "       [ 0.70528555],\n",
              "       [ 0.6303157 ],\n",
              "       [-0.16274004],\n",
              "       [ 0.18653505],\n",
              "       [ 0.66064876],\n",
              "       [ 0.98368603],\n",
              "       [ 0.33759964],\n",
              "       [ 0.04332561],\n",
              "       [ 0.36566338],\n",
              "       [ 0.83222777],\n",
              "       [ 0.99970585],\n",
              "       [ 1.0290539 ],\n",
              "       [ 1.583057  ],\n",
              "       [ 0.00819515],\n",
              "       [ 1.6077311 ],\n",
              "       [-0.12921274],\n",
              "       [-0.13630812],\n",
              "       [-0.2067218 ],\n",
              "       [ 0.39350164],\n",
              "       [ 0.77251416],\n",
              "       [-0.34038135],\n",
              "       [-0.22296226]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 412
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fk7PjGhLdGh",
        "colab_type": "text"
      },
      "source": [
        "## IN CLASS EXERCISE 1: Hyperparameter tuning using Talos\n",
        "\n",
        "Build a network in Keras (fitted for Talos) in order to train MNIST with an MLP. You will need to add a last layer with softmax and shape of 10 for output. Decide a dictionary of hyperparameters and test that it runs with Talos Scan. \n",
        "\n",
        "Try to achieve good results. Try to share the deployed model with a colleage and run it in their computer.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXI7LCArbnGj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8fAIFC2Z8ev",
        "colab_type": "text"
      },
      "source": [
        "# Vanishing/Exploding gradients"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDuDkUGEQfGt",
        "colab_type": "text"
      },
      "source": [
        "## Xavier and He Initialization\n",
        "\n",
        "UUID - #S3C2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZ9BJHXWQfGu",
        "colab_type": "code",
        "outputId": "9cfc1873-9107-4648-c62c-5c138aa9b755",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "[name for name in dir(keras.initializers) if not name.startswith(\"_\")]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Constant',\n",
              " 'GlorotNormal',\n",
              " 'GlorotUniform',\n",
              " 'Identity',\n",
              " 'Initializer',\n",
              " 'Ones',\n",
              " 'Orthogonal',\n",
              " 'RandomNormal',\n",
              " 'RandomUniform',\n",
              " 'TruncatedNormal',\n",
              " 'VarianceScaling',\n",
              " 'Zeros',\n",
              " 'constant',\n",
              " 'deserialize',\n",
              " 'get',\n",
              " 'glorot_normal',\n",
              " 'glorot_uniform',\n",
              " 'he_normal',\n",
              " 'he_uniform',\n",
              " 'identity',\n",
              " 'lecun_normal',\n",
              " 'lecun_uniform',\n",
              " 'ones',\n",
              " 'orthogonal',\n",
              " 'serialize',\n",
              " 'zeros']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOlyYSGbQfGz",
        "colab_type": "code",
        "outputId": "992d1c4f-5c29-4f9f-9ce1-41e257825df1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "keras.layers.Dense(10, activation=\"relu\", kernel_initializer=\"he_normal\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.layers.core.Dense at 0x7fe69eec0898>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYAZWwxYQfG5",
        "colab_type": "code",
        "outputId": "ea541c1f-5005-4845-86cc-87ce5e900110",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "init = keras.initializers.VarianceScaling(scale=2., mode='fan_avg',\n",
        "                                          distribution='uniform')\n",
        "keras.layers.Dense(10, activation=\"relu\", kernel_initializer=init)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.layers.core.Dense at 0x7fe69edb9c88>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8PSa5VfQfHA",
        "colab_type": "text"
      },
      "source": [
        "## Nonsaturating Activation Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUF67BcWQfHC",
        "colab_type": "text"
      },
      "source": [
        "### Leaky ReLU\n",
        "\n",
        "UUID - #S3C3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nveAwfnzQfHE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def leaky_relu(z, alpha=0.01):\n",
        "    return np.maximum(alpha*z, z)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUf9sQifQfHI",
        "colab_type": "code",
        "outputId": "e8f660ac-2b35-4180-832c-06547f6e4403",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        }
      },
      "source": [
        "z = np.linspace(-5, 5, 200)\n",
        "\n",
        "plt.plot(z, leaky_relu(z, 0.05), \"b-\", linewidth=2)\n",
        "plt.plot([-5, 5], [0, 0], 'k-')\n",
        "plt.plot([0, 0], [-0.5, 4.2], 'k-')\n",
        "plt.grid(True)\n",
        "props = dict(facecolor='black', shrink=0.1)\n",
        "plt.annotate('Leak', xytext=(-3.5, 0.5), xy=(-5, -0.2), arrowprops=props, fontsize=14, ha=\"center\")\n",
        "plt.title(\"Leaky ReLU activation function\", fontsize=14)\n",
        "plt.axis([-5, 5, -0.5, 4.2])\n",
        "\n",
        "save_fig(\"leaky_relu_plot\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving figure leaky_relu_plot\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nHnKHvAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvJclMRPQfHN",
        "colab_type": "code",
        "outputId": "06040cf2-0991-4545-885e-bfd789792fda",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "[m for m in dir(keras.activations) if not m.startswith(\"_\")]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['deserialize',\n",
              " 'elu',\n",
              " 'exponential',\n",
              " 'get',\n",
              " 'hard_sigmoid',\n",
              " 'linear',\n",
              " 'relu',\n",
              " 'selu',\n",
              " 'serialize',\n",
              " 'sigmoid',\n",
              " 'softmax',\n",
              " 'softplus',\n",
              " 'softsign',\n",
              " 'swish',\n",
              " 'tanh']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZCaGM9fQfHS",
        "colab_type": "code",
        "outputId": "e0eb8f20-fe36-429b-d053-da34d9e23db8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "[m for m in dir(keras.layers) if \"relu\" in m.lower()]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['LeakyReLU', 'PReLU', 'ReLU', 'ThresholdedReLU']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TE_VxvKIQfH4",
        "colab_type": "text"
      },
      "source": [
        "### ELU\n",
        "\n",
        "UUID - #S3C4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zk2aB1y6QfH5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def elu(z, alpha=1):\n",
        "    return np.where(z < 0, alpha * (np.exp(z) - 1), z)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjO8taRVQfH9",
        "colab_type": "code",
        "outputId": "548b90d7-857d-4531-b036-4c399f277abc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        }
      },
      "source": [
        "plt.plot(z, elu(z), \"b-\", linewidth=2)\n",
        "plt.plot([-5, 5], [0, 0], 'k-')\n",
        "plt.plot([-5, 5], [-1, -1], 'k--')\n",
        "plt.plot([0, 0], [-2.2, 3.2], 'k-')\n",
        "plt.grid(True)\n",
        "plt.title(r\"ELU activation function ($\\alpha=1$)\", fontsize=14)\n",
        "plt.axis([-5, 5, -2.2, 3.2])\n",
        "\n",
        "save_fig(\"elu_plot\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving figure elu_plot\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3deZgU1dn38e8Ng+wgiI4LIkYFNURI\nmOSJGnViiALBYNTgHtEYCIRXiZqovOjja3g0GkwwKigGHyLggrgEkcUltogSFXEIEAFBZFX2BoZt\nmJnz/nF6cOhZm6mZqp7+fa6rrump013n7jM1fXedOnXKnHOIiIhETYOwAxARESmPEpSIiESSEpSI\niESSEpSIiESSEpSIiESSEpSIiESSEpSIiESSEpSIiESSEpSkHTMbb2bT6lE9DczsCTPbYmbOzHJr\nu85KYqmT95yoq42ZbTCzk+qivlSZ2QtmdmvYcWQy00wS9ZuZjQeuK6foA+fc9xPl7ZxzfSp4fQxY\n5JwbkrS+P/Coc65FoAFXr+7W+H03nk71VFJ/H+AlIBf4HNjqnCuozToT9cZIet919Z4Tdf0Jv+9d\nX9t1lVP3ucBtQHfgWOB659z4pOd8C3gHONE5t72uYxTICjsAqRNvAtcmrav1D8DaUlcfFnX4oXQy\n8KVz7v06qq9CdfWezawZcCNwUV3UV44WwCLg6cRShnNuoZl9DlwDPFaHsUmCuvgywz7n3FdJy9ba\nrtTMeprZu2a2zcy2mtksMzutVLmZ2a1m9pmZ7TOztWZ2f6JsPHAe8JtEt5czs44lZWY2zcwGJLqI\nGibV+4yZTa1OHNWpp9R2GpvZqESde83sX2b2g1LlMTMbbWb3mdlmM9toZiPNrML/s0T9fwE6JOr+\notS2Hk1+bkk81anrUNo31fd8qO8b6A044L1y2qS7mb1lZnvMbLmZnWtm/cyszHMPlXNuunNumHNu\nClBcyVOnAlcGVa+kRglKalNzYBTwPXz31XbgVTM7LFF+H3AXcD/wTeDnwJpE2c3AXOB/gWMSS0lZ\niReA1sCPS1aYWQugLzCxmnFUp54SDwKXAzcA3wYWAjPN7JhSz7kaKATOAoYAQxOvqcjNwL3A2kTd\n363kucmqqqum7QvVe8/ViSXZOcDHLukcg5l9F3gXeBs4A/gX8P+A/5t4LyQ9f5iZ5VexnFNJHFX5\nEPiemTWtwTbkEKmLLzP0NLP8pHWPOedur81KnXMvlv7dzK4HduD/4fOA3wJDnXNPJZ6yHP+hiXNu\nu5kVALudc19VsP1tZjYd/+E4M7H6YvwH5dRSz6swDufcnKrqSbymOTAIuNE591pi3a+B84HfAMMT\nT/2Pc+7uxONlZvYr4EfAsxW8h+1mthMoqqz+ClRYVyJRp9y+ZnYo7znl9w2cAKwvZ/1DwKvOuRGJ\n+p4BXgVmO+f+Wc7zHwcmV1BHiXVVlFdmPdAIf55qRQ22I4dACSozzAYGJK2ri5PgJwF/AP4LOBJ/\nxN4A6IA/B9YYeKuG1UwE/m5mzZxzu/HJ6kXn3N5qxlFdJ+E/qA50MznnisxsLnB6qef9O+l164Gj\nUqgnFZXVdTo1b9/qvueqYilPU2BD6RVmdjT+yOqHpVYX4P9WZY6eEvFsBWqzu3pP4qeOoEKgBJUZ\ndjvnlh/ia3fgu9GSHY7vKqvMNHzX1UD8t9hC4D/AYZW9KEWvJbbb18zeAnoAF9ZxHKW7qfaXU3Yo\nXenFgCWta5T0e1B1HYrk4b+pxrIZaJO0ruT85LxS6zoDS51zc8rbiJkNA4ZVHiq9nHPvVvGcirRN\n/Nx0iK+XGlCCkqosBXqbmSWdL/hOoqxcZnYEcCow2Dn3dmLdd/h6n/sU2IfvBvqsgs0UAA0rKAPA\nObfPzF7AHzm1A74CYinEUa168N07BcDZiceYH5xxJvBMFa89FJvw54VK6wp8Uc3XB9G+tfmePwH6\nJ607HJ/YihJ1tcSfe6qs67O2u/i6AOuccxuqfKYETgkqMzROdJ+UVuScK/lW2MrMuiWVx51zXwBj\n8Ce9HzGzJ4G9+BFYVwI/raTObfhvyb8yszXAccCf8EcvOOd2mtnDwP1mtg/fDXkE0N05NyaxjS/w\n56s6Avn464PKG3E1Ed+VdSLwbNJzKo2juvU453aZ2RjgATPbDKzEn+PJBkZX0g6H6p/AKDP7Kf6L\nwEDgeKqZoA61fZO2UZvveVZiu0c457Yk1uXhjxrvNLNJ+L/Tl8DJZnaKc65Moj3ULr7EObqTE782\nwI+i7Ib/268u9dRzErFKCDSKLzP0wP+jl14+KVV+TuL30stIAOfc58C5wCnA6/hRTVcAP3fOzaio\nwsQH/OX4kViL8NeR3IX/Vl/iTuCBxPpPgReB9qXKR+K/wf8Hf0RR0Tmjd/Hfkk/n4NF71Y2juvXc\nDjyPH/mWl9hmT+fclxU8vyaeKrW8B+wEXk5xG0G0b628Z+fcQr7el0rWrcQfMQ0CFuDfcw/83y3o\na8Ry+Hpfb4ofKfgJfkQlAGbWBPgZ8GTAdUs1aSYJEQmFmfUEHgZOd84VhR1PMjP7DdDXOXdB2LFk\nKh1BiUgonHMz8Ue07at6bkj2A/8n7CAymY6gREQkknQEJSIikaQEJSIikRT6MPN27dq5jh07hh1G\nGbt27aJ58+Zhh5F21G6pWbp0KUVFRZx+evLEDFKZdNrPnIPly2HHDjjsMDj1VGiUfMl1HYhym338\n8cebnXNHJq8PPUF17NiRefPmVf3EOhaLxcjNzQ07jLSjdktNbm4u8Xg8kv8DUZYu+1lxMVx1Fcyf\nD0cdBXPmwCmnhBNLlNvMzFaVt15dfCIitcA5uPlmeP55aNkSZswILzmlKyUoEZFaMGIEPPqo79b7\nxz/gO98JO6L0owQlIhKwxx+Hu++GBg3g2Wfhhz+s+jVSVqAJyswmmtmXZrbDzJaZ2Y1Bbl9EJOqm\nTIHBg/3jMWPgkkvCjSedBX0EdT/Q0TnXCj+R6Agz6x5wHSIikfTWW3D11f7804gRMCD5LmySkkAT\nlHNusXOuZBJOl1hOCrIOEZEo+vhjuPhiKCiAm26CYVXdpUqqFPgwczMbjb/PS1P87MDTy3nOABJ3\neM3OziYWiwUdRo3l5+dHMq6oU7ulJh6PU1RUpDZLUdT2szVrmnLTTd8mP/8wzj9/A337fso774Qd\n1cGi1mbVUStz8ZW6qVku8IBzLvlumwfk5OS4KF4DEuVrBqJM7Zaakuug8vLywg4lrURpP1u/Hs46\nC1atggsvhKlT/ci9qIlSmyUzs4+dcznJ62tlFJ9zrihxi+b2+Hu7iIjUO9u2+aS0ahX813/Biy9G\nMzmlq9oeZp6FzkGJSD20ezdcdBEsWgSnnQavvQYRnUkobQWWoMzsKDO7wsxamFlDM7sQf1vwt4Kq\nQ0QkCvbvh3794L33oH17mDULjjgi7KjqnyAHSTh8d97j+MS3ChjqnJsaYB0iIqEqLoYbb/RHTG3b\nwuuvw/HHhx1V/RRYgnLObQLOC2p7IiJRdPvt8PTT0KwZTJ/uu/ekdmiqIxGRavrTn2DkSMjKgpde\n8gMjpPYoQYmIVMP//i/8/vf+8dNP+9F7UruUoEREqjB1KvzqV/7xww/DlVeGG0+mUIISEanEu+/C\n5ZdDUREMH+6nMZK6oQQlIlKBf//bX+u0d6+f+PXee8OOKLMoQYmIlGPlSn+eaft2f8uM0aPBLOyo\nMosSlIhIkg0b4IIL4Kuv/M0GJ02Chg3DjirzKEGJiJSyYwf06gXLl8O3vw2vvAJNmoQdVWZSghIR\nSdi719/T6ZNP4OSTYcYMaNUq7KgylxKUiAh+lN7VV8Pbb8PRR/spjLKzw44qsylBiUjGcw4GD/az\nQ7Ru7Sd/PfHEsKMSJSgRyXh33w1jx/pzTa++CmecEXZEAkpQIpLh/vpXGDHCj9KbPBnOOSfsiKSE\nEpSIZKxnnoGbb/aP//Y3f1GuRIcSlIhkpJkz4brr/OMHH4T+/UMNR8qhBCUiGeeDD+DSS6GwEG67\nDX73u7AjkvIoQYlIRvn0U+jdG3bv9kdQDzwQdkRSESUoEckYa9b4KYy2boU+feDJJ6GBPgUjS38a\nEckIW7b45LR2LZx9Njz/PDRqFHZUUhklKBGp9/Lz4Sc/gSVLoEsXf61Ts2ZhRyVVUYISkXqtoAAu\nu8wPjDjhBD9LRJs2YUcl1aEEJSL1VnGxHz4+axYceaSfX+/YY8OOSqpLCUpE6iXnYOhQePZZaNHC\nz0zeqVPYUUkqlKBEpF667z545BE47DD4xz+ge/ewI5JUKUGJSL0zdiwMH+5v0T5pEpx/ftgRyaFQ\nghKReuXFF2HQIP949Gg/QELSkxKUiNQbb78NV13lB0fcey/8+tdhRyQ1oQQlIvXC/PnQt68fVj5k\niO/ik/SmBCUiae+zz6BnT9i5E664Ah5+2J9/kvSmBCUiaW39ej+F0aZN/uff/6759eoL/RlFJG3F\n4/7I6Ysv4Hvf8wMkDjss7KgkKEpQIpKW9uzxd8BduBA6d4bXXvMX5Er9EViCMrPGZjbOzFaZ2U4z\nyzOzXkFtX0SkRFGRcfnlMGcOHHecn8KoXbuwo5KgBXkElQWsAc4DWgPDgclm1jHAOkQkwzkHI0d2\n4tVX/aSvr78OHTqEHZXUhqygNuSc2wXcU2rVNDNbCXQHvgiqHhHJbHfcATNnHkOzZr5b7/TTw45I\nakutnYMys2ygE7C4tuoQkcwyciQ8+CA0bFjMlClw5plhRyS1KbAjqNLMrBEwCfi7c25JOeUDgAEA\n2dnZxGKx2gijRvLz8yMZV9Sp3VITj8cpKipSm1XDzJnZPPDAaQAMHbqApk23o2arvnT83zTnXLAb\nNGsAPAO0Avo65/ZX9vycnBw3b968QGMIQiwWIzc3N+ww0o7aLTW5ubnE43Hy8vLCDiXSpk2Diy+G\noiIYNQq6dtV+lqoo/2+a2cfOuZzk9YF28ZmZAeOAbODSqpKTiEhV5syBn//cJ6dhw+Dmm8OOSOpK\n0F18Y4DTgB7OuT0Bb1tEMszChf5ap7174cYbYcSIsCOSuhTkdVAnAAOBbsBXZpafWK4Oqg4RyRxf\nfAEXXuhni/jZz2DMGM2vl2mCHGa+CtDuIyI1tnGjn1fvyy/hvPPgmWcgq1aGdEmUaaojEYmUHTug\nVy8/Q3m3bv527U2ahB2VhEEJSkQiY98+3503fz6cdBLMnAmtW4cdlYRFCUpEIqGoCK65Bv75Tzj6\naD+FUXZ22FFJmJSgRCR0zsFvfgNTpkCrVv7I6RvfCDsqCZsSlIiE7p574IknoHFjePVV6No17Igk\nCpSgRCRUjz4K997r74L7/PNw7rlhRyRRoQQlIqF57jm46Sb/+MknoW/fcOORaFGCEpFQvP46/OIX\n/vzTH/8IN9wQdkQSNUpQIlLnPvwQLrkE9u+HW26B3/8+7IgkipSgRKROLVkCvXvDrl1w7bXwpz9p\nCiMpnxKUiNSZtWv9FEZbtvgkNW6cHxwhUh7tGiJSJ7Zs8clpzRo46yx44QVo1CjsqCTKlKBEpNbt\n2gV9+sCnn8I3v+mvdWrWLOyoJOqUoESkVu3fD5ddBv/6F3ToALNmQdu2YUcl6UAJSkRqTXExXH+9\nn7qoXTs/tPy448KOStKFEpSI1Arn/BDySZOgRQuYMQM6dw47KkknSlAiUiv++Ed4+GE/EOLllyEn\nJ+yIJN0oQYlI4P72Nxg2zF/fNHEi9OgRdkSSjpSgRCRQL78MAwf6x489Bv36hRuPpC8lKBEJTCwG\nV17pB0fccw8MGhR2RJLOlKBEJBCffAI//am/bfvgwXD33WFHJOlOCUpEamz5cujZE3bu9F16f/2r\n5teTmlOCEpEa+fJLuPBC2LjRD4Z4+mlo2DDsqKQ+UIISkUMWj0OvXvD5534Y+Usv+du2iwRBCUpE\nDsmePf4OuAsWQKdOMH06tGwZdlRSnyhBiUjKCgv9aL3Zs+HYY/0URkceGXZUUt8oQYlISpzz1zn9\n4x/Qpo1PTiecEHZUUh8pQYlISoYNg6eegqZNYdo0f/sMkdqgBCUi1fbnP/s59ho2hClT/I0HRWqL\nEpSIVMuECXDrrf7x+PH+lu0itUkJSkSq9Npr/r5OAH/5C1xzTbjxSGZQghKRSr3/Pvz851BUBHfe\nCUOHhh2RZAolKBGp0KJF8JOf+GuefvlL+J//CTsiySSBJigzG2Jm88xsn5mND3LbIlK3Vq3yUxjF\n43DxxfD445pfT+pWVsDbWw+MAC4Emga8bRGpI5s2wQUXwPr1cN558OyzkBX0p4VIFQLd5ZxzLwGY\nWQ7QPshti0jd2LnTj9Bbtgy6dvUX5DZpEnZUkolC+U5kZgOAAQDZ2dnEYrEwwqhUfn5+JOOKOrVb\nauLxOEVFRZFps4IC4847z2D+/DYce+we7r77Ez75pCDssMrQfpa6dGyzUBKUc24sMBYgJyfH5ebm\nhhFGpWKxGFGMK+rUbqk5/PDDicfjkWizoiI/v978+ZCdDbNnN+Wkk6J5Ja72s9SlY5tpFJ+I4Bzc\ndBO88AK0agUzZ8JJJ4UdlWQ6JSgR4d57YfRofy+nqVOhW7ewIxIJuIvPzLIS22wINDSzJkChc64w\nyHpEJDijR8M990CDBvDcc37UnkgUBH0ENRzYA9wBXJN4PDzgOkQkIJMnw5Ah/vHYsf56J5GoCHqY\n+T3APUFuU0Rqx5tv+jn1nIP77/czRYhEic5BiWSgjz7yR0v798Nvfwu33x52RCJlKUGJZJilS/2F\nuLt2+SOokSM1hZFEkxKUSAZZt85PYbR5M/Tq5e+M20CfAhJR2jVFMsTWrX7y19Wr4cwz/TVPjRqF\nHZVIxZSgRDLA7t3Qpw8sXgynnw7TpkHz5mFHJVI5JSiRem7/fn/DwblzoUMHmDUL2rYNOyqRqilB\nidRjxcVwww0wfTq0awevvw7tdZ8BSRNKUCL1lHNw220wcaLvzps+HTp3DjsqkepTghKppx58EP7y\nFz8Q4uWX4bvfDTsikdQoQYnUQ+PGwR13+OubJk6EH/847IhEUqcEJVLPvPIKDBjgHz/6KPTrF248\nIodKCUqkHpk9G664wg+O+O//hsGDw45I5NApQYnUEwsWwEUXwb59MGiQT1Ai6UwJSqQe+PxzP0vE\njh3+mqdHHtH8epL+lKBE0txXX/n59TZsgB/9CCZMgIYNw45KpOaUoETS2PbtftLXFSuge3c/nLxx\n47CjEgmGEpRImtq7F/r2hbw86NQJZsyAli3DjkokOEpQImmosBCuvBLeeQeOPdbPr3fkkWFHJRIs\nJSiRNOOcH6X3yitw+OE+OXXsGHZUIsFTghJJM8OHw9/+Bk2b+ttmdOkSdkQitUMJSiSNjBoF993n\nR+m98AKcfXbYEYnUHiUokTQxaRL89rf+8VNPwU9+Em48IrVNCUokDcyYAf37+8cPPQS/+EWo4YjU\nCSUokYibOxcuvdSP3Lv9drjllrAjEqkbSlAiEbZ4se/K27PH3xn3/vvDjkik7ihBiUTU6tV+fr1t\n2+CnP4UnntD8epJZlKBEImjzZj+/3rp1cM458NxzkJUVdlQidUsJSiRi8vOhd29YuhTOOAOmTvXX\nPIlkGiUokQgpKIBLLoGPPoITT4SZM/1sESKZSAlKJCKKi/3w8TfegKOOgtdfh2OOCTsqkfAoQYlE\ngHNw883w/PN+RvKZM+Hkk8OOSiRcSlAiETBiBDz6KBx2mD/n9O1vhx2RSPgCTVBm1tbMXjazXWa2\nysyuCnL7IvXRli2NuftuaNAAnn0WcnPDjkgkGoIeuPoYUABkA92A18xsgXNuccD1iNQLmzbB2rV+\niN7jj/sBEiLimXMumA2ZNQe2AV2cc8sS6yYA65xzd1T0upYtW7ru3bsHEkOQ4vE4h2v4VMrUbtW3\ndSssXJgHwIkndqNDh5ADSiPaz1IX5TZ75513PnbO5SSvD/IIqhNQWJKcEhYA5yU/0cwGAAMAGjVq\nRDweDzCMYBQVFUUyrqhTu1VPfn4Wn3/eHICsrGJatYqjZqs+7WepS8c2CzJBtQB2JK3bDrRMfqJz\nbiwwFiAnJ8fNmzcvwDCCEYvFyNXJgJSp3ao2bx6cf74fuXfMMbkcdVScvLy8sMNKK9rPUhflNrMK\n5vAKcpBEPtAqaV0rYGeAdYiktbw86NkTdu6EK66AU04JOyKR6AoyQS0Dssys9L9cV0ADJESADz+E\nH/4QtmyBPn3g6ac1+atIZQJLUM65XcBLwL1m1tzMzgb6AhOCqkMkXc2ZAz16QDwOF18MU6ZAo0Zh\nRyUSbUFfqDsYaApsBJ4FBmmIuWS6f/7T3zajpFtv8mRo3DjsqESiL9DroJxzW4GLg9ymSDp74QW4\n9lrYtw+uuw7GjYOGDcOOSiQ9aKojkVrgHIwcCf36+eQ0eDA89ZSSk0gqlKBEAlZYCEOGwO9+539/\n4AE/z14D/beJpET36BQJ0PbtcPXV8NprfuLXp5+Gyy8POyqR9KQEJRKQRYv8XHqffQZt28Irr/jb\ntYvIoVGng0gAJk+G73/fJ6euXf0dcZWcRGpGCUqkBvbtg1tu8d14u3b57r3334dvfCPsyETSn7r4\nRA7Rp5/CVVf56YuysuDPf/aDIzQ7hEgwlKBEUuQcPPGEP3Las8cfLU2a5Lv4RCQ46uITScHq1X4e\nvUGDfHK67jr45BMlJ5HaoAQlUg3FxfDYY/DNb8L06dC6tb89+/jx0Cp5Dn8RCYS6+ESqsHgx/PrX\nfsJX8EPJH30Ujjkm3LhE6jsdQYlUIB6HoUP9sPE5c+Doo/0s5C++qOQkUheUoESSFBXBk0/6mwk+\n/LAfFDFoEPznP3DppWFHJ5I51MUnkuCcn/1h+HCfjADOO88nqa5dw41NJBPpCEoynnPw1lt+JN4l\nl/jk1LEjPPccvP22kpNIWHQEJRnLOZgxA+67D957z6/Lzoa77oJf/cpP9ioi4VGCkoxTWAgvvQT3\n3+9ngQA/ueutt8LNN0Pz5uHGJyKeEpRkjK1b/eCHxx6DNWv8uqOPhttug4EDoUWLcOMTkYMpQUm9\n5hz861/+VuvPPONnfwDo1MkPIb/+emjSJNwYRaR8SlBSL331FUyY4G+zvmTJ1+t79oSbboILL9Qd\nbkWiTglK6o1du/ygh6ef9tMRFRX59dnZ8ItfwC9/CZ07hxujiFSfEpSktZ07/e3Vp0zxSamkCy8r\nCy6+GG64wR81NWoUbpwikjolKEk769bBrFkwdSrMnOlvGlji+9+Hfv38jQOPOiq8GEWk5pSgJPL2\n7fPXKc2c6ZeFC78uM/O3Vr/sMn+Rbfv24cUpIsFSgpLI2bcPPvoIZs/2y5w5/vxSiebN4fzzoVcv\n342niVtF6iclKAndhg0+IX3wAbz7rh8WXrrbDqBLF38uqWdP+MEPoHHjcGIVkbqjBCV1auNG30U3\nb55PSh995O9Sm6xLFzj3XL+ccw4ce2zdxyoi4VKCklqxa5e/0d/ChbBokf+5cKFPUMlatIDu3eG7\n3/VHRz/4ARxxRN3HLCLRogQlh2zfPli5Ej77zC/Ll8OHH57Bli2wapWfxSFZy5b+6KhbN/je9/zS\nuTM0bFj38YtItClBSbmcg+3b/Zx1a9b4brjSj1et8j+Li5Nf2Rbw1yGdeip861t+6dLF/zzhBD/y\nTkSkKkpQGaawEDZt8gMTNm70P0uWjRv9FEFr1/rkk59f+bYaNIATT/R3nj3lFDj5ZNiz599ccskZ\nnHiiblchIjWjBJWGnPMzJuzYAdu2+Vm6t22r+vHmzbBlS/ldb+Vp1gw6dIDjj/dL8uPyklAstlXT\nCYlIIAJJUGY2BOgPfAt41jnXP4jtpqviYp9A9u49+Gdl6/Lz/bJzZ+U/S5ayXWvVYwZHHulnWcjO\n/nop/Xv79j4JtWmj7jgRCU9QR1DrgRHAhUDTVF64bx8sW+Yn9iy9FBeXXRfU+sJC2L8fCgoO/ln6\n8bp1p/HII2XXV/S4oODrhLN/f0CtWokmTfyAg7ZtfSIpWSr7vV07v2TpuFlE0oC56vb3VGdjZiOA\n9qkcQZm1dNA9aW0/YDCwG+hdzqv6J5bNwGXllA8CLgfWANeWU34rcBGwFBhYTvlwoAeQBwwtp/w+\n4CzgfWBYOeWjaNq0Gw0bvklBwQgaNOCgpUuXJzjiiM5s2/Yqn332EA0a+FFsJcuNN06gQ4fjyct7\nnjfeGHNQWcOGMGXKFI4+uh3jx49n/PjxZWqfPn06zZo1Y/To0UyePLlMeSwWA2DkyJFMmzbtoLKm\nTZsyY8YMAP7whz/w1ltvHVR+xBFH8OKLLwJw5513Mnfu3IPKGzVqxBtvvAHA0KFDySu5ZW1Cp06d\nGDt2LAADBgxg2bJlB5V369aNUaNGAXDNNdewdu3ag8rPPPNM7r//fgAuvfRStmzZclD5j370I+66\n6y4AevXqxZ6S2WMT+vTpw2233QZAbm4uyfr168fgwYPZvXs3vXuX3ff69+9P//792bx5M5ddVnbf\nGzRoEJdffjlr1qzh2mvL7nu33norF110EUuXLmXgwIHk5eVRWFhITk4OAMOHD6dHjx7k5eUxdGjZ\nfe++++7jrLPO4v3332fYsLL73qhRo+jWrRtvvvkmI0aMKFP+xBNP0LlzZ1599VUeeuihMuUTJkzg\n+OOP5/nnn2fMmDFlyqdMmUK7duHve1dffTXr1q07qLx9+/ZMnDgR0L5X3r53wQUXMGzYsAP7XrIw\n97133nnnY+dcTvJrQvkubWYDgAH+t+YcdlhxoivJYQYtW+6lTZudwC7Wri1MvObr7qZ27fLJzt5C\nUdEWli3bf2B9yTaOPz7Occd9yb59G1iwoAAzV6ocTj11Ex07rmLXrrXMnbsXM3dg+2aOs89eRfv2\n88nPX8msWbsOrC95zs9+toROnRqxcuV/eOmlHQfWN2jgaNAAhgyZxymnxPn44wVMmBAv8/4HDvyA\nDh2+5P33F7JzZ9nyk06ay1FHrWDp0sVA/MCRX4kPPniP1q1bs2TJEuLxsq+fPXs2TZo0YdmyZeWW\nl3xIrFixokz5nj17DpSvXLmyTHlxcfGB8tWrV5cpb9OmzYHytWvXlilfv379gfL169eXKV+7du2B\n8g0bNpQpX7169YHyTZs2seUqZRIAAAX6SURBVGPHjoPKV65ceaB869at7EuakmLFihUHystrm2XL\nlhGLxdi7d2+55UuWLCEWi7F9+/ZyyxcvXkwsFmPjxo3lli9cuJCWLVseaLvCwkKccweeu2DBArKy\nsli+fHm5r58/fz4FBQUsWrSo3PJ58+YRj8dZsGBBueUffPABX375JQsXLiy3fO7cuaxYsYLFixeX\nW/7ee9HY9woKCsqUN2rUSPteJfve3r17icVi5f7fQvj7XnlCP4LKyclx8+bNCyyGoMRisXK/5Ujl\n1G6pyc3NJR6Pl/m2L5XTfpa6KLeZmZV7BFXlPUXNLGZmroJlTu2EKyIima7KLj7nXG4dxCEiInKQ\noIaZZyW21RBoaGZNgELnXGEQ2xcRkcxTZRdfNQ0H9gB3ANckHg8PaNsiIpKBAjmCcs7dA9wTxLZE\nREQguCMoERGRQClBiYhIJClBiYhIJClBiYhIJClBiYhIJClBiYhIJClBiYhIJClBiYhIJClBiYhI\nJClBiYhIJClBiYhIJClBiYhIJClBiYhIJClBiYhIJClBiYhIJClBiYhIJClBiYhIJClBiYhIJClB\niYhIJClBiYhIJClBiYhIJClBiYhIJClBiYhIJClBiYhIJClBiYhIJClBiYhIJClBiYhIJClBiYhI\nJClBiYhIJClBiYhIJClBiYhIJClBiYhIJNU4QZlZYzMbZ2arzGynmeWZWa8gghMRkcwVxBFUFrAG\nOA9oDQwHJptZxwC2LSIiGSqrphtwzu0C7im1apqZrQS6A1/UdPsiIpKZapygkplZNtAJWFzJcwYA\nAwCys7OJxWJBh1Fj+fn5kYwr6tRuqYnH4xQVFanNUqT9LHXp2GbmnAtuY2aNgBnACufcwOq8Jicn\nx82bNy+wGIISi8XIzc0NO4y0o3ZLTW5uLvF4nLy8vLBDSSvaz1IX5TYzs4+dcznJ66s8B2VmMTNz\nFSxzSj2vATABKACGBBq9iIhknCq7+JxzuVU9x8wMGAdkA72dc/trHpqIiGSyoM5BjQFOA3o45/YE\ntE0REclgQVwHdQIwEOgGfGVm+Ynl6hpHJyIiGSuIYearAAsgFhERkQM01ZGIiESSEpSIiERSoNdB\nHVIAZpuAVaEGUb52wOawg0hDarfUqc1SpzZLXZTb7ATn3JHJK0NPUFFlZvPKu3BMKqd2S53aLHVq\ns9SlY5upi09ERCJJCUpERCJJCapiY8MOIE2p3VKnNkud2ix1addmOgclIiKRpCMoERGJJCUoERGJ\nJCUoERGJJCWoajKzU8xsr5lNDDuWKDOzxmY2zsxWmdlOM8szs15hxxVFZtbWzF42s12J9roq7Jii\nTPtWzaTjZ5gSVPU9BnwUdhBpIAtYA5wHtAaGA5PNrGOIMUXVY/gbfGYDVwNjzOyb4YYUadq3aibt\nPsOUoKrBzK4A4sBbYccSdc65Xc65e5xzXzjnip1z04CVQPewY4sSM2sOXArc5ZzLd87NAaYC14Yb\nWXRp3zp06foZpgRVBTNrBdwL3BJ2LOnIzLKBTsDisGOJmE5AoXNuWal1CwAdQVWT9q3qSefPMCWo\nqv0BGOecWxt2IOnGzBoBk4C/O+eWhB1PxLQAdiSt2w60DCGWtKN9KyVp+xmW0QnKzGJm5ipY5phZ\nN6AH8JewY42Kqtqs1PMaABPw51iGhBZwdOUDrZLWtQJ2hhBLWtG+VX3p/hlW4zvqpjPnXG5l5WY2\nFOgIrDYz8N96G5rZ6c6579R6gBFUVZsBmG+scfiT/72dc/trO640tAzIMrNTnHOfJdZ1Rd1VldK+\nlbJc0vgzTFMdVcLMmnHwt9zb8H/sQc65TaEElQbM7HGgG9DDOZcfdjxRZWbPAQ64Ed9e04GznHNK\nUhXQvpWadP8My+gjqKo453YDu0t+N7N8YG86/GHDYmYnAAOBfcBXiW9tAAOdc5NCCyyaBgNPARuB\nLfgPDSWnCmjfSl26f4bpCEpERCIpowdJiIhIdClBiYhIJClBiYhIJClBiYhIJClBiYhIJClBiYhI\nJClBiYhIJClBiYhIJP1/WSNimRllS14AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25RnqeDpQfIB",
        "colab_type": "text"
      },
      "source": [
        "Implementing ELU in TensorFlow is trivial, just specify the activation function when building each layer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1RzNTidQfIC",
        "colab_type": "code",
        "outputId": "f5a3dcfd-c79c-4fa3-d28a-2b557b19ba8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "keras.layers.Dense(10, activation=\"elu\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.layers.core.Dense at 0x7fe6918b0b70>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AyqBScRhQfIG",
        "colab_type": "text"
      },
      "source": [
        "### SELU\n",
        "\n",
        "UUID - #S3C5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xlROs3rQfIG",
        "colab_type": "text"
      },
      "source": [
        "This activation function was proposed in this [great paper](https://arxiv.org/pdf/1706.02515.pdf) by Günter Klambauer, Thomas Unterthiner and Andreas Mayr, published in June 2017. During training, a neural network composed exclusively of a stack of dense layers using the SELU activation function and LeCun initialization will self-normalize: the output of each layer will tend to preserve the same mean and variance during training, which solves the vanishing/exploding gradients problem. As a result, this activation function outperforms the other activation functions very significantly for such neural nets, so you should really try it out. Unfortunately, the self-normalizing property of the SELU activation function is easily broken: you cannot use ℓ<sub>1</sub> or ℓ<sub>2</sub> regularization, regular dropout, max-norm, skip connections or other non-sequential topologies (so recurrent neural networks won't self-normalize). However, in practice it works quite well with sequential CNNs. If you break self-normalization, SELU will not necessarily outperform other activation functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWeSRnZ7QfIH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy.special import erfc\n",
        "\n",
        "# alpha and scale to self normalize with mean 0 and standard deviation 1\n",
        "# (see equation 14 in the paper):\n",
        "alpha_0_1 = -np.sqrt(2 / np.pi) / (erfc(1/np.sqrt(2)) * np.exp(1/2) - 1)\n",
        "scale_0_1 = (1 - erfc(1 / np.sqrt(2)) * np.sqrt(np.e)) * np.sqrt(2 * np.pi) * (2 * erfc(np.sqrt(2))*np.e**2 + np.pi*erfc(1/np.sqrt(2))**2*np.e - 2*(2+np.pi)*erfc(1/np.sqrt(2))*np.sqrt(np.e)+np.pi+2)**(-1/2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVifMVR7QfIK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def selu(z, scale=scale_0_1, alpha=alpha_0_1):\n",
        "    return scale * elu(z, alpha)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-SURrSQQfIR",
        "colab_type": "code",
        "outputId": "a02766e8-1e1c-4193-fd1c-67e949387dc0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        }
      },
      "source": [
        "plt.plot(z, selu(z), \"b-\", linewidth=2)\n",
        "plt.plot([-5, 5], [0, 0], 'k-')\n",
        "plt.plot([-5, 5], [-1.758, -1.758], 'k--')\n",
        "plt.plot([0, 0], [-2.2, 3.2], 'k-')\n",
        "plt.grid(True)\n",
        "plt.title(\"SELU activation function\", fontsize=14)\n",
        "plt.axis([-5, 5, -2.2, 3.2])\n",
        "\n",
        "save_fig(\"selu_plot\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving figure selu_plot\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nXRXZxAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlTjoMHDQfIV",
        "colab_type": "text"
      },
      "source": [
        "By default, the SELU hyperparameters (`scale` and `alpha`) are tuned in such a way that the mean output of each neuron remains close to 0, and the standard deviation remains close to 1 (assuming the inputs are standardized with mean 0 and standard deviation 1 too). Using this activation function, even a 1,000 layer deep neural network preserves roughly mean 0 and standard deviation 1 across all layers, avoiding the exploding/vanishing gradients problem:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lH5tT8LFQfIW",
        "colab_type": "code",
        "outputId": "23dd44b7-e9f6-4b10-84ee-c786cded0e1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "np.random.seed(42)\n",
        "Z = np.random.normal(size=(500, 100)) # standardized inputs\n",
        "for layer in range(1000):\n",
        "    W = np.random.normal(size=(100, 100), scale=np.sqrt(1 / 100)) # LeCun initialization\n",
        "    Z = selu(np.dot(Z, W))\n",
        "    means = np.mean(Z, axis=0).mean()\n",
        "    stds = np.std(Z, axis=0).mean()\n",
        "    if layer % 100 == 0:\n",
        "        print(\"Layer {}: mean {:.2f}, std deviation {:.2f}\".format(layer, means, stds))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Layer 0: mean -0.00, std deviation 1.00\n",
            "Layer 100: mean 0.02, std deviation 0.96\n",
            "Layer 200: mean 0.01, std deviation 0.90\n",
            "Layer 300: mean -0.02, std deviation 0.92\n",
            "Layer 400: mean 0.05, std deviation 0.89\n",
            "Layer 500: mean 0.01, std deviation 0.93\n",
            "Layer 600: mean 0.02, std deviation 0.92\n",
            "Layer 700: mean -0.02, std deviation 0.90\n",
            "Layer 800: mean 0.05, std deviation 0.83\n",
            "Layer 900: mean 0.02, std deviation 1.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XeCH9XGvAzW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Using SELU is easy\n",
        "keras.layers.Dense(10, activation=\"selu\",\n",
        "                   kernel_initializer=\"lecun_normal\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QWI97CCRuPqn"
      },
      "source": [
        "### IN CLASS EXERCISE 2: try different activation and normalizations on the Fashion MNIST dataset\n",
        "\n",
        "Try the following activation functions with their respective initialization parameters:\n",
        "\n",
        "*   ReLU\n",
        "*   Leaky ReLU (play with alpha)\n",
        "*   PReLU\n",
        "*   SELU\n",
        "\n",
        "Build a network with at least 50 -100 layers. \n",
        "Compare the results and share them with others. You could plot val_loss and loss, and val_acc and acc for instance. Use TensorBoard for that if you can.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "T9urGAOyuPqp"
      },
      "source": [
        "Load the dataset like so:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "cc32db99-dbe2-4908-8226-7dd5f1bc00e2",
        "id": "6fZXaWcTuPqp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
        "X_train_full = X_train_full / 255.0\n",
        "X_test = X_test / 255.0\n",
        "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
        "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 1s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QC2LQ_sdui84",
        "colab_type": "text"
      },
      "source": [
        "#### ReLU\n",
        "Let's train a neural network on Fashion MNIST using the ReLU:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNi3SGfAF8Nd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3H-JrzZ0uPq6"
      },
      "source": [
        "#### Leaky ReLU\n",
        "Let's train a neural network on Fashion MNIST using the Leaky ReLU:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OgKrgexuGCK6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Teq3E-xpuPrG"
      },
      "source": [
        "#### PReLU\n",
        "Now let's try PReLU:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dqy3aZIGKNa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ms9gr63wQfIZ",
        "colab_type": "text"
      },
      "source": [
        "#### SELU\n",
        "Let's create a neural net for Fashion MNIST with 100 hidden layers, using the SELU activation function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1eTvRCqaGMXd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xpgslnYQfJV",
        "colab_type": "text"
      },
      "source": [
        "# Batch Normalization\n",
        "\n",
        "UUID - #S3C6"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKQaJeqhQfJV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dense(300, activation=\"relu\"),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dense(100, activation=\"relu\"),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRI4MpHwQfJZ",
        "colab_type": "code",
        "outputId": "89d59a5d-a7e1-47cc-d310-111b55237b2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_10 (Flatten)         (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 784)               3136      \n",
            "_________________________________________________________________\n",
            "dense_425 (Dense)            (None, 300)               235500    \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 300)               1200      \n",
            "_________________________________________________________________\n",
            "dense_426 (Dense)            (None, 100)               30100     \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 100)               400       \n",
            "_________________________________________________________________\n",
            "dense_427 (Dense)            (None, 10)                1010      \n",
            "=================================================================\n",
            "Total params: 271,346\n",
            "Trainable params: 268,978\n",
            "Non-trainable params: 2,368\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOWZ8Fnv5uFS",
        "colab_type": "text"
      },
      "source": [
        "Let’s look at the parameters of the first BN layer. Two are trainable (by backpropagation), and two are not:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTT94fVWQfJc",
        "colab_type": "code",
        "outputId": "d9fbebaa-8bdd-47e0-b82a-ba3b123fd2cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "bn1 = model.layers[1]\n",
        "[(var.name, var.trainable) for var in bn1.variables]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('batch_normalization/gamma:0', True),\n",
              " ('batch_normalization/beta:0', True),\n",
              " ('batch_normalization/moving_mean:0', False),\n",
              " ('batch_normalization/moving_variance:0', False)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hIb2uXsQfJl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer=keras.optimizers.SGD(lr=1e-3),\n",
        "              metrics=[\"accuracy\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yBRqvieQfJo",
        "colab_type": "code",
        "outputId": "2786f266-a37a-4d0f-ff9d-ccd8e61c6d86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        }
      },
      "source": [
        "history = model.fit(X_train, y_train, epochs=10,\n",
        "                    validation_data=(X_valid, y_valid))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.8750 - accuracy: 0.7124 - val_loss: 0.5517 - val_accuracy: 0.8230\n",
            "Epoch 2/10\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.5753 - accuracy: 0.8030 - val_loss: 0.4716 - val_accuracy: 0.8470\n",
            "Epoch 3/10\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.5190 - accuracy: 0.8205 - val_loss: 0.4364 - val_accuracy: 0.8546\n",
            "Epoch 4/10\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4826 - accuracy: 0.8323 - val_loss: 0.4140 - val_accuracy: 0.8600\n",
            "Epoch 5/10\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4565 - accuracy: 0.8406 - val_loss: 0.3984 - val_accuracy: 0.8644\n",
            "Epoch 6/10\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4397 - accuracy: 0.8473 - val_loss: 0.3854 - val_accuracy: 0.8692\n",
            "Epoch 7/10\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4242 - accuracy: 0.8515 - val_loss: 0.3750 - val_accuracy: 0.8700\n",
            "Epoch 8/10\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4143 - accuracy: 0.8540 - val_loss: 0.3699 - val_accuracy: 0.8736\n",
            "Epoch 9/10\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4023 - accuracy: 0.8583 - val_loss: 0.3618 - val_accuracy: 0.8756\n",
            "Epoch 10/10\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3914 - accuracy: 0.8625 - val_loss: 0.3559 - val_accuracy: 0.8756\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSDmPq0gQfJr",
        "colab_type": "text"
      },
      "source": [
        "Sometimes applying BN before the activation function works better (there's a debate on this topic). Moreover, the layer before a `BatchNormalization` layer does not need to have bias terms, since the `BatchNormalization` layer adds some as well, it would be a waste of parameters, so you can set `use_bias=False` when creating those layers:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMAPUCyEQfJs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dense(300, kernel_initializer=\"he_normal\", use_bias=False),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Activation(\"elu\"),\n",
        "    keras.layers.Dense(100, kernel_initializer=\"he_normal\", use_bias=False),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Activation(\"elu\"),\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k13RlvRnQfJw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer=keras.optimizers.SGD(lr=1e-3),\n",
        "              metrics=[\"accuracy\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hRo4NoUQfJ2",
        "colab_type": "code",
        "outputId": "d14ca6aa-27c8-4573-fd7c-f3487711c0e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        }
      },
      "source": [
        "history = model.fit(X_train, y_train, epochs=10,\n",
        "                    validation_data=(X_valid, y_valid))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.9111 - accuracy: 0.6995 - val_loss: 0.6418 - val_accuracy: 0.7860\n",
            "Epoch 2/10\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.6335 - accuracy: 0.7845 - val_loss: 0.5538 - val_accuracy: 0.8138\n",
            "Epoch 3/10\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.5750 - accuracy: 0.8020 - val_loss: 0.5133 - val_accuracy: 0.8260\n",
            "Epoch 4/10\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.5378 - accuracy: 0.8151 - val_loss: 0.4880 - val_accuracy: 0.8346\n",
            "Epoch 5/10\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.5135 - accuracy: 0.8222 - val_loss: 0.4704 - val_accuracy: 0.8416\n",
            "Epoch 6/10\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4978 - accuracy: 0.8259 - val_loss: 0.4560 - val_accuracy: 0.8452\n",
            "Epoch 7/10\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4853 - accuracy: 0.8331 - val_loss: 0.4445 - val_accuracy: 0.8494\n",
            "Epoch 8/10\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4735 - accuracy: 0.8348 - val_loss: 0.4377 - val_accuracy: 0.8516\n",
            "Epoch 9/10\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4646 - accuracy: 0.8388 - val_loss: 0.4299 - val_accuracy: 0.8542\n",
            "Epoch 10/10\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4553 - accuracy: 0.8425 - val_loss: 0.4235 - val_accuracy: 0.8554\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWN8LcO2QfJ7",
        "colab_type": "text"
      },
      "source": [
        "## Gradient Clipping\n",
        "\n",
        "UUID - #S3C7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAWx6aKbQfJ7",
        "colab_type": "text"
      },
      "source": [
        "All Keras optimizers accept `clipnorm` or `clipvalue` arguments:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukmwmNh3QfJ8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = keras.optimizers.SGD(clipvalue=1.0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wc_-QLD0QfJ-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = keras.optimizers.SGD(clipnorm=1.0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZ3Ah5f6EHyy",
        "colab_type": "text"
      },
      "source": [
        "# EXERCISES\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIky8lqyEM1e",
        "colab_type": "text"
      },
      "source": [
        "## EXERCISE 1: hyperparameter tuning on Fashion MNIST\n",
        "\n",
        "Take everything you have learned in this past 3 sessions and find the best model for Fashion MNIST. We will take into account the speed of training and prediction as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKN2m58rEL3K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDBYn6qvEmPs",
        "colab_type": "text"
      },
      "source": [
        "## EXERCISE 2: covid19 related\n",
        "\n",
        "*   **Option 1**: keep on training the Xray images from covid19, but now using Talos hyperparameter optimization, different activation and initializers and batchnorm and clipping when necessary.\n",
        "*   **Option 2**: take the Covid19 prediction challenge and submit a notebook that uses the advanced techniques discussed until now, i.e. using Talos hyperparameter optimization, different activation and initializers and batchnorm and clipping when necessary.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTXlg1yeEG6X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}